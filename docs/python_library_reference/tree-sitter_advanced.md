## 1) Language objects: grammar identity, node kinds, fields, and parse states

### Quick gotchas (high-impact)

* **ABI gates** exist at *both* library + language level: `tree_sitter.LANGUAGE_VERSION` / `MIN_COMPATIBLE_LANGUAGE_VERSION` (library supports a *range*), while each `Language` advertises its own `abi_version` + `semantic_version`. ([Tree-sitter][1])
* **Simulating parse transitions** uses *grammar symbol IDs* (typically `node.grammar_id`), not the potentially-aliased `node.kind_id`. ([Tree-sitter][2])
* **LookaheadIterator starts at `ERROR`** by default; you usually reset/advance before consuming suggestions. ([Tree-sitter][3])

(Construction pattern is the same as in your base overview: `Language(tspython.language())` → `Parser(Language)`.)

---

### 1.0 Mental model

`Language` is the *grammar descriptor* + *introspection surface* (symbol table, field table, parse automaton). It’s the “schema” that makes `Node.kind_id/grammar_id`, `Node.parse_state`, and query compilation meaningful. ([Tree-sitter][2])

---

### 1.1 Grammar identity + compatibility (production-grade “sanity checks”)

**Identity fields**

* `Language.name`: human name (e.g., `"python"`). ([Tree-sitter][2])
* `Language.semantic_version`: semver of the grammar package. ([Tree-sitter][2])
* `Language.abi_version`: ABI version assigned when generated by the Tree-sitter CLI. ([Tree-sitter][2])

**Library ABI window**

* `tree_sitter.LANGUAGE_VERSION` = latest ABI supported by your binding; `tree_sitter.MIN_COMPATIBLE_LANGUAGE_VERSION` = earliest supported ABI; **backwards-compatible but not forwards-compatible** is the rule of thumb. ([Tree-sitter][1])

**Minimal compatibility gate**

```python
from tree_sitter import LANGUAGE_VERSION, MIN_COMPATIBLE_LANGUAGE_VERSION

def assert_lang_abi_compatible(lang) -> None:
    if not (MIN_COMPATIBLE_LANGUAGE_VERSION <= lang.abi_version <= LANGUAGE_VERSION):
        raise RuntimeError(
            f"Language ABI {lang.abi_version} not in supported window "
            f"[{MIN_COMPATIBLE_LANGUAGE_VERSION}, {LANGUAGE_VERSION}]"
        )
```

(Your overview already frames this “ABI window check” as a first-class advanced practice.)

---

### 1.2 Symbol table: node kinds (IDs ↔ names) + “named/visible/supertype” classifiers

**Counts (fast prealloc + bounds)**

* `Language.node_kind_count`: number of distinct node types. ([Tree-sitter][2])
* `Language.parse_state_count`: number of valid parse states. ([Tree-sitter][2])

**Core mapping + classification**

* `id_for_node_kind(kind: str, named: bool) -> int` (name → symbol ID). ([Tree-sitter][2])
* `node_kind_for_id(id: int) -> str` (symbol ID → name). ([Tree-sitter][2])
* `node_kind_is_named(id)`, `node_kind_is_visible(id)`, `node_kind_is_supertype(id)`. ([Tree-sitter][2])
* `supertypes` (list of supertype symbol IDs); `subtypes(supertype_id)` to enumerate specialization lattice. ([Tree-sitter][2])

**Why advanced agents care (indexing / stability)**

* You can compile **grammar-aware, allocation-light** matchers: store symbol IDs for hot node kinds (`identifier`, `call`, `function_definition`, …) and compare ints instead of strings in tight loops.
* You can decide “semantic” vs “syntactic noise” at the grammar level: `named` and `visible` gates are often better heuristics than `type != ...`.

**Cache pattern**

```python
from dataclasses import dataclass

@dataclass(frozen=True)
class LangCache:
    # hot symbol ids; ints are faster than strings in inner loops
    IDENT: int
    CALL: int

def build_cache(lang) -> LangCache:
    return LangCache(
        IDENT=lang.id_for_node_kind("identifier", True),
        CALL=lang.id_for_node_kind("call", True),
    )
```

---

### 1.3 Field table: field IDs as a stable “structural ABI”

Fields are grammar-defined labels (e.g., `name:`, `body:`). Treat them as the *typed slots* of nodes.

* `Language.field_id_for_name(field: str) -> int`; `Language.field_name_for_id(id: int) -> str`. ([Tree-sitter][2])
* `Language.field_count`: number of distinct fields. ([Tree-sitter][2])
* On `Node`: `child_by_field_id(id)` / `children_by_field_id(id)`; `field_name_for_child(i)` / `field_name_for_named_child(i)` for reverse lookup during traversal. ([Tree-sitter][4])

**Reason to prefer field IDs**: `field_id_for_name` once at startup → compare `int` in traversal, avoid repeated string hashing and repeated “what field is this child?” resolution.

---

### 1.4 Parse states: turning the parser into a completion/diagnostics oracle

**Raw state observables**

* `Node.parse_state`: parse automaton state *at this node*. ([Tree-sitter][4])
* `Node.next_parse_state`: parse automaton state *after this node*. ([Tree-sitter][4])

**Transition + lookahead**

* `Language.next_state(state, id)` computes the next state; docs explicitly show the canonical pattern: `language.next_state(node.parse_state, node.grammar_id)`. ([Tree-sitter][2])
* `Language.lookahead_iterator(state)` produces a `LookaheadIterator` for that state. ([Tree-sitter][2])
* `LookaheadIterator`:

  * `names()` / `symbols()` as bulk extraction helpers
  * iteration via `__next__`
  * `current_symbol` / `current_symbol_name`
  * `reset(state, language=None) -> bool`
  * tip: for `ERROR` nodes use the *first leaf* node’s state; for `MISSING` nodes use the *previous non-extra leaf* node. ([Tree-sitter][3])

**Minimal completion probe (state → allowed next symbols)**

```python
def allowed_symbols_at_node(lang, node) -> list[str]:
    # Prefer grammar_id for transition correctness (aliases can differ from grammar symbols).
    state_after = lang.next_state(node.parse_state, node.grammar_id)
    it = lang.lookahead_iterator(state_after)
    # Bulk extract; compact + stable for LLM agents to reason about.
    return it.names()
```

**Node identity note (incremental systems)**

* `Node.id` is unique within a tree; if a node is reused during incremental reparse, it retains the same id across old/new trees—useful for cache keys that survive edits. ([Tree-sitter][4])

---

## 2) Parser deep dive: partial parsing, streaming input, reset, logging, and DOT graphs

### Quick gotchas (high-impact)

* `Parser.parse` expects **bytes** or a **read callback** that returns bytes; *encoding matters* (`utf8` or `utf16` family). ([Tree-sitter][5])
* `timeout_micros` can yield a **partial/failed parse**; if it times out, subsequent parses may **resume** unless you `reset()`. ([Tree-sitter][5])
* Cancellation for **queries** is via `QueryCursor(..., timeout_micros=...)` + `progress_callback`; **Parser.parse itself** (0.25.2 docs) exposes `timeout_micros` but no `progress_callback` parameter. ([Tree-sitter][5])

---

### 2.1 Parser construction + timeout semantics

**API surface**

* `Parser(language, *, included_ranges=None, timeout_micros=None)`; core method: `parse(source, /, old_tree=None, encoding='utf8')`. ([Tree-sitter][5])
* `parse(...)` returns `Tree` on success or `None` if no language assigned *or* timeout expired. ([Tree-sitter][5])

**Timeout + resume behavior (extremely non-obvious but critical)**

* If the parser fails due to timeout, **by default it resumes where it left off on next parse**; if you’re switching documents or want a clean parse, call `reset()` first. ([Tree-sitter][5])

```python
from tree_sitter import Parser

parser = Parser(lang, timeout_micros=50_000)  # 50ms budget (example)
tree = parser.parse(src_bytes)
if tree is None:
    # timed out or missing language; if you now want to parse a different doc:
    parser.reset()
```

---

### 2.2 Streaming / non-contiguous input: read callbacks (byte-offset or point-driven)

`parse` can take a callback instead of bytes:

* Docs: callback takes `(byte_offset, position)` and returns a bytestring starting at that location; at end-of-text return empty slice. ([Tree-sitter][5])
* README clarifies practical contract: read callable may use byte offset or `(row, column)` point; **empty bytes or `None`** terminates parsing for that line; bytes must be UTF-8 or UTF-16. ([GitHub][6])

**Byte-offset callback (rope/mmap-friendly)**

```python
CHUNK = 4096

def read(byte_offset: int, _point) -> bytes:
    return buf[byte_offset: byte_offset + CHUNK]  # empty => EOF
tree = parser.parse(read, encoding="utf8")
```

**Point-based callback (line table / incremental buffer)**

```python
def read(_byte_offset: int, point) -> bytes | None:
    row, col = point
    if row >= len(lines) or col >= len(lines[row]):
        return None
    return lines[row][col:].encode("utf8")
tree = parser.parse(read, encoding="utf8")
```

(These patterns are directly aligned with the upstream README guidance.) ([GitHub][6])

---

### 2.3 Encoding discipline (UTF-8 vs UTF-16)

* Parser-level `encoding` argument exists; docs show default `encoding='utf8'`. ([Tree-sitter][5])
* README shows UTF-16 usage and the key systems constraint: your read callback must slice on the correct byte width (e.g., 2 bytes per code unit for UTF-16). ([GitHub][6])

```python
parser.language = JAVASCRIPT  # language is settable in practice
source_code = bytes("'' && ''", "utf16")

def read_utf16(byte_pos: int, _point):
    return source_code[byte_pos: byte_pos + 2]  # step 2 bytes
tree = parser.parse(read_utf16, encoding="utf16")
```

([GitHub][6])

---

### 2.4 Partial parsing: `included_ranges` as “projection parsing”

**Surface**

* `Parser.included_ranges`: list of `Range` objects describing what text is included in parsing. ([Tree-sitter][5])
* `Range(start_point, end_point, start_byte, end_byte)` defines byte+point bounds. ([Tree-sitter][7])

**Canonical usage**

```python
from tree_sitter import Parser, Range, Point

parser = Parser(lang)

# Example: parse only a slice of a larger document (byte/point math must match the true buffer encoding).
parser.included_ranges = [
    Range(
        start_point=Point(100, 0),
        end_point=Point(140, 0),
        start_byte=doc_line_start_bytes[100],
        end_byte=doc_line_start_bytes[140],
    )
]
tree = parser.parse(doc_bytes)
```

**When to use (advanced tooling)**

* Large files with “ignored” regions (generated blocks, vendor code) where you want smaller parse cost and smaller index churn.
* “Host document projections” (e.g., only fenced code blocks)—this pairs naturally with injection strategies in later sections of the doc.

---

### 2.5 Logging: `Parser.logger` + `LogType` (LEX vs PARSE)

* `Parser.logger` is a settable attribute: “the logger that the parser should use during parsing.” ([Tree-sitter][5])
* `LogType` is an `IntEnum` with `PARSE` and `LEX` categories. ([Tree-sitter][8])

**Minimal structured logger pattern (signature not specified in docs; empirically it’s typically `(LogType, str) -> None`)**

```python
from tree_sitter import LogType

def ts_log(log_type: LogType, msg: str) -> None:
    # Map LEX/PARSE to your logging infra; keep this fast (hot path).
    print(f"{log_type.name}: {msg}")

parser.logger = ts_log
```

---

### 2.6 DOT graphs: `print_dot_graphs` for parser-internals visualization

* `Parser.print_dot_graphs(file)` writes **debugging graphs** in DOT format during parsing; pass `None` to disable. ([Tree-sitter][5])
* This is the “parser engine lens” complement to `Tree.print_dot_graph` (tree-structure lens), as already framed in your base overview.

```python
with open("ts_parse_steps.dot", "w") as f:
    parser.print_dot_graphs(f)
    tree = parser.parse(src_bytes)
# render via graphviz: dot -Tsvg ts_parse_steps.dot > steps.svg
```

---

### 2.7 `reset()` as a correctness primitive (not just cleanup)

Use `reset()` whenever:

* you hit a timeout and do **not** want resume semantics, or
* you reuse a parser instance across unrelated documents (common in daemons / services). ([Tree-sitter][5])

---

If you want, the next two sections naturally flow from here: **Trees/Nodes at scale** (incrementality, node identity, coordinate transforms) and **QueryCursor execution mechanics** (range limiting + match limits + progress callbacks) — but the above fully covers Sections 1–2 at the “agent-ready” API/semantics density level.

[1]: https://tree-sitter.github.io/py-tree-sitter/ "py-tree-sitter — py-tree-sitter 0.25.2 documentation"
[2]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.Language.html "Language — py-tree-sitter 0.25.2 documentation"
[3]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.LookaheadIterator.html "LookaheadIterator — py-tree-sitter 0.25.2 documentation"
[4]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.Node.html "Node — py-tree-sitter 0.25.2 documentation"
[5]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.Parser.html "Parser — py-tree-sitter 0.25.2 documentation"
[6]: https://github.com/tree-sitter/py-tree-sitter "GitHub - tree-sitter/py-tree-sitter: Python bindings to the Tree-sitter parsing library"
[7]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.Range.html "Range — py-tree-sitter 0.25.2 documentation"
[8]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.LogType.html "LogType — py-tree-sitter 0.25.2 documentation"


According to a document from **December 24, 2025**, here are the next two advanced sections in the same “template style” as your attached overview (dense, agent-oriented, minimal fluff).

---

## 3) Trees/Nodes at scale (incrementality, node identity, coordinate transforms)

### Quick gotchas (high-impact)

* **`changed_ranges` is only valid if the *old* tree was correctly `edit(...)`’d** so its offsets match the new text; call it immediately after `parse(..., old_tree=...)`. ([Tree-sitter][1])
* **`Node.text` is only valid if the tree has not been edited**; after `Tree.edit`, reacquire nodes (or manually `Node.edit` if you must keep a specific instance). ([Tree-sitter][2])
* **`Node.child(i)` / `named_child(i)` is `log(i)`**; for long child lists use `children` or a `TreeCursor` walk. ([Tree-sitter][2])
* **`Node.id` is unique only within a tree**; it can be reused across old/new trees *only* when the node is structurally reused during incremental parsing. ([Tree-sitter][2])
* **Byte offsets dominate correctness**: all edits/ranges must be byte-accurate for your encoding; “point math” is a secondary coordinate system (row/col) that must agree with bytes. ([Tree-sitter][1])

---

### 3.0 Mental model (for LLM agents building indexers)

* `Tree` is the immutable-ish CST snapshot *produced by parsing a specific byte buffer*.
* Incremental parsing is a two-step contract:

  1. mutate the **old tree’s coordinate mapping** via `Tree.edit(...)`,
  2. parse the **new bytes** with `Parser.parse(..., old_tree=old_tree)`, enabling subtree reuse,
     then compute invalidation spans via `old_tree.changed_ranges(new_tree)`. ([Tree-sitter][1])
* `Node` is a cheap handle into the tree; treat it as a *view* over the tree, not a durable object unless you explicitly rebase it across edits.

---

### 3.1 Incremental parsing loop (canonical, correctness-first)

**Tree API constraints**

* `Tree.edit(start_byte, old_end_byte, new_end_byte, start_point, old_end_point, new_end_point)` requires both byte offsets and row/col points. ([Tree-sitter][1])
* `Tree.changed_ranges(new_tree)` returns ranges where ancestor paths differ; returned spans may be “slightly larger” than minimal exact edits; tree-sitter tries to keep them small. ([Tree-sitter][1])

**Minimal reusable helper**

```python
from dataclasses import dataclass
from tree_sitter import Point

@dataclass(frozen=True)
class InputEdit:
    start_byte: int
    old_end_byte: int
    new_end_byte: int
    start_point: Point
    old_end_point: Point
    new_end_point: Point

def reparse_incremental(parser, *, old_tree, new_bytes: bytes, edit: InputEdit):
    # 1) shift old_tree’s internal offsets so its nodes/ranges line up with new_bytes
    old_tree.edit(
        edit.start_byte, edit.old_end_byte, edit.new_end_byte,
        edit.start_point, edit.old_end_point, edit.new_end_point,
    )

    # 2) reparse with subtree reuse enabled
    new_tree = parser.parse(new_bytes, old_tree=old_tree)

    # 3) compute minimal invalidation regions (call on old tree)
    changed = list(old_tree.changed_ranges(new_tree))
    return new_tree, changed
```

Matches your overview’s end-to-end incremental loop semantics. ([Tree-sitter][1])

---

### 3.2 Interpreting `changed_ranges`: how to “re-index only what matters”

`changed_ranges` is *not* “text diffs”; it’s “ancestor-path diffs”: outside these ranges, the root→leaf ancestor chain is identical in old/new trees. ([Tree-sitter][1])
Practical consequences:

* **Safe invalidation**: any node whose `byte_range` intersects a changed range is suspect; everything outside is structurally identical. ([Tree-sitter][1])
* **Work minimization**: for each changed range, locate an enclosing “semantic container” and only rerun the query packs that could be impacted (e.g., rerun `imports` queries only if container is module-level).
* **Range expansion**: because matches can straddle boundaries (esp. for multi-node patterns), you often expand each range to a container boundary (statement / block / module) before querying.

Container lift recipe (fast, byte-driven):

```python
def lift_to_named_container(root, start_b: int, end_b: int):
    n = root.named_descendant_for_byte_range(start_b, end_b)
    # climb until a stable container (language-dependent; examples: module, block, statement list)
    while n.parent is not None and n.parent.is_named:
        n = n.parent
    return n
```

API pieces used: `named_descendant_for_byte_range`, `parent`, `is_named`. ([Tree-sitter][2])

---

### 3.3 Node retention & identity: “what can I cache safely?”

**Node durability rules**

* After `Tree.edit`, nodes you *retrieve from the tree afterwards* reflect the edit; you only need `Node.edit` if you are holding a specific `Node` instance and want to keep using it post-edit. ([Tree-sitter][2])
* `Node.text` is only valid if the tree hasn’t been edited; treat it as an unsafe convenience, not a durable source-of-truth. ([Tree-sitter][2])

**`Node.id` semantics (incremental cache lever)**

* Unique within a tree; if a node is reused during incremental reparse, it keeps the same id across old/new trees. ([Tree-sitter][2])
  Implication: you can build **in-memory, per-document, per-session** caches keyed by `Node.id` (plus file identity), but you must still invalidate by `changed_ranges` because ids can disappear or shift when nodes are replaced.

**“Has this node moved?”**

* `Node.has_changes`: flags nodes that have been edited. ([Tree-sitter][2])
  Use as a *local* signal; don’t use it as your only invalidation strategy (changed ancestor paths matter more).

---

### 3.4 Byte-range accelerators and “surgical” navigation

When you’re doing large-tree scans driven by byte offsets:

* `first_child_for_byte(byte)` / `first_named_child_for_byte(byte)` jump into the child list near an offset (avoid linear scan). ([Tree-sitter][2])
* `descendant_for_byte_range` / `named_descendant_for_byte_range` return the smallest spanning node (ideal for “cursor-at-byte” workflows). ([Tree-sitter][2])
* `child_by_field_id/name` & `children_by_field_id/name`: treat fields as a grammar-defined struct layout; prefer field IDs when you’ve precomputed them for speed. ([Tree-sitter][2])

---

### 3.5 Traversal performance: avoid Python list allocations

* Prefer `Node.walk()` / `Tree.walk()` → `TreeCursor` for streaming traversals; this avoids allocating lists of children. ([Tree-sitter][1])
* If you do use `Node.child(i)`, remember it’s `log(i)` and can dominate in deep loops; for bulk iteration use `node.children` or cursor walking. ([Tree-sitter][2])

---

### 3.6 Coordinate transforms: `root_node_with_offset` as “global coordinates for sub-parses”

`Tree.root_node_with_offset(offset_bytes, offset_extent)` returns a root node whose coordinates are shifted forward by the given offset (bytes + row/col extent). ([Tree-sitter][1])
Use case: parse an embedded slice “as if standalone”, then remap nodes back into host-document coordinates.

```python
from tree_sitter import Point, Range

# Suppose you extract a Python snippet starting at (host_row, host_col) and host_start_byte.
host_offset_point = Point(host_row, host_col)

parser.included_ranges = [Range(Point(0,0), Point(snippet_rows,0), 0, len(snippet_bytes))]
t = parser.parse(snippet_bytes)

# Now view that tree in host coordinate space:
root_global = t.root_node_with_offset(host_start_byte, host_offset_point)
```

You can use this instead of manually offsetting every node range. (Conceptual rationale: parse subset standalone, then access in larger document coordinate space.) ([Tree-sitter][1])

---

### 3.7 Debugging at scale

* `Tree.print_dot_graph(file)` dumps CST structure as DOT (tree lens). ([Tree-sitter][1])
* For “why did incremental reuse fail?” pair with parser-step DOT graphs (`Parser.print_dot_graphs`) from earlier section (parser lens). ([Tree-sitter][3])

---

## 4) QueryCursor execution mechanics (range limiting + match limits + progress callbacks)

### Quick gotchas (high-impact)

* `QueryCursor.set_byte_range` / `set_point_range` are **intersection-based**: a match may be returned even if some captures fall outside the range, as long as the match overlaps it. ([Tree-sitter][4])
* If you need “all captures fully inside”, use `set_containing_byte_range` / `set_containing_point_range` (**added in 0.26.0**, so version-gate). ([Tree-sitter][4])
* `match_limit` bounds the number of in-progress matches; always check `did_exceed_match_limit` and treat results as *incomplete/partial*. ([Tree-sitter][4])
* Prefer **`progress_callback` for cancellation/telemetry**; `timeout_micros` exists but is explicitly deprecated in favor of progress callbacks. ([GitHub][5])
* Non-local query patterns disable range optimizations; avoid if you depend on “windowed” execution.  ([Tree-sitter][6])

---

### 4.1 Construction + configuration surface

Signature: `QueryCursor(query, *, match_limit=None, timeout_micros=None)`; attributes: `match_limit`, `did_exceed_match_limit`; methods include range setters + executors. ([Tree-sitter][4])

```python
cursor = QueryCursor(q, match_limit=10_000, timeout_micros=None)
cursor.set_max_start_depth(32)  # optional pruning (see below)
```

`timeout_micros` exists as an argument/property but is deprecated in favor of progress callbacks. ([GitHub][5])

---

### 4.2 Execution modes: `captures(...)` vs `matches(...)`

* `captures(node, predicate=None, progress_callback=None) -> dict[str, list[Node]]`: aggregated capture buckets. ([Tree-sitter][4])
* `matches(node, predicate=None, progress_callback=None) -> list[tuple[int, dict[str, Node]]]`: per-match bindings keyed by pattern index (grouping lets you reason in “whole rule matches” rather than independent captures). ([Tree-sitter][4])

**Operational guidance**

* Use **`matches`** when you need *relational coherence* between captures (e.g., definition node + its name + its body) and want to process match-by-match.
* Use **`captures`** when you want *flat indexing* by capture type (e.g., collect all `@identifier` nodes), but beware it can balloon memory for huge trees—pair with scoping + limits.  ([Tree-sitter][4])

---

### 4.3 Range limiting: intersection vs containment (two-tier windowing)

**Intersection windows (0.25+)**

* `set_byte_range(start, end)` / `set_point_range(start, end)` return matches that *intersect* the window. ([Tree-sitter][4])

**Containment windows (0.26.0+)**

* `set_containing_byte_range(start, end)` / `set_containing_point_range(start, end)` require matches be *fully contained* in the window; can be combined with intersection windows (e.g., intersect a narrow line, but require full containment in a larger band). ([Tree-sitter][4])

**Practical “changed_ranges + query” pattern**

```python
def run_query_over_changed(cursor, root, changed_ranges):
    out = []
    for r in changed_ranges:
        cursor.set_byte_range(r.start_byte, r.end_byte)  # fast focus window
        out.extend(cursor.matches(root))                 # or captures(...)
    return out
```

(Your base overview already frames this: changed_ranges → set_byte_range/point_range → bounded query execution.) ([Tree-sitter][4])

---

### 4.4 Match throttling: `match_limit` + `did_exceed_match_limit` (defensive execution)

* `match_limit` bounds “in-progress matches”; if exceeded, `did_exceed_match_limit` becomes true for the last execution. ([Tree-sitter][4])
* Core bindings typically enforce an upper bound (e.g., 65536 in Rust bindings); treat limits as a hard safety rail. ([Docs.rs][7])

**Degradation strategy (agent-friendly)**

1. If `did_exceed_match_limit`: **tighten windows** (smaller ranges) *or* **simplify patterns** (remove ambiguous alternations / non-local roots) *or* **split query into phases** (coarse filter first, then refined query on smaller subtree).
2. Prefer `matches` over `captures` when you only need “rule matches” and not all raw captures (often fewer objects allocated).

---

### 4.5 Cancellation + telemetry: `progress_callback`

Release notes and other bindings define `progress_callback` as a periodic check: **return `true` to cancel**; also usable for instrumentation (where is the parser/query in the document). ([GitHub][5])

In Go bindings (mirrors core semantics):

* Parser progress callback receives a state with `CurrentByteOffset` and `HasError`. ([Go Packages][8])
* Query progress callback receives a state with `CurrentByteOffset`. ([Go Packages][8])

**Python-side ergonomic pattern (tolerant to exact state shape)**

```python
def cancel_after(byte_budget: int):
    def cb(state) -> bool:
        # works if state exposes CurrentByteOffset (Go-like), or if it's an int, etc.
        cur = getattr(state, "CurrentByteOffset", state)
        return int(cur) > byte_budget  # True => cancel
    return cb

cursor.matches(root, progress_callback=cancel_after(200_000))
```

Treat callbacks as hot-path code: pure, branch-light, no allocations.

---

### 4.6 `set_max_start_depth`: pruning query search

`set_max_start_depth(max_start_depth)` prevents the cursor from exploring children beyond a certain depth (but deep patterns can still force checks). ([Tree-sitter][4])
Use it as a blunt instrument when:

* you only care about top-level constructs (imports, defs, module docs), or
* a file’s deep expression trees cause pathological query exploration.

---

### 4.7 Windowed, incremental query execution (canonical “LLM agent” recipe)

Combine everything into a stable, bounded runner:

1. Parse incrementally; compute changed ranges.
2. For each range: lift to a container; set containing+intersection windows (if available); run match-coherent queries; enforce `match_limit`; cancel via progress callback.
3. If limit/cancel triggers: shrink windows or split queries.

This is exactly the “change-driven analysis” loop your overview is already steering toward: `changed_ranges` + scoped `QueryCursor` + bounded execution. 

[1]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.Tree.html "Tree — py-tree-sitter 0.25.2 documentation"
[2]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.Node.html "Node — py-tree-sitter 0.25.2 documentation"
[3]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.Parser.html "Parser — py-tree-sitter 0.25.2 documentation"
[4]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.QueryCursor.html "QueryCursor — py-tree-sitter 0.25.2 documentation"
[5]: https://github.com/tree-sitter/py-tree-sitter/releases "Releases · tree-sitter/py-tree-sitter · GitHub"
[6]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.Query.html?utm_source=chatgpt.com "Query — py-tree-sitter 0.25.2 documentation"
[7]: https://docs.rs/tree-sitter/latest/tree_sitter/struct.QueryCursor.html?utm_source=chatgpt.com "QueryCursor in tree_sitter - Rust"
[8]: https://pkg.go.dev/github.com/tree-sitter/go-tree-sitter "tree_sitter package - github.com/tree-sitter/go-tree-sitter - Go Packages"


## 5) TreeCursor mastery: traversal patterns that don’t fall over in large trees

### Quick gotchas (high-impact)

* **Scope wall:** a `TreeCursor` can only descend into children of the node it was constructed with—treat that node as the cursor’s “root boundary”. ([Tree-sitter][1])
* **Asymmetric costs:** `goto_last_child()` and `goto_previous_sibling()` can be slower (they may iterate through siblings to recompute positions). ([Tree-sitter][1])
* **Child lookup vs cursor walk:** `Node.child(i)` / `Node.named_child(i)` are *log(i)*; for long sibling lists prefer `Node.children` or `Node.walk()` (cursor traversal). ([Tree-sitter][2])
* **Cursor reuse:** `reset_to(other_cursor)` preserves parent stack and supports “cursor pooling” without losing parent info (unlike re-initializing). ([Tree-sitter][1])
* **Byte/point seek:** `goto_first_child_for_byte` / `goto_first_child_for_point` return an *index* (or `None`), not a boolean—use this both as a seek primitive and a stable “child slot” if needed. ([Tree-sitter][1])

(Your template already frames TreeCursor as the “no list allocations” traversal primitive and explicitly calls out `goto_first_child`, `goto_last_child`, `goto_first_child_for_byte`, etc.)

---

### 5.0 Mental model

A `TreeCursor` is a stateful pointer with an implicit ancestor stack. Each move mutates cursor state; `cursor.node` exposes the current `Node`; `cursor.field_id/field_name` label the edge from parent→current (if any). This is the lowest-allocation way to do *streaming* traversals. ([Tree-sitter][1])

Construction: `Node.walk()` creates a new cursor rooted at that node. ([Tree-sitter][2])

---

### 5.1 Core API surface (the “moves” + the “introspection”)

**Moves**

* `goto_first_child() -> bool`, `goto_last_child() -> bool` (last-child is potentially slower). ([Tree-sitter][1])
* `goto_next_sibling() -> bool`, `goto_previous_sibling() -> bool` (previous-sibling potentially slower). ([Tree-sitter][1])
* `goto_parent() -> bool` (false at root boundary). ([Tree-sitter][1])
* `goto_first_child_for_byte(byte) -> int | None`, `goto_first_child_for_point(point) -> int | None` (seek). ([Tree-sitter][1])
* `goto_descendant(index)` where `0` is the original root node (tree-local addressing). ([Tree-sitter][1])

**Cursor state / metadata**

* `node` (current node), `depth` (relative to original root), `descendant_index` (index among all descendants of original root). ([Tree-sitter][1])
* `field_id` / `field_name` (edge label; may be absent). ([Tree-sitter][1])

**Reuse**

* `copy()` creates an independent cursor snapshot. ([Tree-sitter][1])
* `reset(node)` re-initializes to the original construction root (practically: restart walk). ([Tree-sitter][1])
* `reset_to(cursor)` re-initializes to another cursor’s position **without** losing parent info; enables cursor pooling. ([Tree-sitter][1])

---

### 5.2 Traversal patterns that scale (canonical, no recursion)

#### A) Preorder DFS iterator (named-only optional), stackless-ish

```python
from tree_sitter import Node

def walk_preorder(root: Node):
    c = root.walk()  # TreeCursor rooted at root (scope wall)
    reached_root = False
    while not reached_root:
        yield c.node

        if c.goto_first_child():
            continue
        if c.goto_next_sibling():
            continue

        # climb until we can move sideways or we hit root boundary
        while True:
            if not c.goto_parent():
                reached_root = True
                break
            if c.goto_next_sibling():
                break
```

Why this pattern: it is O(nodes visited) in Python-level control flow, and never allocates child lists. `Node.walk()` exists explicitly for this use. ([Tree-sitter][2])

#### B) Field-aware walk (cheap edge labeling)

Use `cursor.field_id/field_name` as the “relationship type” for parent→child (e.g., `name`, `body`, `condition`). This avoids repeated `parent.field_name_for_child(i)` calls and turns traversal into a labeled-edge stream. ([Tree-sitter][1])

Minimal pattern:

```python
def walk_edges(root):
    c = root.walk()
    stack = []
    while True:
        n = c.node
        yield (c.depth, c.field_name, n)  # field_name may be None

        if c.goto_first_child():
            continue
        if c.goto_next_sibling():
            continue
        while True:
            if not c.goto_parent():
                return
            if c.goto_next_sibling():
                break
```

#### C) “Seek to offset” in wide nodes (avoid linear scans)

When you have a byte offset and are sitting at a container node with many children, `goto_first_child_for_byte(byte)` jumps to the first child that contains or starts after that offset. Treat the returned index as “where did I land” (useful for resuming scans). ([Tree-sitter][1])

---

### 5.3 Tree-local addressing + restoration strategies (pick the right key)

* **`descendant_index` + `goto_descendant(i)`**: fastest “restore cursor position” within the same rooted subtree (stable only as long as the subtree shape is unchanged). ([Tree-sitter][1])
* **`Node.id`**: unique within a tree; can persist across incremental reparses only for reused nodes (good cache key, but not guaranteed). ([Tree-sitter][2])
* **Byte range + re-find**: store `(start_byte, end_byte)` and re-locate via `named_descendant_for_byte_range` (more robust across edits if you “lift” to semantic containers). ([Tree-sitter][2])

---

### 5.4 Performance micro-rules (the ones that matter)

* Prefer `goto_next_sibling` over `goto_previous_sibling` in hot paths; “previous” can be worst-case O(#siblings). ([Tree-sitter][1])
* Avoid `goto_last_child` inside loops; it may iterate all children to compute position—use `goto_first_child` + repeated `goto_next_sibling` if you need to touch all children anyway. ([Tree-sitter][1])
* If you’re tempted to do `for i in range(node.child_count): node.child(i)` on wide nodes: don’t—`child(i)` is log(i) and `walk()` exists specifically for long lists / deep recursion. ([Tree-sitter][2])

---

## 6) Query language: syntax, structure, and “how to write queries that scale”

### Quick gotchas (high-impact)

* **Queries are pattern lists:** each pattern is an S-expression matching a subtree shape (node type + optional child patterns). ([Tree-sitter][3])
* **Specificity matters:** use **fields** (`name:` / `body:`) and **negated fields** (`!type_parameters`) to avoid match explosions. ([Tree-sitter][3])
* **Named vs anonymous:** named nodes use `(node_type)`; anonymous tokens use **double-quoted literals** (`"!="`, `"return"`). ([Tree-sitter][3])
* **Wildcards differ:** `_` matches named *or* anonymous; `(_)` matches named-only. ([Tree-sitter][3])
* **ERROR/MISSING are queryable:** `(ERROR)` and `(MISSING ...)` are first-class patterns for diagnostics and partial parses. ([Tree-sitter][3])
* **Non-local patterns are expensive:** patterns with multiple roots disable range-based optimizations; enforce “rooted, local” patterns unless you explicitly need non-local semantics. ([Tree-sitter][4])

(Your template already summarizes the DSL essentials—fields, negated fields, wildcards, ERROR/MISSING, predicates, non-local caution—so this section expands that into a complete “operator + scaling” reference.)

---

### 6.0 Mental model (execution + failure modes)

A query is compiled into N patterns; the engine searches for pattern roots, then validates child constraints, then yields *matches* (bindings of capture names → nodes). Under-constrained patterns (wildcards, missing fields, absent anchors) create combinatorial match sets and push you into `match_limit`/cancellation territory—so the primary “scaling” move is **structural specificity**. ([Tree-sitter][3])

---

### 6.1 Core syntax primitives (what the parser actually understands)

#### Node patterns + child patterns

* `(binary_expression (number_literal) (number_literal))` matches a node + its immediate child sequence. Children can be omitted to relax constraints. ([Tree-sitter][3])

#### Fields (high-value specificity)

* Prefix child patterns with `field_name:` to constrain by grammar field, not by positional coincidence. ([Tree-sitter][3])

#### Negated fields (absence constraints)

* `!field_name` inside a parent pattern asserts the field is absent. ([Tree-sitter][3])

#### Anonymous nodes (token literals)

* Use `"..."` to match anonymous tokens (operators, punctuation-like literals) because parenthesized node syntax applies to named nodes. ([Tree-sitter][3])

#### Special nodes

* Wildcards: `_` vs `(_)` difference (named-only is often safer). ([Tree-sitter][3])
* `(ERROR)` + `(MISSING ...)` for robust tooling on incomplete code. ([Tree-sitter][3])
* Supertypes: `(expression)` matches any subtype; refine via `supertype/subtype`. ([Tree-sitter][3])

---

### 6.2 Operators (expressiveness + where scaling collapses)

#### Captures (`@name`)

Captures label nodes you want returned; capture names attach to the *preceding* node pattern. ([Tree-sitter][5])

#### Quantifiers (`+`, `*`, `?`)

* Postfix repetition operators match repeating sibling sequences: `+` (1+), `*` (0+), `?` (optional). ([Tree-sitter][5])
* Quantifiers can apply to **groups**, not just single nodes. ([Tree-sitter][5])

Scaling implication: quantified captures can balloon result cardinality; prefer to quantify *structure* while capturing only the subset you truly need.

#### Grouping sibling sequences

Parentheses can group sibling patterns; quantifiers can be applied to the group. ([Tree-sitter][5])

#### Alternations (`[ ... ]`)

Square brackets specify “match any of these patterns”, analogous to regex character classes. ([Tree-sitter][5])

#### Anchors (`.`) — the single most important “anti-duplication” operator

Anchor `.` constrains *how* child patterns align:

* Before first child: match only if the child is the **first named** node. ([Tree-sitter][5])
* After last child: match only if the child is the **last named** node. ([Tree-sitter][5])
* Between children: require **immediate siblings** (prevents non-consecutive pairing explosions). ([Tree-sitter][5])
  Anchor constraints ignore anonymous nodes. ([Tree-sitter][5])

---

### 6.3 Predicates & directives (semantic filtering + metadata)

#### Predicates (`#... ?`)

Predicate S-expressions add conditions; CLI-default families include `eq?`, `match?`, `any-of?`, `is?` (with `not-` / `any-` variants where applicable). ([Tree-sitter][6])
Key scaling rule for quantified captures: by default, quantified captures satisfy predicates only if **all** captured nodes satisfy; `#any-*` variants loosen that to “any”. ([Tree-sitter][6])

#### Directives (`#... !`)

Directives attach metadata or post-process captured text:

* `#set! key value` sets arbitrary key/value metadata (notably `injection.language`). ([Tree-sitter][6])
* `#select-adjacent!` restricts capture text to nodes adjacent to another capture. ([Tree-sitter][6])
* `#strip!` regex-removes text from a capture’s text. ([Tree-sitter][6])

Engine boundary: predicates/directives aren’t executed by the C library; they’re exposed structurally and may be implemented by higher-level bindings. ([Tree-sitter][6])

---

### 6.4 “Write queries that scale” (mechanical rules, enforceable by tooling)

#### Rule 1: Enforce rooted, local patterns (unless you *really* need non-local)

Non-local patterns disable some optimizations for range-scoped execution. In py-tree-sitter you can introspect this per-pattern. ([Tree-sitter][4])

```python
from tree_sitter import Query

def lint_patterns(q: Query, *, allow_non_local: bool = False) -> None:
    for i in range(q.pattern_count):
        if not q.is_pattern_rooted(i):
            raise ValueError(f"pattern[{i}] is not rooted (multiple roots) -> likely costly")
        if q.is_pattern_non_local(i) and not allow_non_local:
            raise ValueError(f"pattern[{i}] is non-local -> disables some range optimizations")
```

(Use this as a “query pack contract” for large-codebase tooling.)

#### Rule 2: Replace positional coincidences with fields

Fields drastically prune search space by binding to grammar-defined slots (`name:`, `body:`, `left:`, etc.). ([Tree-sitter][3])

#### Rule 3: Use anchors to prevent quadratic pairing

If your pattern can align with many sibling choices, it will. Put `.` between siblings when you mean adjacency; use leading/trailing `.` when you mean first/last. ([Tree-sitter][5])

#### Rule 4: Quantify structure, not captures

Use `+/*/?` to describe optional/repeating structure, but keep captures on the “semantic leaves” you’ll actually index (names, identifiers, call sites), otherwise result sets explode. ([Tree-sitter][5])

#### Rule 5: Prefer supertypes + alternations to mega-pattern duplication

Use supertypes `(expression)` when grammars define them, and alternations for small “either/or” sets—both keep query packs maintainable without forcing non-local structure. ([Tree-sitter][3])

#### Rule 6: Treat predicates as *late filters*

Predicates are powerful for text-level constraints (`#eq?`, `#match?`, `#any-of?`), but structural pruning (fields/anchors) is usually cheaper than predicate-heavy scanning. (Predicates are applied by higher-level code, not the core C engine.) ([Tree-sitter][6])

---

If you want next: I can continue with **7) Query compilation & introspection** (using `Query.capture_quantifier`, `pattern_settings`, `disable_capture/pattern`, etc.) as a “query pack engineering” section, which pairs naturally with the scaling rules above. ([Tree-sitter][4])

[1]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.TreeCursor.html "TreeCursor — py-tree-sitter 0.25.2 documentation"
[2]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.Node.html "Node — py-tree-sitter 0.25.2 documentation"
[3]: https://tree-sitter.github.io/tree-sitter/using-parsers/queries/1-syntax.html "Basic Syntax - Tree-sitter"
[4]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.Query.html "Query — py-tree-sitter 0.25.2 documentation"
[5]: https://tree-sitter.github.io/tree-sitter/using-parsers/queries/2-operators.html "Operators - Tree-sitter"
[6]: https://tree-sitter.github.io/tree-sitter/using-parsers/queries/3-predicates-and-directives.html "Predicates and Directives - Tree-sitter"


## 7) Query compilation & introspection (treat queries as data)

### Quick gotchas (high-impact)

* **Compilation is explicit**: `Query(language, source)` parses/validates the query and raises `QueryError` on any creation failure. ([Tree-sitter][1])
* **Predicates aren’t “free-form”**: py-tree-sitter documents a default supported predicate set (`#eq?/#match?/#any-of?/#is?/#set!` plus negations/any-variants where listed). Anything else must be implemented via a custom `QueryPredicate` passed at execution time. ([Tree-sitter][1])
* **Non-local patterns are a perf cliff**: `Query.is_pattern_non_local(i)` identifies patterns that disable certain range-execution optimizations; lint and reject by default for large-codebase indexing. ([Tree-sitter][1])
* **Feature-toggles are destructive**: `disable_capture(...)` / `disable_pattern(i)` cannot be undone on that `Query` instance; treat them as “one-way specialization”. ([Tree-sitter][1])
* **`is_pattern_guaranteed_at_step(offset)` uses a byte offset into the *query source*** (the “step” is specified by its byte offset in the query text), not an input-document offset—use it only for query-engine introspection/optimization tooling. ([GitHub][2])

---

### 7.0 Mental model

A compiled `Query` is an **IR**: a list of **patterns** (indexed `0..pattern_count-1`), a canonicalized **capture table** (indexed `0..capture_count-1`), and a **string literal table** (indexed `0..string_count-1`). Your tooling should *never* treat raw query source as opaque text once compiled—interrogate the IR, derive contracts, and enforce invariants before running it on trees. ([Tree-sitter][1])

---

### 7.1 Compilation: build once, then validate “query pack contracts”

```python
from tree_sitter import Query, QueryError

def compile_query(lang, source: str) -> Query:
    try:
        return Query(lang, source)
    except QueryError as e:
        # py-tree-sitter exposes QueryError as a ValueError subclass; message may be your only payload.
        raise ValueError(f"Invalid Tree-sitter query: {e}") from e
```

`Query(language, source)` is the canonical constructor; `QueryError` is the dedicated exception type. ([Tree-sitter][1])

**Default predicate surface (compile-time expectations)**

* py-tree-sitter documents the built-ins as:
  `#eq?/#not-eq?/#any-eq?/#any-not-eq?`,
  `#match?/#not-match?/#any-match?/#any-not-match?`,
  `#any-of?/#not-any-of?`,
  `#is?/#is-not?`,
  `#set!`. ([Tree-sitter][1])

---

### 7.2 Query “IR” introspection: counts + stable lookup tables

**Shape**

* `query.pattern_count`, `query.capture_count`, `query.string_count`. ([Tree-sitter][1])

**Capture table**

* `query.capture_name(capture_index) -> str`. ([Tree-sitter][1])

**String literal table**

* `query.string_value(string_index) -> str`. ([Tree-sitter][1])

**Agent-facing precomputation (minimize runtime branching)**

```python
def build_capture_index(q: Query) -> dict[str, int]:
    return {q.capture_name(i): i for i in range(q.capture_count)}
```

This lets downstream code treat captures as integer IDs (fast dict lookup once, then stable ints).

---

### 7.3 Pattern slicing: recover per-pattern source for debugging, linting, and provenance

* `start_byte_for_pattern(i)` / `end_byte_for_pattern(i)` give the byte offsets where that pattern lives inside the query’s **source**. ([Tree-sitter][1])

**Use**: build “pattern provenance” artifacts (“this match came from pattern i: <exact text>”), and enable pattern-level debugging without splitting the query file manually.

```python
def pattern_source_slices(q: Query, source: str) -> list[str]:
    b = source.encode("utf8")
    out = []
    for i in range(q.pattern_count):
        s = q.start_byte_for_pattern(i)
        e = q.end_byte_for_pattern(i)
        out.append(b[s:e].decode("utf8"))
    return out
```

---

### 7.4 Property metadata: `#set!` settings and `#is?/#is-not?` assertions

**Settings (properties set by `#set!`)**

* `pattern_settings(i) -> dict[str, str|None]` (keys with optional values). ([Tree-sitter][1])
  This is how injection/highlight/tag query packs attach structured metadata to patterns.

**Assertions (property checks by `#is?` / `#is-not?`)**

* `pattern_assertions(i) -> dict[str, tuple[str|None, bool]]` where the tuple is *(optional property value, positive?)*. ([Tree-sitter][1])

**Practical contract**

* Treat settings/assertions as **static schema** for your query pack:

  * validate that required keys exist (`injection.language`, `kind`, `role`, etc.)
  * reject “unknown” keys (to prevent silent typos)
  * surface settings in your indexing outputs as “pattern metadata”.

---

### 7.5 Structural quality gates: rootedness, locality, and early guarantees

**Rootedness**

* `is_pattern_rooted(i)` → single root node vs multiple roots. ([Tree-sitter][1])

**Non-locality**

* `is_pattern_non_local(i)` flags “multiple roots + matches within repeating sequences” and explicitly notes it disables some range-execution optimizations. ([Tree-sitter][1])

**Guarantee-at-step**

* `is_pattern_guaranteed_at_step(offset)` exists at the `Query` level; the “step” is specified by **byte offset in the query’s source code** (query text). ([GitHub][2])

**Canonical “query pack lint”**

```python
def lint_query_pack(q: Query, *, allow_non_local: bool = False) -> None:
    for i in range(q.pattern_count):
        if not q.is_pattern_rooted(i):
            raise ValueError(f"pattern[{i}] not rooted (multi-root): reject for scalable packs")
        if q.is_pattern_non_local(i) and not allow_non_local:
            raise ValueError(f"pattern[{i}] non-local: reject (range optimizations disabled)")
```

This is the mechanical rule that keeps `QueryCursor.set_*_range(...)` strategies viable at scale. ([Tree-sitter][1])

---

### 7.6 Capture cardinality analysis: `capture_quantifier(pattern_i, capture_i)`

* `capture_quantifier(pattern_index, capture_index)` returns the quantifier for that capture in that pattern (optional/repeating vs exactly-one). ([Tree-sitter][1])

**Why you care**

* It lets you derive a **typed match schema**:

  * captures that are `?` are “maybe”
  * captures that are `*`/`+` are “list”
  * captures that are “one” are “exactly-one”
* That schema is what lets LLM agents generate stable downstream code without guessing whether `captures["@x"]` is scalar vs list.

---

### 7.7 Specialization toggles: disabling captures/patterns (one-way)

* `disable_capture(name)` and `disable_pattern(i)` disable execution of that capture/pattern; **no undo**. ([Tree-sitter][1])

**Recommended practice**

* Treat “toggle variants” as separate compiled queries (or recompile from source) so you can produce:

  * a “full fidelity” query pack
  * a “fast path” query pack (fewer patterns/captures)
  * a “safe mode” pack (only rooted/local patterns)

---

### 7.8 Custom predicate execution: `QueryPredicate` protocol + how to wire it

py-tree-sitter defines `QueryPredicate` as a `Protocol` with a `__call__` signature:
`(predicate: str, args: list[tuple[str, Literal["capture","string"]]], pattern_index: int, captures: dict[str, list[Node]]) -> bool`. ([Tree-sitter][3])

**Execution wiring**

* `QueryCursor.captures(..., predicate=..., ...)` and `QueryCursor.matches(..., predicate=..., ...)` accept a `predicate` callable. ([Tree-sitter][4])

**Minimal custom predicate (example: `#has-capture? @cap`)**

```python
from tree_sitter import QueryCursor

def my_predicate(predicate, args, pattern_index, captures) -> bool:
    if predicate == "has-capture?":
        # args: list of (value, kind) where kind is "capture" or "string"
        ((name, kind),) = args
        if kind != "capture":
            return False
        return bool(captures.get(name))
    # unknown predicate => default false (or raise if you want strictness)
    return False

cursor = QueryCursor(query)
matches = cursor.matches(tree.root_node, predicate=my_predicate)
```

**Operational posture**

* Use custom predicates to keep query text expressive *without* exploding patterns, but keep the predicate implementation branch-light and allocation-free (it sits on the hot path). ([Tree-sitter][3])

---

If you want next, the natural continuation is: **8) Query execution playbooks** that combine (a) the introspection gates above (rooted/local), (b) `set_byte_range`/`set_containing_*`, (c) `match_limit`/progress callbacks, into a single “bounded query runner” utility that agents can reuse across indexing tasks.

[1]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.Query.html "Query — py-tree-sitter 0.25.2 documentation"
[2]: https://github.com/tree-sitter/py-tree-sitter/releases "Releases · tree-sitter/py-tree-sitter · GitHub"
[3]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.QueryPredicate.html "QueryPredicate — py-tree-sitter 0.25.2 documentation"
[4]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.QueryCursor.html "QueryCursor — py-tree-sitter 0.25.2 documentation"


## 8) Query execution engine (QueryCursor): range limiting, match limits, and progress callbacks

### Quick gotchas (high-impact)

* **QueryCursor is the execution surface in 0.25+**: scoping (`set_*_range`), throttling (`match_limit`), and runners (`captures/matches`) live on `QueryCursor`, not `Query`.
* **Range scoping is intersection-based**: `set_byte_range` / `set_point_range` return matches that *intersect* the window (captures may spill outside). ([Tree-sitter][1])
* **Containment scoping is different and version-gated**: `set_containing_*_range` enforces “all nodes fully contained” and is **added in 0.26.0**; combine it with intersection windows for “needle-on-line, fully-contained-in-band” semantics. ([Tree-sitter][1])
* **`matches()` shape can silently drop duplicates**: it returns a `dict[capture_name -> Node]`; if a pattern yields multiple captures with the same name, you only keep the last one (doc hint explicitly contrasts `captures()` vs `matches()`). Prefer unique capture names per pattern or use `captures()`. ([Tree-sitter][1])
* **Always check `did_exceed_match_limit`** after execution; treat results as *partial* if true. ([Tree-sitter][1])
* **`timeout_micros` is legacy**: py-tree-sitter deprecates `QueryCursor.timeout_micros` in favor of `progress_callback` on `matches()` / `captures()`. ([GitHub][2])
* **Cancellation polarity is “True means cancel”** (counterintuitive; called out by core maintainers). ([GitHub][3])

---

### 8.0 Mental model (how QueryCursor actually “runs”)

A compiled `Query` is an IR of patterns/captures; a `QueryCursor` is the **stateful search machine** that:

1. chooses candidate start positions (pattern roots),
2. expands/validates constraints (fields, anchors, predicates),
3. accumulates in-progress partial matches (bounded by `match_limit`),
4. emits either **capture-oriented** results (`captures`) or **match-oriented** results (`matches`). ([Tree-sitter][1])

This is why “scaling” is mostly about: **(a) windowing**, **(b) pruning**, **(c) throttling**, **(d) cooperative cancellation**—exactly the knobs your template calls out (`set_*_range`, `match_limit`, `progress_callback`).

---

### 8.1 Construction + stable knobs

**Constructor**

```python
cursor = QueryCursor(query, match_limit=10_000, timeout_micros=None)
```

The documented signature is `QueryCursor(query, *, match_limit=None, timeout_micros=None)`. ([Tree-sitter][1])

**Stable properties**

* `match_limit`: “maximum number of in-progress matches” (controls memory/work explosion). ([Tree-sitter][1])
* `did_exceed_match_limit`: indicates last execution exceeded the limit. ([Tree-sitter][1])

**Core-limit reality (important for defensive design)**
Tree-sitter’s core cursor enforces `match_limit > 0` and `<= 65536` (Rust binding documents the underlying constraint; treat it as a hard ceiling when designing “max safety” configs). ([Docs.rs][4])

---

### 8.2 Execution outputs: `captures()` vs `matches()` (choose based on downstream contract)

#### `captures(node, predicate=None, progress_callback=None) -> dict[str, list[Node]]`

* Returns **all captures grouped by capture name** across all matches. ([Tree-sitter][1])
* Best when your downstream is “index all X-nodes” (flat inverted index by capture key).
* Risk: can allocate huge `list[Node]` per capture in large trees; pair with windowing + match limits.

#### `matches(node, predicate=None, progress_callback=None) -> list[tuple[int, dict[str, Node]]]`

* Returns **match groups** keyed by pattern index; each match contains a `dict` mapping capture name to node. ([Tree-sitter][1])
* Best when you need relational structure (“definition + name + body” as one unit).
* **Critical nuance**: the dict shape implies **one Node per capture name** in that match. If your pattern can bind the same capture name multiple times, you’ve designed a lossy representation. The docs’ hint explicitly pushes you toward `captures()` if you need “all captures”. ([Tree-sitter][1])

**Rule (mechanical, enforceable):** in any query pack intended for `matches()`, enforce “unique capture names per pattern” (or suffix them: `@arg.0`, `@arg.1`, …).

---

### 8.3 Range limiting: intersection windows (fast “changed_ranges → scoped query”)

**Byte window**

* `set_byte_range(start, end)` sets a byte window for execution; raises `ValueError` if start > end. ([Tree-sitter][1])
* Semantics: returns matches that **intersect** the window; captures may lie outside. ([Tree-sitter][1])

**Point window**

* `set_point_range(start_point, end_point)` is the analogous row/col window; also intersection-based; also raises `ValueError` on inverted ranges. ([Tree-sitter][1])

**Agent-grade idiom (byte-first)**

```python
def run_scoped_matches(cursor, root, r):
    cursor.set_byte_range(r.start_byte, r.end_byte)
    out = cursor.matches(root)
    return out, cursor.did_exceed_match_limit
```

Your template’s intent is exactly this: compute syntactic deltas (`changed_ranges`) then query only inside those deltas. 

---

### 8.4 Containment windows (0.26+): “no spill” semantics + two-tier scoping

`set_containing_byte_range(start, end)` / `set_containing_point_range(start, end)` restrict matches to those where **all nodes are fully contained** within the given range; both can be combined with intersection windows (doc explicitly gives the “intersect line 5000, contained in lines 4500–5500” pattern). ([Tree-sitter][1])

**Version-gated usage**

```python
def set_containing_if_supported(cursor, start_b, end_b):
    fn = getattr(cursor, "set_containing_byte_range", None)
    if fn is not None:
        fn(start_b, end_b)  # 0.26+
```

**Practical pattern (“needle-on-line, fully-contained-in-band”)**

* Intersection window: the tight “what just changed” slice.
* Containing window: a slightly expanded semantic band (e.g., enclosing statement/block), ensuring matches don’t leak across boundaries while still being discoverable.

---

### 8.5 Depth pruning: `set_max_start_depth` (cheap “don’t even start deep” guardrail)

py-tree-sitter exposes `set_max_start_depth(max_start_depth)`. ([Tree-sitter][1])

Core semantics (from tree-sitter Rust docs; this is the most explicit explanation you can treat as “engine truth”):

* It prevents the cursor from exploring children beyond a certain depth for **pattern root starts**; patterns can still inspect deeper nodes depending on their structure.
* `0` has a special “stay on node; destructure via captures” behavior.
* Set to `None` to remove the maximum start depth. ([Docs.rs][4])

**Use cases**

* “Top-level only” packs (imports/defs/module docs).
* Safety brake on pathological deeply-nested expression trees.

---

### 8.6 Match throttling: designing for partial results (not failure)

`match_limit` bounds the number of **in-progress** partial matches the engine will track; exceeding it flips `did_exceed_match_limit`. ([Tree-sitter][1])

**Operational contract**

* If `did_exceed_match_limit == True`: results are **incomplete**; you must degrade:

  * shrink windows (byte ranges),
  * simplify patterns (anchors, fields, avoid non-local),
  * split query packs (coarse filter → refined pass on smaller subtrees),
  * tighten `set_max_start_depth`.

---

### 8.7 Cooperative cancellation: `progress_callback` (preferred over timeouts)

py-tree-sitter wires `progress_callback` into `QueryCursor.captures(...)` and `QueryCursor.matches(...)`. ([Tree-sitter][1])

**Cancellation polarity**
Core discussion: returning `true` means **cancel**, `false` means **continue**. ([GitHub][3])

**What state you get**
The engine maintains a query progress state with at least a **current byte offset** (core `QueryCursorState.current_byte_offset()`). ([Docs.rs][5])

**Robust callback (works across bindings that expose state differently)**

```python
def cancel_on_byte_budget(max_byte: int):
    def cb(state) -> bool:
        # try common shapes: method, attribute, or raw int-like
        if hasattr(state, "current_byte_offset"):
            cur = state.current_byte_offset() if callable(state.current_byte_offset) else state.current_byte_offset
        elif hasattr(state, "CurrentByteOffset"):
            cur = state.CurrentByteOffset
        else:
            cur = state
        return int(cur) >= max_byte  # True => cancel
    return cb
```

**Deprecation posture**
py-tree-sitter explicitly deprecates `QueryCursor.timeout_micros` and points you to `progress_callback` on `matches()` / `captures()` instead. ([GitHub][2])

---

### 8.8 Reference “bounded runner” (window + depth + throttle + cancellation)

This is the minimal reusable utility most LLM agents end up wanting (single place to encode all guardrails):

```python
from __future__ import annotations
from dataclasses import dataclass

@dataclass(frozen=True)
class QueryRunResult:
    matches: list[tuple[int, dict[str, object]]]
    exceeded_match_limit: bool
    cancelled: bool  # best-effort signal; treat as “may be partial”

def run_bounded(
    cursor,
    root,
    *,
    byte_ranges: list[tuple[int, int]] | None = None,
    containing_range: tuple[int, int] | None = None,  # (start,end), 0.26+
    max_start_depth: int | None = None,
    progress_callback=None,
) -> QueryRunResult:
    if max_start_depth is not None:
        cursor.set_max_start_depth(max_start_depth)

    if containing_range is not None:
        fn = getattr(cursor, "set_containing_byte_range", None)
        if fn is not None:
            fn(*containing_range)

    cancelled = False
    out: list[tuple[int, dict[str, object]]] = []

    ranges = byte_ranges or [(0, 2**63 - 1)]
    for (s, e) in ranges:
        cursor.set_byte_range(s, e)
        try:
            out.extend(cursor.matches(root, progress_callback=progress_callback))
        except Exception:
            # py-tree-sitter does not document a dedicated “cancel exception” type;
            # treat any interruption here as cancellation and return partials.
            cancelled = True
            break

        if cursor.did_exceed_match_limit:
            break

    return QueryRunResult(out, cursor.did_exceed_match_limit, cancelled)
```

This is the “engine harness” that makes everything else (incremental parsing, query-pack linting, indexing pipelines) predictable under worst-case inputs—exactly the scale/cancellation posture your template points toward. 

---

If you want next, the natural follow-on section is **9) Query-powered subsystems** (highlights/locals/injections/tags) written as “query pack engineering”: file layout, capture vocabularies, metadata via `#set!`, and how to test packs mechanically.

[1]: https://tree-sitter.github.io/py-tree-sitter/classes/tree_sitter.QueryCursor.html "QueryCursor — py-tree-sitter 0.25.2 documentation"
[2]: https://github.com/tree-sitter/py-tree-sitter/releases "Releases · tree-sitter/py-tree-sitter · GitHub"
[3]: https://github.com/tree-sitter/tree-sitter/discussions/4312 "Parser and query `progress_callback` behavior / name is confusing · tree-sitter tree-sitter · Discussion #4312 · GitHub"
[4]: https://docs.rs/tree-sitter/latest/tree_sitter/struct.QueryCursor.html "QueryCursor in tree_sitter - Rust"
[5]: https://docs.rs/tree-sitter/latest/src/tree_sitter/lib.rs.html "lib.rs - source"


## 9) Query-powered subsystems: highlighting, locals, and injections as reusable indexing primitives

### Quick gotchas (high-impact)

* Tree-sitter’s “syntax highlighting system” is really **three query packs** (highlights/locals/injections) plus an execution engine (`tree-sitter-highlight`, used by GitHub.com for multiple languages) that interprets those packs. ([Tree-sitter][1])
* Query pack locations are convention-based: `tree-sitter.json` points to `queries/highlights.scm`, `queries/locals.scm`, `queries/injections.scm` by default. ([Tree-sitter][1])
* **Highlights** capture names are arbitrary (e.g., `@keyword`, `@function`, dot-qualified like `@function.builtin`) → treat capture names as your *semantic token classes*. ([Tree-sitter][1])
* **Locals** capture names are fixed semantics: `@local.scope`, `@local.definition`, `@local.reference`, plus `@ignore` for exclusion; **ordering matters** (ignore patterns must come before broader tagging patterns). ([Tree-sitter][1])
* Locals info can feed highlights via `(#is-not? local)` to prevent “locals are method names” style mis-highlights. ([Tree-sitter][1])
* **Injections** are declared by captures `@injection.content` (+ optional `@injection.language`) and controlled by pattern properties: `injection.language`, `injection.combined`, `injection.include-children`, `injection.self`, `injection.parent`. ([Tree-sitter][1])
* For injection execution in “real systems”, **`included_ranges` is the engine primitive** that lets you parse disjoint subranges while still producing a tree whose node ranges align with the *full* host document. ([Docs.rs][2])

---

### 9.0 Mental model (agent-ready)

Treat {highlights, locals, injections} as **declarative indexing packs** over a host CST:

* **Highlights pack ⇒** `(byte_span → class)` annotations (arbitrary capture taxonomy). ([Tree-sitter][1])
* **Locals pack ⇒** `(scope intervals, definitions, references)` events; you can resolve references → defs by name with a scope stack. ([Tree-sitter][1])
* **Injections pack ⇒** `(host span → embedded language)` + parse policy (combined/include-children/self/parent). ([Tree-sitter][1])

Your Python pipeline is then: `parse host → run query packs via QueryCursor → emit typed artifacts`.

---

### 9.1 Pack discovery + file conventions (how the ecosystem packages semantics)

Tree-sitter CLI expects (and many grammars ship):

* a `tree-sitter.json` describing language detection + query paths: `highlights`, `locals`, `injections` (defaults under `queries/`). ([Tree-sitter][1])
* `.scm` query files (chosen because the syntax is Lisp-like; extension is a convention). ([Tree-sitter][1])
* optional `injection-regex` in `tree-sitter.json` to decide if a language is eligible as an injection target. ([Tree-sitter][1])

For Python tooling, you typically ignore the CLI config layer and just ship/query-load those `.scm` files directly, but the conventions matter because they define the “standard packs” most grammars provide. ([Tree-sitter][1])

---

### 9.2 Highlights query = structural tokenizer (arbitrary capture taxonomy)

**Spec**

* Highlights query “uses captures to assign arbitrary highlight names”; common names include `keyword`, `function`, `type`, `property`, `string`, and dot-qualified names like `function.builtin`. ([Tree-sitter][1])

**Example (from docs)**

```scm
; highlights.scm
"func" @keyword
"return" @keyword
(type_identifier) @type
(int_literal) @number
(function_declaration name: (identifier) @function)
```

([Tree-sitter][1])

**Execution (py-tree-sitter idiom)**

```python
from tree_sitter import Query, QueryCursor

q_hi = Query(lang, open("queries/highlights.scm", "r", encoding="utf8").read())
cur = QueryCursor(q_hi)

caps = cur.captures(tree.root_node)  # dict[capture_name -> list[Node]]

# Flatten into a stable, byte-sorted token stream:
tokens = [
    (n.start_byte, n.end_byte, cap)
    for cap, nodes in caps.items()
    for n in nodes
]
tokens.sort()
```

**Scaling rules (highlights)**

* Design capture names as a **stable semantic vocabulary** for downstream indexing (LLM retrieval, lint classes, UI affordances). The docs explicitly treat capture names as “arbitrary highlight names” → you own the taxonomy. ([Tree-sitter][1])
* Prefer **field constraints** and **anchors** (from §6) to prevent “match explosion”; highlights is where overly-generic patterns first hurt. (You already have the QueryCursor throttles in §8; treat them as guardrails, not as your primary control plane.)

---

### 9.3 Locals query = declarative scope graph scaffold (fixed capture semantics)

**Spec**
Locals query uses a fixed capture vocabulary:

* `@local.scope`: node introduces a local scope
* `@local.definition`: node contains a definition name within current scope
* `@local.reference`: node contains a name that may refer to an earlier definition in an enclosing scope
* `@ignore`: exclusion capture; must be placed *before* broader tagging patterns to take effect ([Tree-sitter][1])

Tree-sitter’s highlighting system uses these to track scopes/definitions at positions and to keep references/defs colored consistently. ([Tree-sitter][1])

**Highlights↔locals coupling**
You can disable highlight patterns for nodes identified as locals via `(#is-not? local)` (used in the docs’ Ruby example). ([Tree-sitter][1])

**Indexing interpretation (Python agent posture)**
Treat locals captures as a *declarative spec* for building:

* a **scope stack** (interval nesting via `@local.scope` nodes’ byte ranges),
* a **definition table per scope** (`name → def_node`),
* a **resolved reference stream** (`ref_node → def_node|unresolved`) using “nearest enclosing def by text” resolution (Tree-sitter’s own highlight behavior matches by node text). ([Tree-sitter][1])

Minimal resolution skeleton (byte-ordered, scope-stack):

```python
from collections import defaultdict

q_loc = Query(lang, open("queries/locals.scm", "r", encoding="utf8").read())
cur = QueryCursor(q_loc)
caps = cur.captures(tree.root_node)

scopes = caps.get("local.scope", [])
defs   = caps.get("local.definition", [])
refs   = caps.get("local.reference", [])
ign    = set(caps.get("ignore", []))

# Pre-index by byte ranges for fast “which scopes contain me?”
# (implementation detail: sort scopes by start_byte; build an interval stack).

def node_text(n) -> str:
    # Prefer slicing bytes for robustness; Node.text is OK if the tree wasn't edited.
    return src_bytes[n.start_byte:n.end_byte].decode("utf8", errors="replace")

# Pseudocode:
# - walk events in start_byte order (enter/exit scopes, defs, refs)
# - maintain stack: list[dict[name -> def_node]]
# - on def: stack[-1][name] = def_node
# - on ref: resolve by searching stack from inner→outer
```

**Practical constraint:** locals gives you *just enough* to build a cross-language “good-enough” symbol table; anything requiring type/flow sensitivity is downstream of this scaffold.

---

### 9.4 Injections query = embedded-language parse planner (captures + properties)

**Spec**
Injections are declared by:

* `@injection.content`: node whose contents should be re-parsed as another language
* `@injection.language`: node whose text may contain the language name to use ([Tree-sitter][1])

Pattern properties control behavior:

* `injection.language`: hard-code language name
* `injection.combined`: parse all matching nodes as one nested document
* `injection.include-children`: include child text in the injected document (default excludes children)
* `injection.self`: parse using the node’s own language
* `injection.parent`: parse using the parent’s language ([Tree-sitter][1])

Example (docs): derive language name from a captured node, or force via `#set! injection.language "ruby"`. ([Tree-sitter][1])

**Execution strategy A (recommended): parse embedded language over host bytes using `included_ranges`**
This mirrors the *core* library capability: you can parse a portion (or multiple disjoint portions) of a document and still get a tree whose ranges line up with the full document. ([Docs.rs][2])

Sketch:

```python
from tree_sitter import Query, QueryCursor, Parser, Range

inj_q = Query(host_lang, open("queries/injections.scm", "r", encoding="utf8").read())
cur = QueryCursor(inj_q)

# Use matches() so you get pattern_index (needed to read pattern settings like injection.language/combined)
for pattern_i, m in cur.matches(host_tree.root_node):
    content = m.get("injection.content")
    if content is None:
        continue

    settings = inj_q.pattern_settings(pattern_i)  # §7: settings reflect #set! properties
    forced = settings.get("injection.language")

    lang_name = forced
    if lang_name is None:
        lang_node = m.get("injection.language")
        if lang_node is not None:
            lang_name = src_bytes[lang_node.start_byte:lang_node.end_byte].decode("utf8", "replace")

    # Resolve lang_name -> Language via your registry (normalize/casefold as needed).
    embedded_lang = registry.resolve(lang_name)

    # Build included range(s) for the embedded parser.
    # Minimal form: parse exactly the content node's byte span as embedded language.
    r = Range(content.start_point, content.end_point, content.start_byte, content.end_byte)

    p = Parser(embedded_lang, included_ranges=[r])
    embedded_tree = p.parse(src_bytes)

    # embedded_tree node ranges are in host-document coordinates (key payoff).
```

**Execution strategy B (fallback): extract bytes and parse standalone**
If you can guarantee the injected text is contiguous and you are not excluding children, you can parse `content.text` standalone and then map coordinates back with offsets (see §3 `root_node_with_offset`). (This breaks down when `injection.include-children` is false, because you’re effectively parsing a “hole-punched” string.)

**Handling `injection.combined`**

* If `injection.combined` is set, treat all matching `@injection.content` ranges as one logical embedded document. ([Tree-sitter][1])
* With strategy A, this becomes “just” multiple disjoint `included_ranges` (ordered, non-overlapping) on a single embedded parser run—exactly what the core API supports. ([Docs.rs][2])

**Handling `injection.include-children`**

* When unset, injected content excludes child nodes’ text; when set, include entire node text. ([Tree-sitter][1])
* If you implement injections yourself (strategy B), “exclude children” implies splitting content into a set of non-child byte ranges (again: `included_ranges` is the clean primitive).

---

### 9.5 Unit testing query packs (highlight-level regression harness)

Tree-sitter provides a built-in way to verify syntax highlighting results:

* tests are source files with specially formatted comments asserting highlight scopes, stored in `test/highlight` in a grammar repo ([Tree-sitter][1])
* two assertion syntaxes:

  * `^` caret asserts scope at specific columns (multiple carets = multiple columns)
  * `<-` arrow asserts scope at the comment’s column
  * `!` negates a selector (e.g., `!keyword`) ([Tree-sitter][1])

For “query pack engineering”, these tests are your *contract suite* for highlights/locals/injections changes: they catch drift immediately when you tweak patterns or upgrade grammars.

---

If you want next, the natural continuation is **10) Static node types (`node-types.json`)** as an enforcement tool for query packs (compile-time validation of node/field names + typed wrappers + “query pack CI”).

[1]: https://tree-sitter.github.io/tree-sitter/3-syntax-highlighting.html "Syntax Highlighting - Tree-sitter"
[2]: https://docs.rs/tree-sitter/%2A/tree_sitter/struct.Parser.html?utm_source=chatgpt.com "Parser in tree_sitter - Rust"


## 10) Static node types: leveraging `node-types.json` for typed tooling and codegen

### Quick gotchas (high-impact)

* `node-types.json` is a **generated** grammar artifact: a **complete, machine-readable “node schema”** for every syntax node type in a grammar. ([Tree-sitter][1])
* A node type’s identity is the pair **(`"type"`, `"named"`)**; no two top-level entries should share both values. ([Tree-sitter][1])
* `"named": false` node types correspond to **string-literal / anonymous tokens** (the same category you must write in queries using **double quotes**, not parenthesized node syntax). ([Tree-sitter][1])
* Internal node structure is described via **fields** + **children**, each expressed as a **child-type set** with `required`, `multiple`, and a `types[]` union. ([Tree-sitter][1])
* “Supertype nodes” only appear if a hidden rule is added to the grammar’s **`supertypes` list**; these entries use `"subtypes"` to enumerate the union they represent. ([Tree-sitter][1])

---

### 10.0 Mental model

Treat `node-types.json` as the grammar’s **static structural contract**:

* **What node kinds exist** (including anonymous token kinds),
* **Which fields exist per node**, with **cardinality** (`required`/`multiple`) and **allowed types**,
* **Which positional named children exist** when not bound to fields,
* **Which supertypes compress unions** into a single abstract category (and which subtypes they wrap). ([Tree-sitter][1])

This unlocks: *typed wrappers*, *visitor/transformer codegen*, *query-pack linting*, *schema drift detection*, and *fast runtime validators*.

---

### 10.1 File schema (only what matters for tooling)

`node-types.json` is **an array of objects**; each object minimally has: ([Tree-sitter][1])

* `"type": str` — grammar rule name or literal token string (maps to `ts_node_type`). ([Tree-sitter][1])
* `"named": bool` — whether it corresponds to a named grammar rule vs a literal token. ([Tree-sitter][1])

**Internal node structure fields (optional per node type)**: ([Tree-sitter][1])

* `"fields": { field_name: ChildTypeSet }`
* `"children": ChildTypeSet` (named children without fields)

**ChildTypeSet schema**: ([Tree-sitter][1])

* `"required": bool` (≥1 child in this set exists)
* `"multiple": bool` (can be >1)
* `"types": [{ "type": str, "named": bool }, ...]` (union)

**Supertype entry**: ([Tree-sitter][1])

* `"subtypes": [{ "type": str, "named": bool }, ...]`

---

### 10.2 Build a “grammar schema” object once (lookup tables; no runtime guessing)

Minimal loader + indices (dense but production-useful):

```python
from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Iterable

NodeTypeId = tuple[str, bool]  # (type, named)

@dataclass(frozen=True)
class ChildTypeSet:
    required: bool
    multiple: bool
    types: tuple[NodeTypeId, ...]

@dataclass(frozen=True)
class NodeTypeSpec:
    id: NodeTypeId
    fields: dict[str, ChildTypeSet]
    children: ChildTypeSet | None
    subtypes: tuple[NodeTypeId, ...] | None  # present for supertypes

def _parse_child_set(obj: dict[str, Any]) -> ChildTypeSet:
    return ChildTypeSet(
        required=bool(obj.get("required", False)),
        multiple=bool(obj.get("multiple", False)),
        types=tuple((t["type"], bool(t["named"])) for t in obj.get("types", [])),
    )

def load_node_types(node_types_json: list[dict[str, Any]]) -> dict[NodeTypeId, NodeTypeSpec]:
    out: dict[NodeTypeId, NodeTypeSpec] = {}
    for entry in node_types_json:
        nid: NodeTypeId = (entry["type"], bool(entry["named"]))
        fields = {k: _parse_child_set(v) for k, v in entry.get("fields", {}).items()}
        children = _parse_child_set(entry["children"]) if "children" in entry else None
        subtypes = tuple((t["type"], bool(t["named"])) for t in entry.get("subtypes", [])) or None
        out[nid] = NodeTypeSpec(id=nid, fields=fields, children=children, subtypes=subtypes)
    return out
```

This mirrors the official schema: node entries have `"type"`/`"named"`, optional `"fields"`/`"children"`, and supertypes have `"subtypes"` unions. ([Tree-sitter][1])

---

### 10.3 Typed wrappers: node schema → Python types that *stop agents from hallucinating fields*

Even in Python, `node-types.json` lets you generate a **typed AST facade** that:

* turns `"fields"` into attributes,
* encodes `required`/`multiple` as `T | None` vs `list[T]`,
* encodes `"types"` unions as `Union[SubtypeA, SubtypeB, ...]`,
* encodes supertypes as `Protocol`/`Union` to let agents write polymorphic code safely.

**Rule mapping (mechanical)**

* `required=True, multiple=False` → `T`
* `required=False, multiple=False` → `T | None`
* `multiple=True` → `list[T]` (and if `required=False`, list may be empty)

**Supertype compression**

* If a field allows `{("_expression", True)}` and `_expression` has `subtypes=[...]`, you can expand to a concrete union at codegen time while still preserving the supertype symbol for query authoring. ([Tree-sitter][1])

---

### 10.4 Query-pack linting: validate query source *against the node schema* before runtime

Goal: prevent “silent zero matches” caused by typos in node/field names, and enforce your §6 scaling rules (rooted/local).

**Static checks driven by `node-types.json`**

* Every parenthesized node name in the query must exist as `("type", named=True)` in the schema. ([Tree-sitter][1])
* Every quoted token in the query must exist as `("type", named=False)` (anonymous kinds). ([Tree-sitter][1])
* Every `field:` or `!field` referenced under a node type must exist in that node’s `"fields"` set. ([Tree-sitter][1])

Implementation note: Tree-sitter does not ship a “query AST” API in py-tree-sitter; in practice you:

* parse query text with a **small S-expression tokenizer** sufficient to extract node identifiers and field labels, then check them against the schema (fast and deterministic), *or*
* treat `Query(...)` compilation as the first gate, then apply stricter semantic lint using schema + `Query` introspection (pattern rootedness/locality from §7). ([Tree-sitter][2])

---

### 10.5 Drift detection + CI gates: treat `node-types.json` diffs as breaking-change signals

Because the file is a complete “node schema”, you can use it to detect:

* **renamed/removed node types** (`(type,named)` keys disappear),
* **field additions/removals** per node type,
* **cardinality changes** (`required/multiple` flips),
* **union changes** (types list changed),
* **supertype reshaping** (subtype sets changed). ([Tree-sitter][1])

Practical CI posture for large indexing systems:

* On grammar upgrades: diff old vs new `node-types.json`, categorize changes, and:

  * regenerate wrappers/visitors,
  * re-run query-pack lint + compile,
  * fail CI if a “public surface” node/field used by your query packs disappears.

---

### 10.6 Runtime reconciliation: `node-types.json` vs `Language` introspection

Use both layers:

* `node-types.json` is the **build-time** schema for codegen + query lint. ([Tree-sitter][1])
* `Language` is the **runtime** authority for symbol IDs/fields/parse states (used in §1).
  Best practice: at startup, sample-check that a handful of critical node/field names from schema exist in the loaded `Language` (detect “wrong grammar loaded” / ABI mismatch earlier than downstream failures). ([Tree-sitter][1])

---

### 10.7 Concrete payoff summary (why advanced agents should care)

* **Typed traversal**: agents write `node.name` / `node.body` instead of stringly `child_by_field_name("name")`.
* **Safer queries**: node/field typos become compile/lint errors, not silent empties.
* **Stable indexing**: schema drift becomes a first-class artifact diff, not a surprise regression.
* **Better scaling**: you can force query authors toward fields/anchors by making “positional children only” patterns fail lint unless justified.

If you want next, I can continue with **11) Grammar authoring (advanced)** focusing only on the parts that *directly shape* `node-types.json` output (fields/alias/inline/extras/supertypes) and how to reason about downstream typed tooling impact.

[1]: https://tree-sitter.github.io/tree-sitter/using-parsers/6-static-node-types "Static Node Types - Tree-sitter"
[2]: https://tree-sitter.github.io/tree-sitter/using-parsers/queries/1-syntax.html "Basic Syntax - Tree-sitter"


## 11) Grammar authoring (advanced): precedence, conflicts, scanners, and “shaping the tree”

### Quick gotchas (high-impact)

* **Parse precedence ≠ lexical precedence**: `prec(...)` outside `token(...)` resolves *LR(1) parse conflicts* at generation time; `token(prec(N, ...))` assigns **lexical precedence** to break ties between tokens that match the same characters. ([Tree-sitter][1])
* **Dynamic precedence is runtime-only + conflict-gated**: `prec.dynamic(N, rule)` is only meaningful when you’ve declared a conflict (`conflicts`) and there’s a genuine ambiguity; Tree-sitter sums dynamic precedence and chooses the highest. ([Tree-sitter][1])
* **Conflicts are not “errors”**: `conflicts: $ => [[...]]` explicitly opts into GLR exploration for intended ambiguities; without it, many grammars “resolve” by accident via lexical rules or rule order. ([Tree-sitter][1])
* **External scanners run *before* normal lexing when an external token is valid**; they can override default lexing and must be incremental-safe via serialize/deserialize. ([Tree-sitter][2])
* **Tree shape is an API**: underscores hide rules; `inline` removes nodes; `supertypes` hide abstract categories but keep them queryable; fields/aliases change downstream Query + `node-types.json` semantics. ([Tree-sitter][3])

---

### 11.0 Mental model (what you’re actually “programming”)

A Tree-sitter grammar is a *declarative spec* that compiles into:

* a **context-aware lexer** (tries only tokens valid at that parse position; resolves ties via lexical precedence, match length, string-vs-regex specificity, and rule order), and
* a **GLR-capable LR parser** (resolves local ambiguities with precedence/associativity; explores declared ambiguities via `conflicts`). ([Tree-sitter][3])

“Advanced grammar authoring” = controlling **ambiguity resolution** (parse vs lex), **tree surface** (node/field layout for tooling), and **lexing escape hatches** (external scanners) while preserving incremental correctness.

---

### 11.1 Precedence + associativity (parse-time), and lexical precedence (lex-time)

#### 11.1.1 Parse precedence: `prec`, `prec.left`, `prec.right`

* `prec(N, rule)`: assigns numerical precedence (default 0) used to resolve LR(1) conflicts at generation time; higher precedence wins. ([Tree-sitter][1])
* `prec.left([N], rule)`: for equal precedence, prefer rule that ends earlier (left-associative). ([Tree-sitter][1])
* `prec.right([N], rule)`: for equal precedence, prefer rule that ends later (right-associative). ([Tree-sitter][1])

Canonical operator tower (agent-friendly; minimize tree noise by hiding wrappers, see §11.4):

```js
module.exports = grammar({
  name: "lang",
  rules: {
    expression: $ => choice($.binary_expression, $.unary_expression, $.identifier),

    unary_expression: $ => prec(2, choice(seq("-", $.expression), seq("!", $.expression))),

    binary_expression: $ => choice(
      prec.left(2, seq($.expression, "*", $.expression)),
      prec.left(1, seq($.expression, "+", $.expression)),
    ),

    identifier: $ => /[a-z_]+/,
  }
});
```

This is the exact conflict/associativity pattern Tree-sitter’s docs walk through (`prec` to fix unary-vs-binary conflicts; `prec.left` to fix `a*b*c`). ([Tree-sitter][3])

#### 11.1.2 Lexical precedence: `token(prec(...))`

When multiple tokens match the same characters, Tree-sitter breaks ties by: (1) context-aware lexing, (2) lexical precedence, (3) match length, (4) string-over-regex specificity, (5) rule order; external scanners can further affect this. ([Tree-sitter][3])
Lexical precedence is expressed by wrapping `prec` **inside** `token(...)` (`token(prec(1, 'foo'))`). ([Tree-sitter][1])

---

### 11.2 Declared ambiguity: `conflicts` + GLR + `prec.dynamic`

#### 11.2.1 `conflicts`: opt-in ambiguity sets

`conflicts: $ => [[ruleA, ruleB, ...]]` declares intended LR(1) conflicts; at runtime Tree-sitter uses GLR to explore interpretations and, if multiple succeed, selects the subtree with the highest total dynamic precedence. ([Tree-sitter][1])

Minimal “array literal vs array pattern” ambiguity pattern (docs example):

```js
module.exports = grammar({
  name: "javascript_like",
  conflicts: $ => [[ $.array, $.array_pattern ]],
  rules: { /* ... */ }
});
```

([Tree-sitter][3])

#### 11.2.2 Dynamic precedence: `prec.dynamic(N, rule)`

`prec.dynamic` applies precedence **at runtime** (not generation-time) and is only needed when resolving genuine ambiguity via `conflicts`; Tree-sitter compares *total dynamic precedence* and selects highest. ([Tree-sitter][1])

**Heuristic**: if you can resolve via parse precedence/associativity *without* GLR, do so; declare conflicts only for true syntactic duality (expression-vs-pattern, type-vs-expression in some langs, etc.). (This is implied by the docs’ framing: conflicts are “sometimes desirable”, not a default escape hatch.) ([Tree-sitter][3])

---

### 11.3 Lexing control: `token`, `token.immediate`, keyword extraction (`word`), and contextual keywords (`reserved`)

#### 11.3.1 Tokens: `token(...)` and `token.immediate(...)`

* `token(rule)` forces a complex terminal rule (built from `seq/choice/repeat/...`) to be lexed as a **single token**; it accepts only terminal rules (e.g., `token($.foo)` is invalid if `$.foo` is non-terminal). ([Tree-sitter][1])
* `token.immediate(rule)` requires no whitespace/extras before the token (useful for constructs where adjacency is semantic). ([Tree-sitter][1])

#### 11.3.2 Keyword extraction: `word`

`word: $ => $.identifier` enables a keyword-extraction optimization: Tree-sitter identifies keyword tokens overlapping the `word` token and lexes keywords via a two-step “match word then classify” process, improving correctness (e.g., `instanceofSomething` should be one identifier, not `instanceof` + identifier) and often improving compile time. The word token must be a unique token not reused elsewhere (alias it if needed). ([Tree-sitter][3])

#### 11.3.3 Contextual keywords: `reserved(...)` + grammar-level `reserved` sets

* `reserved(wordset, rule)` overrides the global reserved-word set for contextual keywords (e.g., keywords forbidden as identifiers in most contexts but allowed in property contexts). ([Tree-sitter][1])
* Grammar-level `reserved` defines word sets and associated reserved rules; first set is global, and reserved rules must be terminal + actually used in the grammar. ([Tree-sitter][1])

**Practical posture**: treat `word` + `reserved` as your “keyword correctness layer” before reaching for external scanners; most “why does this tokenize wrong?” problems are lexical precedence/keyword extraction issues. ([Tree-sitter][3])

---

### 11.4 Shaping the tree (your downstream tooling contract)

#### 11.4.1 Hide wrapper rules: leading underscore

Prefixing a rule name with `_` hides it in the syntax tree (still participates in parsing). Use for abstract wrappers that add depth/noise. ([Tree-sitter][3])

#### 11.4.2 Fields: `field(name, rule)`

Fields create stable child access slots (`name:`, `body:`, etc.), powering field-based APIs and dramatically improving query specificity. ([Tree-sitter][3])

```js
function_definition: $ => seq(
  "func",
  field("name", $.identifier),
  field("parameters", $.parameter_list),
  field("return_type", $._type),
  field("body", $.block),
);
```

([Tree-sitter][3])

#### 11.4.3 Aliases: `alias(rule, name)`

`alias` renames a matched rule in the CST: alias to a symbol yields a named node; alias to a string yields an anonymous node (as if it were a literal token). This is a *tree surface* tool—use it to stabilize node taxonomy for tooling without restructuring parsing. ([Tree-sitter][1])

#### 11.4.4 `inline` and `supertypes`

* `inline`: replaces usages of a rule with its definition to remove runtime nodes (good for “macro rules” used in many places but not meaningful as nodes). ([Tree-sitter][1])
* `supertypes`: hides abstract category rules from the tree while preserving them for queries; reduces depth/noise for “expression/declaration/type” unions. ([Tree-sitter][3])

#### 11.4.5 Extras: `extras` and “don’t inline expensive tokens”

Extras can appear anywhere (whitespace/comments). Docs warn: avoid inlining complex tokens directly inside `extras` because it can bloat the lexer/parser; define a named rule and reference it from `extras`. ([Tree-sitter][3])

---

### 11.5 External scanners (lexing escape hatch; must be incremental-safe)

#### 11.5.1 When you need one

Use external scanners for tokens hard/impossible to express as regex/DSL terminals (Python indent/dedent, heredocs, etc.). External scanners are custom C code called during lexing. ([Tree-sitter][2])

#### 11.5.2 Wiring: `externals` + `src/scanner.c`

* Declare external tokens in `externals: $ => [$.indent, $.dedent, ...]`. ([Tree-sitter][2])
* Implement `src/scanner.c` (path required for CLI recognition). ([Tree-sitter][2])
* Define `enum TokenType { ... }` matching the order of `externals`. ([Tree-sitter][2])
* Implement 5 required functions: `*_create`, `*_destroy`, `*_serialize`, `*_deserialize`, `*_scan`. ([Tree-sitter][2])

Minimal skeleton (compressing to the essential invariants):

```c
#include "tree_sitter/parser.h"
#include "tree_sitter/alloc.h"

enum TokenType { INDENT, DEDENT, NEWLINE };

void *tree_sitter_my_language_external_scanner_create() { return NULL; }
void tree_sitter_my_language_external_scanner_destroy(void *p) {}

unsigned tree_sitter_my_language_external_scanner_serialize(void *p, char *buffer) { return 0; }
void tree_sitter_my_language_external_scanner_deserialize(void *p, const char *b, unsigned n) {}

bool tree_sitter_my_language_external_scanner_scan(void *p, TSLexer *lexer, const bool *valid) {
  // Only attempt tokens whose valid_symbols entry is true.
  // Use lexer->advance(lexer, skip) and lexer->mark_end(lexer); set lexer->result_symbol; return true.
  return false;
}
```

Core requirements and lexer APIs (`lookahead`, `advance(skip)`, `mark_end`, `result_symbol`, `eof`, `get_column`, `is_at_included_range_start`) are explicitly documented. ([Tree-sitter][2])

#### 11.5.3 Incremental correctness: serialize/deserialize is not optional

Tree-sitter stores scanner state in the syntax tree so the scanner can be restored correctly across edits/ambiguities; `serialize` must capture full state compactly; `deserialize` must restore it. ([Tree-sitter][2])

#### 11.5.4 Priority + error recovery mode

* External scanners have priority over normal lexing when an external token is valid at a position. ([Tree-sitter][2])
* During error recovery, Tree-sitter calls scan with all tokens marked valid; docs suggest a sentinel external token to detect this mode. ([Tree-sitter][2])

#### 11.5.5 Allocator + arrays (production hygiene)

Docs recommend using `ts_*` alloc macros from `tree_sitter/alloc.h` (allocator override support) and `tree_sitter/array.h` macros for scanner-managed stacks (indent stacks, etc.). ([Tree-sitter][2])

---

### 11.6 Downstream impacts (what grammar authors must “owe” indexing/query tooling)

* Every change to **fields/aliases/supertypes/hiding/inline/extras** changes the effective node/field surface and therefore query packs + `node-types.json` (typed tooling) behavior. ([Tree-sitter][3])
* Treat grammar edits like a schema migration:

  * regenerate + diff node types,
  * recompile + lint query packs (rooted/local, field existence),
  * run `tree-sitter test` suites (especially highlight/locals/injections packs) to prevent silent regressions. ([Tree-sitter][3])

If you want next, I can proceed with **12) CLI workflows for parser + query development** (`tree-sitter generate/build/parse/test/version`) written as “mechanical playbooks” for grammar iteration + regression control.

[1]: https://tree-sitter.github.io/tree-sitter/creating-parsers/2-the-grammar-dsl.html "The Grammar DSL - Tree-sitter"
[2]: https://tree-sitter.github.io/tree-sitter/creating-parsers/4-external-scanners.html "External Scanners - Tree-sitter"
[3]: https://tree-sitter.github.io/tree-sitter/creating-parsers/3-writing-the-grammar.html "Writing the Grammar - Tree-sitter"


## 12) CLI workflows for parser + query development (build/parse/test/version)

### Quick gotchas (high-impact)

* `tree-sitter build` emits a **loadable parser artifact**: native shared object (`.so/.dylib/.dll`) **or** Wasm; `--wasm` requires a WASI SDK (auto-downloadable) and is controlled by `TREE_SITTER_WASI_SDK_PATH`. ([Tree-sitter][1])
* `tree-sitter parse` is multi-file + glob aware, parses stdin if no paths, and **exits non-zero if any parse errors occur** (great for CI gating). ([Tree-sitter][2])
* `--lib-path` short-circuits the CLI’s cached/auto-built library; if you use it, you may need `--lang-name` to select the exported `tree_sitter_<lang>` symbol from the library. ([Tree-sitter][2])
* `--debug-graph` writes `log.html` using Graphviz `dot`; it logs stack/tree graphs + lex/parse logs (debugging grammars/scanners quickly). ([Tree-sitter][2])
* `tree-sitter test --update` will **not** update tests containing `ERROR` or `MISSING` nodes. ([Tree-sitter][3])
* `tree-sitter version` is a **multi-binding sync tool**: updates version in multiple manifest files; some updates require `cargo` and/or `npm`. ([Tree-sitter][4])

---

### 12.0 Mental model

The CLI is an “inner-loop compiler+runner” for grammar repos:

* **Build path**: grammar repo → compiled parser library (`build`). ([Tree-sitter][1])
* **Run path**: parse real files / corpus tests using either:

  * an auto-built/cached library, or
  * a pinned library via `--lib-path` (reproducible, scriptable). ([Tree-sitter][2])
* **Debug path**: `--debug` (stderr logs), `--debug-build` (compile with debug flags), `--debug-graph` (HTML graph log). ([Tree-sitter][2])
* **Release hygiene**: `version` keeps grammar version synchronized across bindings. ([Tree-sitter][4])

---

## 12.1 `tree-sitter build`: produce a deterministic parser artifact (native/Wasm)

**Signature + behavior**

* `tree-sitter build [OPTIONS] [PATH]` (alias: `b`); builds parser in `PATH` or CWD. ([Tree-sitter][1])
* Toolchain knobs:

  * `CC` chooses compiler, `CFLAGS` appends flags; macOS/iOS deployment targets via `MACOSX_DEPLOYMENT_TARGET` / `IPHONEOS_DEPLOYMENT_TARGET`. ([Tree-sitter][1])

**Options (complete)**

* `-w/--wasm`: compile as Wasm; uses WASI SDK at `TREE_SITTER_WASI_SDK_PATH`, auto-downloads to a cache dir if missing. ([Tree-sitter][1])
* `-o/--output <PATH>`: output path for `.so/.dylib/.dll` **or** `.wasm`; defaults to `parser.{so|wasm}` (or inferred from parent dir name if possible). ([Tree-sitter][1])
* `--reuse-allocator`: reuse core allocator for external scanner (allocator-override consistency). ([Tree-sitter][1])
* `-0/--debug`: compile with debug flags (gdb/lldb friendly). ([Tree-sitter][1])

**Canonical build invocations**

```bash
# Native shared object (explicit output):
tree-sitter build -o ./dist/parser.so

# Wasm output (explicit output; WASI SDK path optional if already installed/cached):
TREE_SITTER_WASI_SDK_PATH=/path/to/wasi-sdk tree-sitter build --wasm -o ./dist/parser.wasm

# Debug build for native debugging:
tree-sitter build --debug -o ./dist/parser_dbg.so
```

---

## 12.2 `tree-sitter parse`: parse real code, dump trees, and replay incremental edits

**Core behavior**

* `tree-sitter parse [OPTIONS] [PATHS]...` (alias: `p`); accepts multiple paths + globs; parses stdin if no paths; exits non-zero if any parse errors occurred. ([Tree-sitter][2])

### 12.2.1 Library resolution (reproducibility knobs)

* `-p/--grammar-path <PATH>`: directory containing grammar. ([Tree-sitter][2])
* `-l/--lib-path <PATH>`: use this dynamic library instead of cached/auto-generated. ([Tree-sitter][2])
* `--lang-name <NAME>`: when using `--lib-path`, choose the language name used to extract the library’s language function. ([Tree-sitter][2])
* `--scope <SCOPE>`: choose a language scope when ambiguous. ([Tree-sitter][2])
* `-r/--rebuild`: force rebuild before parsing/tests. ([Tree-sitter][2])

**Pinned-lib parse (tight, scriptable)**

```bash
# 1) Build once:
tree-sitter build -o /tmp/ts_python.so

# 2) Parse using that exact binary:
tree-sitter parse --lib-path /tmp/ts_python.so --lang-name python path/to/file.py --cst
```

(Use this pattern when you want “no hidden cache effects”.)

### 12.2.2 Output formats + suppression

* `--dot`: parse tree in Graphviz dot. ([Tree-sitter][2])
* `-x/--xml`: XML tree. ([Tree-sitter][2])
* `-c/--cst`: pretty-printed CST (human-facing, copy/paste friendly). ([Tree-sitter][2])
* `--no-ranges`: omit node ranges from default parse output (useful for copying S-exprs into corpus tests). ([Tree-sitter][2])
* `-q/--quiet`: suppress main output. ([Tree-sitter][2])
* `-j/--json-summary`: JSON summary of parsing results. ([Tree-sitter][2])

**Common “author a test” mode**

```bash
tree-sitter parse ./samples/min.py --cst --no-ranges
```

### 12.2.3 Debugging + graph logs

* `-d/--debug`: parsing + lexing logs to stderr. ([Tree-sitter][2])
* `-0/--debug-build`: compile parser with debug flags. ([Tree-sitter][2])
* `-D/--debug-graph`: produce `log.html` with stack/tree graphs + logs; uses Graphviz `dot`. ([Tree-sitter][2])
* `--open-log`: open `log.html` automatically (with `--debug-graph`). ([Tree-sitter][2])

```bash
tree-sitter parse ./samples/weird_case.lang --debug --debug-graph --open-log
```

### 12.2.4 Performance + safety rails

* `--timeout <MICROS>`: timeout per file in microseconds. ([Tree-sitter][2])
* `-t/--time`: print time to parse; if edits are provided, also prints time after each edit. ([Tree-sitter][2])
* `-s/--stat`: show parsing statistics. ([Tree-sitter][2])

### 12.2.5 Incremental-reparse simulation: `--edits`

* `--edits <EDITS>...`: apply edits after parsing; edit format: `row,col|position delcount insert_text` where row/col or position are 0-indexed. ([Tree-sitter][2])

```bash
# Example: insert "x" at row=0,col=0; then delete 1 char at absolute position 10.
tree-sitter parse ./samples/min.py --time \
  --edits '0,0|0 0 x' '0,0|10 1 '
```

### 12.2.6 Encoding control (avoid “byte/point mismatch” hell)

* `--encoding <ENCODING>`: one of `utf8`, `utf16-le`, `utf16-be`; default: detect BOM for UTF-16BE/LE, else UTF-8. ([Tree-sitter][2])

### 12.2.7 Corpus targeting from `parse`

* `-n/--test-number <N>`: parse a specific corpus test by number (numbers are shown in `tree-sitter test` output). ([Tree-sitter][2])

---

## 12.3 `tree-sitter test`: corpus regression + diffs + update workflow

**Signature**

* `tree-sitter test [OPTIONS]` (alias: `t`). ([Tree-sitter][3])

### 12.3.1 Selecting tests (surgical repro)

* `-i/--include <REGEX>`: run tests whose names match regex. ([Tree-sitter][3])
* `-e/--exclude <REGEX>`: skip tests whose names match regex. ([Tree-sitter][3])
* `--file-name <NAME>`: only run tests from given corpus filename. ([Tree-sitter][3])

```bash
tree-sitter test --file-name expressions.txt --include 'precedence'
```

### 12.3.2 Parser selection + rebuild control

* `-p/--grammar-path <PATH>`; `--lib-path <PATH>`; `--lang-name <NAME>`; `-r/--rebuild`; `-0/--debug-build`; `--wasm` (only if CLI built with wasm feature). ([Tree-sitter][3])

### 12.3.3 Output modes + debugging

* `-d/--debug`: lex/parse logs to stderr. ([Tree-sitter][3])
* `-D/--debug-graph`: graph logs to `log.html`. ([Tree-sitter][3])
* `--open-log`: open log in browser. ([Tree-sitter][3])
* `--overview-only`: suppress diffs, show only overview. ([Tree-sitter][3])
* `--json-summary`: JSON summary output. ([Tree-sitter][3])
* `--show-fields`: force showing fields in test diffs. ([Tree-sitter][3])

### 12.3.4 Updating expected outputs (`--update`)

* `-u/--update`: update expected test output; **tests containing `ERROR` or `MISSING` nodes will not be updated**. ([Tree-sitter][3])

```bash
tree-sitter test --update
```

### 12.3.5 Test statistics (find outliers)

* `--stat <STAT>`: `all` | `outliers-and-total` | `total-only`. ([Tree-sitter][3])

---

## 12.4 `tree-sitter version`: synchronized multi-binding version bumps

**Behavior**

* `tree-sitter version <VERSION>` (alias: `publish`) updates version across a set of binding manifests if present: `tree-sitter.json`, `Cargo.toml`, `Cargo.lock`, `package.json`, `package-lock.json`, `Makefile`, `CMakeLists.txt`, `pyproject.toml`. ([Tree-sitter][4])
* Auto-bump forms: `tree-sitter version --bump {patch|minor|major}` using the version in `tree-sitter.json`. ([Tree-sitter][4])
* Print current version without bumping: `tree-sitter version`. ([Tree-sitter][4])
* Tooling requirements: updating Cargo files requires `cargo`; updating `package-lock.json` requires `npm`. ([Tree-sitter][4])

```bash
# Explicit set:
tree-sitter version 0.9.0

# Semver bump (from tree-sitter.json):
tree-sitter version --bump patch
```

---

## 12.5 Config plumbing that affects parse/test/highlight loops (`init-config`, `--config-path`)

Even when you’re “just” building/testing parsers, the CLI’s config impacts discovery + rendering:

* `tree-sitter init-config` creates a per-user `config.json` under platform defaults (XDG on Unix; AppData on Windows); CLI works without it but uses defaults. ([Tree-sitter][5])
* `parser-directories`: directories scanned for `tree-sitter-*` grammar repos (used by CLI language autodetection, notably highlighting). ([Tree-sitter][5])
* `theme`: maps highlight names (e.g. `function.method`, `keyword`) to styling. ([Tree-sitter][5])
* `parse-theme`: colors for `tree-sitter parse --cst` output (node kind, node text, fields, ERROR/MISSING, etc.). ([Tree-sitter][5])
* `tree-sitter parse` / `tree-sitter test` both accept `--config-path <CONFIG_PATH>` to use an alternate `config.json`. ([Tree-sitter][2])

```bash
tree-sitter init-config
tree-sitter parse --config-path ./ci/tree-sitter-config.json --cst ./samples/min.py
```

---

### Minimal “mechanical” inner loops (recipes)

**Grammar change → debug → lock it in**

```bash
tree-sitter test --debug-graph --open-log
tree-sitter parse ./samples/repro.lang --debug --debug-graph --open-log
tree-sitter test --update
```

(Everything above is just compositions of documented flags/options.) ([Tree-sitter][2])

[1]: https://tree-sitter.github.io/tree-sitter/cli/build.html "Build - Tree-sitter"
[2]: https://tree-sitter.github.io/tree-sitter/cli/parse.html "Parse - Tree-sitter"
[3]: https://tree-sitter.github.io/tree-sitter/cli/test.html "Test - Tree-sitter"
[4]: https://tree-sitter.github.io/tree-sitter/cli/version.html "Version - Tree-sitter"
[5]: https://tree-sitter.github.io/tree-sitter/cli/init-config.html "Init Config - Tree-sitter"
