
Yes — the “right” move here is to build a **tiny declarative query DSL** (a “constructor language”) that compiles into a **plan** that chooses among:

* **ripgrep** for *fast file narrowing*
* **ast-grep** for *structural extraction when needed* (and with JSON streaming output) ([AstGrep][1])
* **Python AST / symtable / bytecode** for *cross-file relationships + scoping + runtime-coupling signals* (symtable is generated by the compiler from the AST just before bytecode generation) ([Python documentation][2])

This makes the agent experience “semantic-ish” (because it asks for *intent* like “callers of this function”) while staying grounded in deterministic tooling.

Below is a concrete architecture that keeps the surface area small and makes it very skill-friendly.

---

## 1) The shape of a declarative query

You want a **small number of orthogonal constructors**:

### A. Selector (what “things” are we finding?)

* `entity=function | method | class | module | callsite | import`
* `name=...` (exact / regex)
* `where=...` (a few high-value predicates; keep it tight)

### B. Expanders (what relationships do we want?)

* `callers(depth=N)` / `callees(depth=N)`
* `imports(depth=N)` / `imported_by(depth=N)`
* `raises()` / `catches()` / `uncaught(depth=N)` (heuristic)
* `scope()` (free vars / globals / nonlocals via symtable)
* `bytecode_surface()` (loaded globals/attrs/constants/opcodes)

### C. Scope filters (where in repo?)

* `in=src/x` (subdir)
* `exclude=tests,build,.venv`
* `glob=*.py`

### D. Projection (what fields come back?)

* `fields=def,loc,signature,callers,evidence,hazards`
* `limit=K`

### E. “Explain plan”

* `explain=true` (prints “why we used rg vs ast-grep vs AST”)

That’s it. If you keep these four buckets, you won’t explode the skill surface.

---

## 2) A query string format that’s *easy* for agents

Give it a **single string** with a stable grammar and a tiny vocabulary:

### Option 1: key=value tokens (the most robust)

```bash
cq q "entity=function name=foo expand=callers(depth=2) in=src/x exclude=tests limit=200 fields=def,callers,evidence"
```

### Option 2: “pipes” for readability (still easy to parse)

```bash
cq q "function foo | callers 2 | in src/x | -in tests | fields def callers evidence | limit 200"
```

### Option 3: “constructor” JSON/YAML *as input* (but not output)

If you really want “constructors”, accept a compact JSON5-ish string **as input only**:

```bash
cq q '{entity:"function", name:"foo", expand:[{callers:{depth:2}}], in:"src/x", exclude:["tests"]}'
```

You can still print Markdown output; the user never opens JSON files.

---

## 3) The planner: how a “semantic” query becomes deterministic steps

Think of it as a mini SQL engine:

### Step 0 — Normalize + compile to an IR

Turn the query string into an internal object:

```python
Query(
  entity="function",
  name="foo",
  expand=[Callers(depth=2)],
  scope=Scope(in="src/x", exclude=["tests"]),
  fields=["def","callers","evidence"],
  limit=200,
)
```

### Step 1 — Build a candidate file set (fast)

Use **ripgrep first** to restrict IO:

* If query has `name=foo`, run:
  `rg --files-with-matches '\bfoo\b' src/x`
* If it has `in=...` and no name, run `fd`/`find` or `git ls-files` (even faster).

If you want machine-readable match details, ripgrep has JSON lines output, but it can’t be combined with output flags like `--files-with-matches`. ([ripgrepy.readthedocs.io][3])
So: **use `--files-with-matches`** to get file lists, and only use `--json` if you actually need match records.

### Step 2 — Extract entities + edges

Now choose the extractor(s):

* **Python AST** for:

  * def sites (functions/classes/methods)
  * call sites + call edges
  * imports graph
  * exception raises/catches

* **symtable** on-demand for:

  * free vars / globals / nonlocals (scope hazards) ([Python documentation][2])

* **bytecode surface** on-demand via `compile()` + `dis.get_instructions()` (no execution):

  * loaded globals/attrs/constants/opcode profile (hidden coupling)

* **ast-grep** only when it’s clearly better than AST traversal:

  * “find async defs containing X call”
  * “find pattern shape with captures”
  * “find structural statement templates quickly”
  * and when you want `inside/has/stopBy` constraints (it’s very good at those) ([AstGrep][1])

When you do use ast-grep, prefer `scan --inline-rules` and JSON output (`--json=stream` for scalable parsing). ([AstGrep][1])

### Step 3 — Resolve + expand relationships

Now compute what the user asked:

* `callers(depth=2)` becomes:

  1. resolve target function key(s) (module + class methods)
  2. build caller edges
  3. BFS upstream by depth
  4. return aggregated results, plus “hazards” where resolution is ambiguous

This is the “pseudo semantic” value: the user requested “callers”, and you return a caller graph — even though it’s built from AST+tooling.

### Step 4 — Render output for an agent (Markdown, not JSON)

By default, print:

* **Summary**
* **Key findings**
* **Evidence index** (file:line:col anchors)
* **Details** (callers/callees/imports/scope/bytecode sections)
* **Notes / uncertainty**
* Optionally: “Artifacts saved to …” (JSON for caching/debugging), but not required to open.

---

## 4) Why this works better with agents than raw ast-grep

Agents tend to:

* under-use unfamiliar tools
* struggle to author correct structural rules on the fly

So you hide that behind a **“planner contract”**:

* The agent says: “function foo, show callers, within src/x.”
* Your planner decides:

  * “use rg for file set”
  * “use AST index for call graph”
  * “use symtable only if scope requested”
  * “use ast-grep only if a structural predicate requires it”

This is exactly the situation where a skill shines: Claude Code skills support **dynamic context injection**, so the skill can run `cq q ...` and inject the Markdown report directly into the prompt. ([Claude Code][4])

---

## 5) A minimal “IR + operator” design (keeps surface area small)

### Core operator types

* `NarrowFiles(op)` → returns file list
* `ExtractEntities(op)` → defs/calls/imports
* `ResolveSymbols(op)` → resolves `foo` to `pkg.mod:foo`, `Class.method`, etc.
* `ExpandGraph(op)` → callers/callees/import edges BFS
* `Enrich(op)` → symtable + bytecode surfaces only when asked
* `Render(op)` → Markdown + (optional) JSON artifact

### Data model (stable internal schema)

* `SymbolDef`: key, kind, file, range, signature
* `CallEdge`: caller_key, callee_key (or unresolved), location, arg_bindings (optional)
* `ImportEdge`: from_module, to_module, location
* `ScopeInfo`: free/globals/nonlocals/cell vars
* `BytecodeInfo`: loaded_globals, loaded_attrs, constants_sample, opcode_stats
* `Hazard`: dynamic dispatch, getattr, *args/**kwargs forwarding, unresolved import aliasing

This is enough to answer “semantic” queries while staying deterministic.

---

## 6) How you’d expose it in `SKILL.md` without bloat

You don’t want 50 slash commands. You want **one**:

* `/q "<query string>"`

Example skill snippet (conceptually):

* `Results: !\`./scripts/cq q "$ARGUMENTS"`` ([Claude Code][4])

Then teach 6–8 canonical query templates in the skill doc:

* “Find function + callers in subdir”
* “Find callsites with kwarg X”
* “Find modules importing Y”
* “Find async hazards”
* “Scope hazards”
* “Bytecode coupling”

That’s a *small* training set for the agent.

---

## 7) A concrete example end-to-end

User intent:

> “What functions are dependent on this input and what files are they located in, only in subdirectory X”

Query:

```bash
cq q "entity=function name=foo expand=callers(depth=3) in=src/x fields=def,callers,evidence,hazards"
```

Planner:

1. `rg --files-with-matches '\bfoo\b' src/x`
2. parse AST for defs + calls in those files
3. resolve `foo` → one or more defs
4. build reverse call graph → callers BFS depth 3
5. output Markdown with anchors and hazard notes

If an extra predicate appears like “only callers that pass `timeout=`”, planner can:

* either check AST call keywords directly, or
* compile an ast-grep structural predicate (particularly good when pattern constraints get complex). ([AstGrep][5])

---


[1]: https://ast-grep.github.io/reference/cli/scan.html?utm_source=chatgpt.com "ast-grep scan"
[2]: https://docs.python.org/pl/3.8/library/symtable.html?utm_source=chatgpt.com "symtable — Access to the compiler's symbol tables"
[3]: https://ripgrepy.readthedocs.io/?utm_source=chatgpt.com "Welcome to ripgrepy's documentation! — ripgrepy ..."
[4]: https://code.claude.com/docs/en/skills?utm_source=chatgpt.com "Extend Claude with skills - Claude Code Docs"
[5]: https://ast-grep.github.io/guide/rule-config.html?utm_source=chatgpt.com "Rule Essentials"

You can absolutely rewrite this so the “semantic-ish” query engine is **natively constructed out of CLI primitives**:

* **ripgrep** = *candidate file selection / cheap lexical filters*
* **ast-grep scan** = *structural extraction of facts as JSON stream*
* **python (ast/symtable/dis)** = *selective enrichment (scope + bytecode coupling) on just the small set of targets that survive the first two stages*

The key shift is: **stop hand-walking Python AST for baseline facts** (defs/calls/imports/raise/except/ctor-assign). Instead, treat ast-grep as your *fact extractor* and Python’s compiler services as *enrichers*.

Below is an implementation-oriented outline with representative code and the exact mechanics that make this work reliably at repo scale.

---

## 0) Non-negotiable mechanics for “tool-native” orchestration

### ast-grep: inline multi-rule scanning + JSON streaming + metadata

* You can run **multiple rules** in a single inline rules string by separating YAML docs with `---`. ([AstGrep][1])
* Use `ast-grep scan --inline-rules … --json=stream` to get **one JSON object per match line** (streamable). ([AstGrep][1])
* JSON matches include `range` + `file` + optional `metaVariables` captures. Line/column offsets are **0-based**. ([AstGrep][2])
* You can embed custom `metadata:` in each rule, and with `--include-metadata` it appears in JSON output. ([AstGrep][1])
* CLI `--globs` supports include/exclude with `!` and uses gitignore-style matching—perfect for `in=subdir` / `exclude=` filters. ([AstGrep][1])

### ripgrep: file narrowing without JSON gotchas

* `rg --json` **cannot** be combined with flags that emit other output types like `--files-with-matches`, `--files`, `--count`, etc. ([Ripgrepy][3])
  So your tool-native planner does a **two-phase rg strategy**:

1. `rg --files-with-matches …` for file lists
2. optionally `rg --json …` later if you want match-level evidence records

---

## 1) High-level architecture: “declarative constructors → tool plan”

### Inputs (declarative)

You want something like:

```text
entity=function name=foo
need=callers(depth=2),def,loc
scope=in:src/x exclude:tests
```

### Plan (tool-native)

Compile that into a small fixed set of operators:

1. `RG_NARROW` → candidate files for “foo” + scope filters
2. `SG_EXTRACT` → run a **facts ruleset** over those files
3. `RELATE` → build caller graph / joins in-memory from extracted facts
4. optional `PY_ENRICH` → symtable/bytecode only for the few defs in the answer
5. `RENDER` → Markdown to stdout + (optional) JSON artifact

The important part: **Step (2) produces tables**, not prose.

---

## 2) The core trick: turn ast-grep into a “fact table generator”

You define a small number of *record types* you care about:

* `def` (function, async function, method, class)
* `call` (name call, attribute call)
* `import` (import, import-from, aliases)
* `except` / `raise`
* `assign_ctor` (x = Foo(...), x = mod.Foo(...))

…and you give each one a rule with metadata like:

```yaml
metadata:
  record: call
  kind: attr_call
```

Because ast-grep can include metadata in JSON output, you can treat every match as a typed record. ([AstGrep][1])

### Representative multi-rule inline YAML (Python)

This is “enough to implement”; you’ll extend patterns over time.

```yaml
# --- DEFs ---
id: py-def-fn
language: Python
metadata: { record: def, kind: function }
rule:
  pattern: def $F($$$ARGS): $$$BODY
---
id: py-def-async
language: Python
metadata: { record: def, kind: async_function }
rule:
  pattern: async def $F($$$ARGS): $$$BODY
---
id: py-def-class
language: Python
metadata: { record: def, kind: class }
rule:
  pattern: class $C($$$BASES): $$$BODY

# --- CALLs ---
---
id: py-call-name
language: Python
metadata: { record: call, kind: name_call }
rule:
  pattern: $F($$$ARGS)
---
id: py-call-attr
language: Python
metadata: { record: call, kind: attr_call }
rule:
  pattern: $OBJ.$M($$$ARGS)

# --- IMPORTs ---
---
id: py-import
language: Python
metadata: { record: import, kind: import }
rule:
  pattern: import $MOD
---
id: py-import-as
language: Python
metadata: { record: import, kind: import_as }
rule:
  pattern: import $MOD as $ALIAS
---
id: py-from-import
language: Python
metadata: { record: import, kind: from_import }
rule:
  pattern: from $MOD import $NAME
---
id: py-from-import-as
language: Python
metadata: { record: import, kind: from_import_as }
rule:
  pattern: from $MOD import $NAME as $ALIAS

# --- EXCEPT/RAISE ---
---
id: py-raise
language: Python
metadata: { record: raise }
rule:
  pattern: raise $E
---
id: py-except
language: Python
metadata: { record: except }
rule:
  pattern: except $E: $$$BODY

# --- CTOR assigns (type hints for var.method resolution) ---
---
id: py-ctor-assign-name
language: Python
metadata: { record: assign_ctor, kind: name_ctor }
rule:
  pattern: $V = $C($$$ARGS)
---
id: py-ctor-assign-attr
language: Python
metadata: { record: assign_ctor, kind: attr_ctor }
rule:
  pattern: $V = $MOD.$C($$$ARGS)
```

You run this as one scan: `ast-grep scan --inline-rules '<YAML>' --json=stream --include-metadata …` ([AstGrep][1])

---

## 3) Tool-native execution: orchestrator code

### (A) ripgrep narrowing operator

```python
from pathlib import Path
import subprocess

def rg_files_with_matches(root: Path, pattern: str, *, glob="*.py", in_dir=None, exclude=None) -> list[Path]:
    # For "scope", prefer restricting paths passed to rg rather than post-filtering.
    search_root = root / in_dir if in_dir else root

    cmd = ["rg", "--files-with-matches", "-g", glob, pattern, str(search_root)]
    # NOTE: do not use --json here; it's incompatible with --files-with-matches. :contentReference[oaicite:8]{index=8}
    p = subprocess.run(cmd, text=True, capture_output=True)
    if p.returncode not in (0, 1):  # 0 match, 1 no match
        raise RuntimeError(p.stderr)

    files = [Path(line) for line in p.stdout.splitlines() if line.strip()]

    if exclude:
        ex = tuple(exclude)
        files = [f for f in files if all(e not in str(f) for e in ex)]
    return files
```

### (B) ast-grep scan operator (JSON stream parser)

```python
import json, subprocess
from dataclasses import dataclass

@dataclass
class SgRecord:
    record: str
    kind: str | None
    file: str
    start_line: int
    start_col: int
    end_line: int
    end_col: int
    text: str
    meta: dict
    metas: dict  # metaVariables

def sg_scan_records(paths: list[Path], inline_rules_yaml: str, *, globs=None) -> list[SgRecord]:
    cmd = [
        "ast-grep", "scan",
        "--inline-rules", inline_rules_yaml,
        "--json=stream",           # stream requires '=' form :contentReference[oaicite:9]{index=9}
        "--include-metadata",      # include metadata in JSON :contentReference[oaicite:10]{index=10}
    ]
    if globs:
        for g in globs:
            cmd += ["--globs", g]  # include/exclude via ! supported :contentReference[oaicite:11]{index=11}

    cmd += [str(p) for p in paths]

    p = subprocess.Popen(cmd, text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    out: list[SgRecord] = []

    # ast-grep scan exit codes: 1 if any rule matches, 0 if none. :contentReference[oaicite:12]{index=12}
    assert p.stdout is not None
    for line in p.stdout:
        obj = json.loads(line)
        meta = obj.get("metadata") or {}
        rec = meta.get("record")
        kind = meta.get("kind")
        rng = obj["range"]
        out.append(SgRecord(
            record=rec,
            kind=kind,
            file=obj["file"],
            start_line=rng["start"]["line"],   # 0-based line/col :contentReference[oaicite:13]{index=13}
            start_col=rng["start"]["column"],
            end_line=rng["end"]["line"],
            end_col=rng["end"]["column"],
            text=obj.get("text", ""),
            meta=meta,
            metas=obj.get("metaVariables") or {},
        ))

    rc = p.wait()
    if rc not in (0, 1):
        raise RuntimeError(p.stderr.read() if p.stderr else "ast-grep failed")
    return out
```

### Why this is “native”

You’re not building your own parser for baseline structural facts. You’re letting ast-grep/tree-sitter do it and emitting streamable JSON objects. ([AstGrep][2])

---

## 4) The hard part: “caller graph” needs caller context

If you just extract `call` records, you still need to know **which function/method contains the call**.

You have two tool-native approaches:

### Option 1 (pure ast-grep): “call inside function” rule that captures caller name

You can add call rules that require the call to be `inside` a function def and capture `$CALLER`. The `inside/has/stopBy:end` relational style is idiomatic. ([AstGrep][4])

Example (sketch):

```yaml
id: py-call-name-with-caller
language: Python
metadata: { record: call, kind: name_call }
rule:
  pattern: $F($$$ARGS)
  inside:
    pattern: def $CALLER($$$P): $$$BODY
    stopBy: end
```

Then `metaVariables.single.CALLER.text` gives you the caller function name in the same match object (ast-grep includes captured metavariables in JSON output). ([AstGrep][2])

You can do the same for methods by adding an additional enclosing class capture (either via nested relational rules or by separately extracting def ranges and doing containment).

### Option 2 (still tool-native): interval containment join

Extract:

* `def` records with ranges
* `call` records with ranges

Then assign each call to the smallest enclosing def range via an interval tree. This is very robust and doesn’t require sophisticated ast-grep nesting.

Representative interval join:

```python
def build_interval_index(defs: list[SgRecord]):
    # defs are 0-based positions; compare by (line, col) tuples
    spans = []
    for d in defs:
        spans.append(((d.start_line, d.start_col), (d.end_line, d.end_col), d))
    # sort by start asc, end desc
    spans.sort(key=lambda x: (x[0], (-x[1][0], -x[1][1])))
    return spans

def contains(span, pt):
    (s, e, _) = span
    return s <= pt <= e

def assign_calls_to_defs(def_spans, calls: list[SgRecord]) -> dict[SgRecord, SgRecord | None]:
    # naive O(N*M) is fine for small sets; for big repos use an interval tree
    out = {}
    for c in calls:
        pt = (c.start_line, c.start_col)
        candidates = [d for d in def_spans if contains(d, pt)]
        out[c] = candidates[0][2] if candidates else None  # smallest enclosing first
    return out
```

This is still “native from ast-grep”: you’re only using Python to join tool-emitted facts.

---

## 5) Query compilation: constructors → operators → joins

Your **constructor language** can stay tiny (and agent-friendly) if it compiles to a *facts request*:

* Which record types do we need? (`def`, `call`, `import`, `raise`, `except`, `assign_ctor`)
* Which scopes/globs are in force?
* Which expansions require graph traversals? (`callers depth`, `imports depth`)

Example compilation:

```python
@dataclass
class Query:
    entity: str            # "function"|"class"|...
    name: str              # "foo"
    in_dir: str | None     # "src/x"
    exclude: list[str]
    want_callers_depth: int | None
    want_scope: bool
    want_bytecode: bool

def compile_to_plan(q: Query):
    # 1) narrow by name in requested scope
    rg_pat = rf"\b{re.escape(q.name)}\b"

    # 2) choose ast-grep ruleset: always defs+calls for caller graph queries
    need_defs = True
    need_calls = q.want_callers_depth is not None
    need_imports = False  # etc.

    return {
        "rg_pattern": rg_pat,
        "need": {"def": need_defs, "call": need_calls, "import": need_imports},
    }
```

Then execute:

1. `rg_files_with_matches(...)`
2. `sg_scan_records(...)` with ruleset reduced to needed record types
3. Build tables + traverse graph

### Callers query (tool-native) in practice

```python
def callers_of(q: Query, root: Path):
    files = rg_files_with_matches(root, rf"\b{re.escape(q.name)}\b",
                                  in_dir=q.in_dir, exclude=q.exclude)

    rules = build_rules_yaml(need={"def","call","import"})  # your YAML builder
    records = sg_scan_records(files, rules, globs=["*.py"])

    defs = [r for r in records if r.record == "def"]
    calls = [r for r in records if r.record == "call"]

    def_spans = build_interval_index(defs)
    call_to_def = assign_calls_to_defs(def_spans, calls)

    # Build reverse edges: callee_name -> caller_def_key
    # (callee name extraction is via metaVariables if you want; else parse r.text minimally)
    edges = defaultdict(set)
    for c in calls:
        caller_def = call_to_def.get(c)
        if not caller_def:
            continue
        callee = extract_callee_name(c)  # from metaVariables or text
        edges[callee].add(def_key(caller_def, root))

    # BFS up callers
    depth = q.want_callers_depth or 1
    out = set()
    frontier = {q.name}
    for _ in range(depth):
        nxt = set()
        for callee in frontier:
            for caller in edges.get(callee, []):
                if caller not in out:
                    out.add(caller)
                    nxt.add(simple_name(caller))
        frontier = nxt

    return out
```

---

## 6) Selective enrichment: symtable + bytecode as *post-filters*

Once you have a small set of target defs, enrich them.

### symtable (scope hazards)

Symtable is explicitly positioned as “compiler’s symbol tables” and is generated from AST right before bytecode generation. ([Python documentation][5])

```python
import symtable

def symtable_for_file(path: Path):
    src = path.read_text(encoding="utf-8", errors="replace")
    st = symtable.symtable(src, str(path), "exec")  # no imports/execution
    # walk st / children, collect free/global/nonlocal/etc
    return st
```

### bytecode “surface” (coupling)

You can compile source to a code object (no execution) and disassemble instructions. (Use `dis.get_instructions` in-process; the CLI exists too.) ([Python documentation][6])

```python
import dis

def bytecode_surface(path: Path):
    src = path.read_text(encoding="utf-8", errors="replace")
    co = compile(src, str(path), "exec")  # no execution
    globals_loaded = set()
    attrs_loaded = set()
    op_hist = {}
    for ins in dis.get_instructions(co):
        op_hist[ins.opname] = op_hist.get(ins.opname, 0) + 1
        if ins.opname in {"LOAD_GLOBAL", "LOAD_NAME"} and isinstance(ins.argval, str):
            globals_loaded.add(ins.argval)
        if ins.opname in {"LOAD_ATTR", "LOAD_METHOD"} and isinstance(ins.argval, str):
            attrs_loaded.add(ins.argval)
    return {"globals": sorted(globals_loaded), "attrs": sorted(attrs_loaded), "ops": op_hist}
```

---

## 7) How to make this “feel semantic” for agents without teaching ast-grep

The key UX win is: **one query constructor → one tool plan**.

In a `SKILL.md`, you expose *one* command:

* `/q "function foo callers=2 in=src/x"`

…and your tool:

* compiles it to `rg` + `ast-grep scan` + joins + optional enrich
* prints a structured Markdown report (with anchors + uncertainty notes)

Because the engine itself is always using the same operators, Claude doesn’t need to learn ast-grep deeply. It learns *your* mini constructor language.

---

## 8) A minimal “ruleset generator” you should actually implement

This is the last representative snippet that makes the whole approach maintainable:

```python
def build_rules_yaml(need: set[str]) -> str:
    docs = []
    def add(doc: str):
        docs.append(doc.strip() + "\n")

    if "def" in need:
        add("""
id: py-def-fn
language: Python
metadata: { record: def, kind: function }
rule: { pattern: "def $F($$$ARGS): $$$BODY" }
""")
        add("""
id: py-def-class
language: Python
metadata: { record: def, kind: class }
rule: { pattern: "class $C($$$BASES): $$$BODY" }
""")

    if "call" in need:
        add("""
id: py-call-name
language: Python
metadata: { record: call, kind: name_call }
rule: { pattern: "$F($$$ARGS)" }
""")
        add("""
id: py-call-attr
language: Python
metadata: { record: call, kind: attr_call }
rule: { pattern: "$OBJ.$M($$$ARGS)" }
""")

    if "import" in need:
        add("""
id: py-import-as
language: Python
metadata: { record: import, kind: import_as }
rule: { pattern: "import $MOD as $ALIAS" }
""")

    # Multiple rules separated by '---' is supported. :contentReference[oaicite:19]{index=19}
    return "\n---\n".join(docs)
```

If you want name filtering pushdown, use `constraints:` for single metavariables (it’s explicitly supported and applied after rule match). ([AstGrep][7])

---

## What I’d do next (keeps it “surgical”)

1. Implement the **facts ruleset** + interval join approach first (least fragile).
2. Add the pure ast-grep “call inside def captures caller” variant later as an optimization (fewer joins, richer metaVariables). ([AstGrep][4])
3. Add a tiny on-disk cache keyed by `(git_head, query_hash, tool_versions)` so repeated “semantic queries” are instant.


[1]: https://ast-grep.github.io/reference/cli/scan.html "ast-grep scan | ast-grep"
[2]: https://ast-grep.github.io/guide/tools/json.html "JSON Mode | ast-grep"
[3]: https://ripgrepy.readthedocs.io/?utm_source=chatgpt.com "Welcome to ripgrepy's documentation! — ripgrepy ..."
[4]: https://ast-grep.github.io/guide/rule-config.html?utm_source=chatgpt.com "Rule Essentials"
[5]: https://docs.python.org/3/library/symtable.html?utm_source=chatgpt.com "symtable — Access to the compiler's symbol tables"
[6]: https://docs.python.org/3/library/dis.html?utm_source=chatgpt.com "dis — Disassembler for Python bytecode"
[7]: https://ast-grep.github.io/reference/yaml.html "Configuration Reference | ast-grep"
