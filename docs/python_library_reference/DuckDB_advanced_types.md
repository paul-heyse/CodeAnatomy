# DuckDB Types API (Advanced Features)

## Overview of DuckDB Python Type System

DuckDB’s Python client uses the `DuckDBPyType` class to represent data types in DuckDB. This allows precise control over column types, especially for complex or nested types, directly from Python. The DuckDB Types API enables **implicit conversion** from common Python and NumPy types into DuckDB types, as well as **explicit construction** of advanced types (LIST, STRUCT, MAP, UNION, DECIMAL, etc.). This is crucial when creating tables, defining UDFs, or ensuring data schema consistency across systems. An expert familiar with Python and DuckDB should leverage this API to enforce correct typing and avoid unexpected type inference issues.

**Implicit type conversion:** In many DuckDB Python APIs, whenever a DuckDB type is expected, you can pass a Python type or NumPy dtype and DuckDB will automatically map it to the corresponding DuckDB type[\[1\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=The%20table%20below%20shows%20the,in%20types%20to%20DuckDB%20type)[\[2\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=Type%20DuckDB%20type%20bool%20BOOLEAN,BIGINT%20int8%20TINYINT%20uint16%20USMALLINT). For example:

- Passing a Python `int` where a DuckDB type is needed will be interpreted as DuckDB’s `BIGINT` type[\[1\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=The%20table%20below%20shows%20the,in%20types%20to%20DuckDB%20type). A `str` becomes `VARCHAR`, `bool` becomes `BOOLEAN`, and `float` becomes `DOUBLE`.
- NumPy data types are also recognized: e.g. `numpy.int8` -\> `TINYINT`, `numpy.int16` -\> `SMALLINT`, `np.int64` -\> `BIGINT`; similarly `float32` -\> `FLOAT`, `float64` -\> `DOUBLE`[\[2\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=Type%20DuckDB%20type%20bool%20BOOLEAN,BIGINT%20int8%20TINYINT%20uint16%20USMALLINT). This implicit mapping makes it easy to specify schema using familiar types.

**DuckDBPyType and duckdb.sqltypes:** Under the hood, these mappings produce a `DuckDBPyType` instance, which is DuckDB’s Python type wrapper class. You can also directly use the predefined DuckDB types from the `duckdb.sqltypes` module to be explicit about types. For instance, `duckdb.sqltypes.INTEGER` or `duckdb.sqltypes.VARCHAR` can be used to denote those column types precisely. This is often used when registering Python functions (UDFs) or creating tables with specific type requirements. Using the `duckdb.sqltypes` constants is clearer and ensures the intended DuckDB type is used (for example, distinguishing between `INTEGER` vs `BIGINT`, or specifying `HUGEINT` for 128-bit integers).

**Type inference from annotations:** DuckDB can also **infer types from Python type hints** in function annotations. When defining a Python UDF (see below), if your function is annotated with Python types, you can pass `None` for the parameter types to `create_function()` and DuckDB will deduce the DuckDB types automatically[\[3\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=data%20types%20for%20the%20input,By). For example, a function annotated with `x: int -> int` will be registered with `INTEGER` inputs and output if no explicit types are given[\[3\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=data%20types%20for%20the%20input,By). This convenience helps integrate with Python’s typing to reduce boilerplate.

## Mapping and Constructing Complex Types

DuckDB’s type system supports **nested and complex types** (STRUCTs, LISTs, MAPs, UNIONs, DECIMAL, etc.), and the Python API provides ways to construct these types.

- **List (ARRAY) Types:** A Python `list[T]` in a type context will map to DuckDB’s `LIST` of type `T`. This can be nested arbitrarily (e.g. `list[int]` becomes `LIST<INT>`, and `list[list[str]]` would become `LIST<LIST<VARCHAR>>`). For example, using Python’s typing syntax: `duckdb.sqltypes.DuckDBPyType(list[int])` yields a `LIST` of BIGINT[\[4\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=Nested%20Types). You can also use a helper function like `duckdb.list_type(child_type)` or `duckdb.array_type(child_type)` to explicitly create a list type[\[5\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=).

- **Maps:** A Python `dict[key_type, value_type]` as a type will convert to DuckDB’s `MAP<key_type, value_type>`. For instance, `DuckDBPyType(dict[str, int])` results in `MAP(VARCHAR, BIGINT)`[\[6\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=). This is how you can represent key-value pairs. Programmatically, `duckdb.map_type(key_type, value_type)` can be used to construct a map type object[\[7\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=).

- **Structs:** A Python dict literal defining a structure, e.g. `{'a': str, 'b': int}`, will map to a DuckDB `STRUCT(a VARCHAR, b BIGINT)`[\[8\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=,and%20values%20of%20the%20dict). The keys become field names and their Python types are converted to DuckDB field types. You can also provide a list of field types or a dict for named fields to `duckdb.struct_type` (also aliased as `row_type`)[\[9\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=). This is useful for representing nested records or tuples with named fields.

- **Union (Tagged Union):** Using `typing.Union` in a type context will produce DuckDB’s `UNION` type (a discriminated union of multiple types). For example, `Union[int, str, bool]` becomes a DuckDB `UNION(u1 BIGINT, u2 VARCHAR, u3 BOOLEAN)`[\[10\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=). This allows a column to hold values of different types with tags. You may construct a union type explicitly via `duckdb.union_type([type1, type2, ...])` or by providing a dict of tags to types[\[11\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=).

- **Decimals:** DuckDB supports fixed-point decimals. Python’s built-in `Decimal` type isn’t directly mapped by default, but you can explicitly create a DuckDB decimal type by specifying precision and scale. The function `duckdb.decimal_type(width, scale)` returns a `DuckDBPyType` for a DECIMAL of the given width (total digits) and scale (digits after decimal)[\[12\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=). This is crucial if you need exact decimal arithmetic or specific numeric precision (e.g., currency values) in your schema. If you have a pandas Series with dtype `Decimal`, ensure to convert it appropriately or specify the target DECIMAL type when creating a table.

- **UUID, DATE, etc.:** DuckDB has specialized types like UUID, HUGEINT (128-bit int), etc. These can be accessed via `duckdb.sqltypes.UUID`, `HUGEINT`, etc., from the `sqltypes` module[\[13\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=DuckDB%20type%20BIGINT%20BIT%20BLOB,DATE%20DOUBLE%20FLOAT%20HUGEINT%20INTEGER)[\[14\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=TIMESTAMP_NS%20TIMESTAMP_S%20TIMESTAMP_TZ%20TIMESTAMP%20TINYINT,UBIGINT%20UHUGEINT%20UINTEGER%20USMALLINT%20UTINYINT). Most are straightforward, but note DuckDB’s time and timestamp types: e.g., `TIMESTAMP` vs `TIMESTAMP_TZ` (with timezone), and variants like `TIMESTAMP_S` (seconds precision), `TIMESTAMP_NS` (nanoseconds precision) are all available as constants[\[15\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=INTERVAL%20SMALLINT%20SQLNULL%20TIME_TZ%20TIME,TIMESTAMP_MS%20TIMESTAMP_NS%20TIMESTAMP_S%20TIMESTAMP_TZ%20TIMESTAMP). Use these when you need a specific temporal precision or timezone awareness.

**Using types in practice:** The Types API is used whenever you need to **define or enforce a schema** in Python code:

- **Creating tables with specific schema:** If you want to create an empty table or one from Python data but control column types, you can use a DDL with type names or the Python API equivalents. For example, `conn.execute("CREATE TABLE t(x INT, y VARCHAR)")` works, but you could also do `conn.register('my_df_view', df)` and then `conn.execute("CREATE TABLE t AS SELECT CAST(x AS INTEGER), CAST(y AS VARCHAR) FROM my_df_view")` to ensure types. A more direct way is to specify `dtype` in `duckdb.from_df()` if supported, or cast within a relation projection using the Expression API (see below).

- **Function parameters and UDFs:** When registering Python functions as SQL UDFs using `conn.create_function`, you must specify the input and output DuckDB types, unless you rely on annotation inference. For example, if not using annotations, you might do: `conn.create_function('myfunc', myfunc, [duckdb.sqltypes.DOUBLE, duckdb.sqltypes.VARCHAR], duckdb.sqltypes.BOOLEAN)`, which declares a function taking (DOUBLE, VARCHAR) and returning BOOLEAN. Mismatched types or omitting them can lead to runtime errors, so the Types API is critical here.

- **Casting and Expression API:** DuckDB’s Expression API (covered next) allows applying `.cast(type)` on an expression to convert it to a desired type[\[16\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=The%20Expression%20class%20also%20contains,applied%20to%20any%20Expression%20type). For that, you’ll often use a DuckDBPyType to specify the target type. For example, `.cast(duckdb.sqltypes.INTEGER)` will cast an expression to INT32. This is particularly important when dealing with Union or ambiguous types in expressions.

In summary, the DuckDB Types API provides a **bridge between Python’s types and DuckDB’s richer type system**, enabling advanced schema definitions and ensuring that data moves between Python and DuckDB with the intended types. Being familiar with it prevents issues like unexpected type promotion or precision loss, and unlocks usage of DuckDB’s nested data capabilities fully within Python.

## Examples of Type Usage

Let’s illustrate a couple of scenarios:

**Nested type example:** Suppose you want to create a DuckDB table with a column that is a list of integers and another that is a map from string to int. You can construct those types and create a table as follows:

    import duckdb
    conn = duckdb.connect(':memory:')

    # Define complex types using DuckDBPyType
    list_of_ints = duckdb.list_type(duckdb.sqltypes.INTEGER)        # LIST<INTEGER>
    map_str_int = duckdb.map_type(duckdb.sqltypes.VARCHAR, duckdb.sqltypes.INTEGER)  # MAP<VARCHAR, INTEGER>

    # Use these types in a table creation
    conn.execute("CREATE TABLE example(col1 LIST<INT>, col2 MAP<VARCHAR, INT>)")
    # Alternatively, using parameters in Python API:
    conn.execute("CREATE TABLE example(col1 $1, col2 $2)", [list_of_ints, map_str_int])

Here we used the **type objects** `list_of_ints` and `map_str_int` to parameterize the SQL. DuckDB’s parameter binding recognizes DuckDBPyType and will create the table with the correct complex types. This approach ensures the Python-side types are communicated to DuckDB correctly, avoiding the need to hard-code the type strings in SQL. (Note: The second method uses parameter binding with `$1, $2` placeholders, which DuckDB supports for schema DDL as well.)

**Union type example:** If you have data where a column can hold either an integer or text, you might define it as a UNION type. Using Python’s typing:

    from typing import Union
    union_type = duckdb.sqltypes.DuckDBPyType(Union[int, str])
    print(union_type)  # This would output something like: UNION(u1 BIGINT, u2 VARCHAR)

You could then use `union_type` in a table or function definition. If inserting Python data into such a column, ensure to wrap values as DuckDB Value objects if needed or let DuckDB cast.

**Decimal usage:** To avoid floating-point issues, you might choose a DECIMAL type for financial data. For example:

    price_type = duckdb.decimal_type(9, 2)  # DECIMAL(9,2) for prices up to 7 digits + 2 decimal places
    conn.execute("CREATE TABLE sales(item VARCHAR, price $1)", [price_type])

This creates a table with a `price` column as DECIMAL(9,2). Any insertion of a Python `float` or `Decimal` should then respect that scale (DuckDB will cast Python floats to DECIMAL automatically in this context).

**Takeaway:** The Types API gives a **fine-grained control** over data representation in DuckDB. When designing a complex system that builds higher-order data (like graphs composed of nested structures), using these type controls ensures that when you store or pass data through DuckDB, it retains the structure you intend. An expert user can thereby plan the schema early (e.g., if you know a graph adjacency list should be a LIST of nodes, or properties should be a STRUCT) and implement it with DuckDBPyType, rather than discovering later that a generic type was used and needing a redesign.

# DuckDB Expression API (Building SQL Expressions in Python)

## Why Use the Expression API?

The **Expression API** in DuckDB’s Python client allows you to construct SQL expressions programmatically, rather than writing them as strings inside SQL queries[\[17\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=Why%20Would%20I%20Use%20the,Expression%20API). This feature is highly significant for dynamic query generation and for integrating with Python code that may generate complex filtering or computation logic. Instead of concatenating SQL strings (which can be error-prone or unsafe), you build an expression tree using Python objects. DuckDB’s query engine then treats these as if they were part of a SQL query.

By using the Expression API, you gain **fine-grained control** over query construction. You can dynamically assemble expressions (conditions, calculations, function calls, etc.) based on program logic, all while ensuring they are valid and safe (DuckDB will handle proper quoting and types internally). This is analogous to building an abstract syntax tree (AST) for a SQL query in Python. It enables advanced scenarios like generating queries from user input in a safe way, reusing expression objects in multiple queries, or programmatically optimizing certain operations.

The `Expression` class is the base; specific subclasses correspond to different kinds of SQL expressions (columns, constants, function calls, etc.)[\[18\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=The%20,instance%20of%20an%20expression). These objects can be passed to DuckDB’s relation methods (like `select`, `filter`, etc.) or combined to form larger expressions.

## Core Expression Types and Usage

DuckDB’s Python API provides several expression classes out-of-the-box:

- **ColumnExpression:** References a column by name[\[19\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=Column%20Expression). You instantiate it with the column name. For example, `col_expr = duckdb.ColumnExpression('amount')`. This object represents the column “amount” in whatever context it’s used (table or subquery). It can be used in `select()`, `filter()`, joins, etc. If you use it in a relation without qualification, DuckDB assumes it refers to a column of the current relation.

- **ConstantExpression:** Encapsulates a literal value[\[20\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=This%20expression%20contains%20a%20single,value)[\[21\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=const%20%3D%20duckdb). E.g., `c = duckdb.ConstantExpression(42)` represents the constant value 42. This is useful for selecting literal values, comparing a column to a constant, or as an argument to a function expression. The constant can be of any type (string, number, boolean, etc.). DuckDB will infer the type (`VARCHAR` for strings, etc.) for the constant. For instance, `duckdb.ConstantExpression('hello')` produces a constant of type VARCHAR[\[21\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=const%20%3D%20duckdb). In a query result, a constant expression will produce that value for every row (much like `SELECT 'hello' FROM table` would yield 'hello' for each row).

- **StarExpression:** Represents the `*` (wildcard) in a select-list[\[22\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=Star%20Expression)[\[23\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=star%20%3D%20duckdb.StarExpression%28exclude%20%3D%20,show). Normally, writing `SELECT *` selects all columns. Using `duckdb.StarExpression()` allows you to programmatically include all columns of the input relation. You can also provide an `exclude` list to `StarExpression(exclude=[...])` to omit certain columns[\[24\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=This%20expression%20selects%20all%20columns,of%20the%20input%20source)[\[23\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=star%20%3D%20duckdb.StarExpression%28exclude%20%3D%20,show). The exclude can be specified by name or even by expression. For example, `duckdb.StarExpression(exclude=['id'])` would select all columns except “id”. This is particularly helpful if you want most columns but need to drop or modify a few in the projection.

- **FunctionExpression:** Represents a call to a SQL function (built-in or UDF)[\[25\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=Function%20Expression)[\[26\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=). You create it with the function name and a list of argument expressions. For instance, `duckdb.FunctionExpression('POWER', duckdb.ColumnExpression('x'), duckdb.ConstantExpression(2))` would correspond to the SQL expression `POWER(x, 2)`. If you’ve registered a Python UDF named “myfunc”, you could do `FunctionExpression('myfunc', arg1, arg2)`. The number and types of arguments should match the function’s expectation. In the example in DuckDB docs, `FunctionExpression('multiply', ColumnExpression('a'), ConstantExpression(2))` yields the expression `multiply(a, 2)`[\[26\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=). (Note: “multiply” could be a UDF or built-in; DuckDB will resolve it when executing the query.)

- **CaseExpression:** Represents a `CASE WHEN ... THEN ... [ELSE ...] END` conditional expression[\[27\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=Case%20Expression)[\[28\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=hello%20%3D%20ConstantExpression,world). You can construct it by specifying a condition (an Expression yielding boolean) and a result value Expression if the condition is true. The `CaseExpression` can be chained with `.when()` for additional conditions and `.otherwise()` (or `.else()`) for the else-case[\[27\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=Case%20Expression). By default, if no ELSE is provided, it yields NULL when no condition matches[\[29\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=This%20expression%20contains%20a%20,value%20%3D). For example:

<!-- -->

    from duckdb import CaseExpression, ColumnExpression, ConstantExpression
    case_expr = CaseExpression(condition=ColumnExpression('status') == ConstantExpression('ok'),
                               value=ConstantExpression(1)) \
                   .when(condition=ColumnExpression('status') == ConstantExpression('error'),
                         value=ConstantExpression(0)) \
                   .otherwise(ConstantExpression(-1))

This would correspond to `CASE WHEN status = 'ok' THEN 1 WHEN status = 'error' THEN 0 ELSE -1 END`. In the simpler example from docs, they used `CaseExpression(condition=ColumnExpression('b') == False, value=world).otherwise(hello)` which effectively checks `b == false` then yields "world", else "hello"[\[28\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=hello%20%3D%20ConstantExpression,world). Note that `ColumnExpression('b') == False` uses Python’s comparison operator to produce a DuckDB boolean expression (this works because the Expression classes implement rich comparison methods). This **overloading** means you can write `ColumnExpression('x') + 5` to get an ArithmeticExpression representing `x + 5`, or `colA == colB` to get a ComparisonExpression for equality. Under the hood, these operators (`__add__`, `__eq__`, etc.) are overloaded to return combined Expression objects.

- **SQLExpression:** A convenient escape hatch that allows you to wrap an arbitrary SQL snippet as an Expression[\[30\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=match%20at%20L652%20SQLExpression%28,1%20then%201%20else%200)[\[31\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=SQL%20Expression). For example, `duckdb.SQLExpression("LOWER(name)")` yields an Expression that DuckDB will treat as the SQL `LOWER(name)`. This can be useful if the API doesn’t (yet) cover a specific kind of expression or for quick inline usage. However, one must ensure the snippet is valid and any identifiers are properly quoted if needed. It’s basically telling DuckDB “insert this expression as-is”. In practice, you might rarely need this if using other Expression classes, but it’s available. The docs show using `SQLExpression("b is true")` as a filter condition and also constructing multiple select expressions with SQL text and aliasing them[\[30\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=match%20at%20L652%20SQLExpression%28,1%20then%201%20else%200)[\[32\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=duckdb.df%28df%29.filter%28%20SQLExpression%28,%29.aggregate). In those examples, it was used to mix literal SQL and then `.alias("name")` to label them in the result.

## Combining Expressions and Operations

The power of the Expression API comes from combining these pieces using both the provided methods and Python operators:

- **Arithmetic and Comparison:** As noted, `ColumnExpression` instances and other expressions support Python arithmetic operators (`+`, `-`, `*`, `/`) and comparison operators (`==`, `!=`, `<`, `>=`, etc.). For example, `duckdb.ColumnExpression('price') * ConstantExpression(1.2)` produces an expression representing a 20% increase of the price. `ColumnExpression('age') >= ConstantExpression(18)` yields a boolean expression for adulthood. These can be directly passed to `filter()` (where a boolean Expression is expected) or to select/project (where any scalar expression is allowed). Under the hood, these become the corresponding SQL expression (e.g., `price * 1.2` or `age >= 18`). This means you can build quite complex expressions purely in Python using natural syntax, and DuckDB will incorporate them into the query plan.

- **Logical operations:** You can combine boolean expressions with Python’s bitwise operators as logical AND/OR/NOT (since Python’s `and/or` cannot be overridden). In DuckDB’s Expression API, `&` corresponds to SQL AND, `|` to SQL OR, and `~` (bitwise NOT) to SQL NOT. For example: `(ColumnExpression('x') > 0) & (ColumnExpression('y') > 0)` yields a combined condition “x \> 0 AND y \> 0”. This is intuitive for those familiar with pandas or NumPy where elementwise logical ops use `&`/`|`. Just be careful with operator precedence and always parenthesize compound conditions.

- **Alias and Cast:** Any Expression can have an alias applied via `.alias("new_name")`. This is equivalent to writing `expr AS new_name` in SQL. For example, `expr = (ColumnExpression('a') + 5).alias('a_plus_5')` can then be used in a select to give that computed column a name[\[33\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=SQLExpression%28,1%20then%201%20else%200). Casting is done via `.cast(DuckDBPyType)`, as mentioned earlier. If you want to force the type of an expression, e.g., treat a VARCHAR as DATE, you might do `ColumnExpression('date_str').cast(duckdb.sqltypes.DATE)`. This would produce a runtime conversion in the query (and will throw an error if the conversion is invalid at execution time).

- **Null-checking:** Expressions have methods `.isnull()` and `.isnotnull()` to produce IS NULL/IS NOT NULL checks[\[34\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=col_list%20%3D%20%5B%20duckdb.ColumnExpression%28%27a%27%29%20,col_list%29.show)[\[16\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=The%20Expression%20class%20also%20contains,applied%20to%20any%20Expression%20type). In the earlier example, `duckdb.ColumnExpression('b').isnull()` was used to get a boolean expression that is true if `b` is NULL[\[35\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=col_list%20%3D%20%5B%20duckdb.ColumnExpression%28%27a%27%29%20,col_list%29.show). This is more convenient and less error-prone than writing a raw SQL string for the same. Similarly `.isin(*exprs)` and `.isnotin(*exprs)` will generate an `IN (...)` list membership check against multiple expressions or values[\[16\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=The%20Expression%20class%20also%20contains,applied%20to%20any%20Expression%20type). For instance, `ColumnExpression('category').isin(ConstantExpression('A'), ConstantExpression('B'))` yields `category IN ('A','B')`. These are very useful for filtering against multiple allowed values.

- **Order modifiers:** While not exactly separate expression types, DuckDB allows you to specify ordering null placement and direction by applying `.asc()`, `.desc()`, `.nulls_first()`, `.nulls_last()` on an expression[\[36\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=Order%20Operations). These are used when you pass expressions to a `relation.order()` call. For example: `relation.order(ColumnExpression('timestamp').desc().nulls_last())` would sort by timestamp descending with NULLs at the end. These methods simply annotate the expression with the desired ordering, to be interpreted in the context of sorting.

**Example – Building a dynamic query:** Consider you have a DataFrame `df` with columns `value` and `flag`, and you want to select either `value` or 0 based on `flag`, and also compute a ratio. Using the Expression API:

    from duckdb import df as duckdf, ColumnExpression, CaseExpression, ConstantExpression, FunctionExpression

    # Assuming df is a pandas DataFrame or similar with 'value', 'flag'
    rel = duckdf(df)  # create relation from DataFrame

    # Build expressions
    val_col = ColumnExpression('value')
    flag_col = ColumnExpression('flag')
    zero_const = ConstantExpression(0)
    conditional_val = CaseExpression(condition = flag_col == ConstantExpression(True),
                                     value = val_col).otherwise(zero_const)  # if flag is true, use value, else 0
    ratio_expr = FunctionExpression('div', val_col, ConstantExpression(100))  # call SQL division: div(value, 100)

    # Use expressions in the query
    result_rel = rel.project(conditional_val.alias('filtered_value'), ratio_expr.alias('value_ratio'))
    print(result_rel.to_df())

In this snippet, we constructed `conditional_val` as a CASE expression and `ratio_expr` as a function call (assuming DuckDB has a `div` function or we could simply use `val_col / ConstantExpression(100)` to rely on operator overloading). We then projected those as new columns. No SQL string was written by hand – the construction is entirely via Python objects. DuckDB will compile this into a single SQL query equivalent to:

    SELECT CASE WHEN flag = TRUE THEN value ELSE 0 END AS filtered_value,
           div(value, 100) AS value_ratio
    FROM df;

and execute it when `to_df()` or another materialization is called.

This approach is extremely powerful when the query needs to be assembled based on varying conditions (for instance, building different filters based on user inputs, adding/removing columns dynamically, etc.) while ensuring correctness and avoiding SQL injection issues.

## Using Expressions in the Relational API

The Expression API is designed to work hand-in-hand with DuckDB’s Relation (query building) API. All relational algebra methods (`filter`, `project`, `aggregate`, `join`, etc.) accept either raw SQL strings **or** Expression objects. This means you can mix and match. For example, you could do:

    rel.filter(ColumnExpression('age') > ConstantExpression(30))          # Expression in filter
       .project("name, age")                                              # String in project
       .filter("name IS NOT NULL")                                        # String in filter
       .project((ColumnExpression('age') + 5).alias('age_plus_five'))     # Expression in project

This flexibility allows gradually transitioning part of a query to programmatic construction. Using an Expression for a complex computed column while using a simple string for a straightforward condition is perfectly fine.

One important note: when using the `aggregate` method in the relation API, it has parameters `aggr_expr` and `group_expr` which can accept Expression objects. For instance:

    rel.aggregate(
        aggr_expr=[duckdb.SQLExpression("SUM(value)").alias("total_value"), ConstantExpression(1).alias("const_col")],
        group_expr=["category"]  # group by category column (can be string or Expression)
    )

In the above, we mixed a SQLExpression for summation and a ConstantExpression as a dummy aggregated column, and used a string for grouping. DuckDB will aggregate accordingly. The documentation example shows using `SQLExpression` with alias and also that you can just pass column names as strings in the list[\[37\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=SQLExpression%28). Each element of these lists can be a string (interpreted as a column name or alias) or an Expression.

**Performance consideration:** The Expression API, by building an AST, allows DuckDB to avoid re-parsing portions of the query repeatedly if you reuse them. For example, if you create a complex expression once and reuse it in multiple queries or multiple places in the same query, you ensure consistency and potentially save parse time. However, be mindful that overly dynamic expression generation can complicate the query plan readability. For debugging, you can always do `relation.explain()` on the final relation to see the query plan, and the expressions will appear in the plan output.

In summary, DuckDB’s Expression API opens up **dynamic SQL composition** in Python while maintaining the full power of DuckDB’s query optimizer. This is especially useful in an evolving product or complex system, where you might need to generate queries to analyze progressively higher-order derived data (graphs from parsed code in your case) on the fly. By using expressions and the relational API together, you can avoid future costly redesigns: all the options for creating computed columns, conditional logic, and custom function calls in queries are available as Python constructs from the start.

# DuckDB Connection API – Advanced Operations

## Managing Multiple Databases in One Environment

One of DuckDB’s unique capabilities is to **attach multiple databases** to a single connection. This allows a single DuckDB query to access tables from different DuckDB files (or even other database systems via extensions) concurrently. In a complex system, this means you can organize data in separate DuckDB files (for modularity or concurrency reasons) and still query across them without manual data movement.

**Attaching additional databases:** Use the SQL `ATTACH` command to attach another DuckDB database file to the current connection, giving it an alias. For example:

    ATTACH 'analytics.duckdb' AS analytics_db;
    ATTACH 'metrics.duckdb' AS metrics_db (READ_ONLY);

This will bring the contents of those DuckDB files into the current connection’s context, namespacing them under `analytics_db` and `metrics_db` respectively[\[38\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=The%20ATTACH%20statement%20adds%20another,be%20queried%20with%20that%20qualifier)[\[39\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=SELECT%20s,customer_id). Tables in the attached databases can be referenced with the alias as a schema qualifier (e.g., `analytics_db.some_table`). You can attach as many databases as needed. The `(READ_ONLY)` option, as shown, attaches in read-only mode which is safer for concurrent access when you don’t need to modify the attached database[\[40\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Here%20sales_db,only%5B94%5D%5B95). If you omit `AS name`, DuckDB will auto-assign the alias based on the filename (e.g., attaching `data.duckdb` becomes alias `data`)[\[41\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=only).

**Cross-database queries:** Once attached, you can perform joins or selects across databases easily. For instance, suppose `sales_db` and `analytics_db` are attached:

    SELECT s.order_id, s.amount, a.region
    FROM sales_db.orders s
    JOIN analytics_db.regions a ON s.region_id = a.id;

This query joins a table from the sales database with a table from the analytics database, demonstrating seamless cross-DB querying[\[42\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=can%20do%20cross)[\[43\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=ATTACH%20%27s3%3A%2F%2Fmy). DuckDB’s engine handles this internally, so you don’t have to manually combine data. This is extremely helpful if you’re parsing data into separate logical databases (perhaps each phase of graph construction is a separate DuckDB file for modularity) but then need to combine results.

**Data transfer between attached DBs:** DuckDB provides efficient means to copy or move data between attached databases: - You can directly **create a table in one database from a select on another**. For example: `CREATE TABLE analytics_db.new_table AS SELECT * FROM main_db.existing_table;` will bulk-copy data from the main connection’s database into the attached analytics_db[\[44\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Data%20transfer%20between%20DBs%3A%20Attaching,2%2B%20introduced%20COPY%20FROM%20DATABASE). Similarly, `INSERT INTO analytics_db.table SELECT * FROM other_db.table;` works. - **COPY FROM DATABASE:** As of DuckDB 1.2+, a command `COPY FROM DATABASE source_db_alias TO target_db_alias;` can copy all tables from one attached database to another in one go[\[44\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Data%20transfer%20between%20DBs%3A%20Attaching,2%2B%20introduced%20COPY%20FROM%20DATABASE). This can be filtered by specifying specific tables or schemas. It’s a convenient way to snapshot or merge entire databases. - After finishing, you can `DETACH analytics_db;` to detach the database and release it from the current connection[\[45\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Inferring%20alias%3A%20If%20you%20do,96).

**Attaching other database systems:** Through extensions, DuckDB can attach non-DuckDB databases. For example, if you load the `sqlite` extension (`LOAD sqlite;`), you can attach a SQLite file: `ATTACH 'legacy.db' AS legacy (TYPE sqlite);`[\[46\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Additionally%2C%20ATTACH%20supports%20attaching%20non,databases%20via%20extensions). This mounts the SQLite database’s tables as read-only tables in DuckDB. Similarly, with the Postgres or MySQL extensions, you can attach those systems (the syntax uses a connection string or parameters). This can broaden your system by allowing DuckDB to query data in other sources. In an advanced setup, you might use DuckDB as a query engine that federates across its own storage, SQLite, and others.

**Encrypted databases:** DuckDB supports encrypted database files. If you have an encrypted DuckDB file, you attach it by providing the encryption key: e.g., `ATTACH 'secret.duckdb' AS secret (ENCRYPTION_KEY 'mySecretPwd');`[\[47\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Encryption%3A%20DuckDB%20supports%20transparent%20encryption,you%20supply%20an%20encryption%20key). Once attached with the correct key, DuckDB transparently decrypts/encrypts data. This is critical if your system demands at-rest encryption for sensitive data. All SQL operations work the same; the encryption is handled at the storage layer. (Note: The `httpfs` extension must be loaded for optimized AES encryption, which is typically included by default in DuckDB Python.)

**Importing and exporting entire databases:** If you need to **migrate or backup** a DuckDB database (especially between versions), DuckDB offers: - `EXPORT DATABASE 'dir_path' (FORMAT parquet);` which will export the current database’s contents to a directory in a specified format[\[48\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=When%20upgrading%20DuckDB%20versions%20or,EXPORT%20DATABASE%20and%20IMPORT%20DATABASE)[\[49\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=EXPORT%20DATABASE%20%27backup_dir%27%20). By default, it exports as a set of CSV files plus SQL scripts, but as shown, you can choose `FORMAT PARQUET` for tables. This essentially dumps every table (and related objects) in a portable form. - `IMPORT DATABASE 'dir_path';` does the inverse, reading an exported directory to load data into the current database[\[50\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=IMPORT%20DATABASE%20%27dir_path%27%20does%20the,115). This is version-safe and a reliable way to upgrade (export from old version, import into new version) or to bulk-load a lot of data. In context, if your design evolves and you need to merge or split databases, these commands provide a straightforward method without writing custom export code.

In an advanced pipeline, these features mean you can **structure multi-stage data processing** in separate DuckDB files (for isolation or parallel development) and combine them when needed, or maintain archives of data in file snapshots.

## Transaction Management and Concurrency

DuckDB operates with a **single-writer, multi-reader** concurrency model for a database file. By default, DuckDB’s Python API runs each SQL statement in its own transaction (auto-commit mode)[\[51\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Auto,commit%28%29%20methods%20if%20needed), meaning modifications are immediately committed. This is usually fine for analytical workloads, but you can manually control transactions if needed: - Use `conn.begin()` / `conn.commit()` (or the SQL `BEGIN/COMMIT`) to group multiple statements into one transaction. E.g., if you need to bulk insert into multiple tables and ensure either all or none are committed, explicitly begin a transaction. - DuckDB’s transactions are **ACID** for a single file. When attaching multiple DBs, each one maintains separate transactional context (no distributed transactions across attached DBs).

**Multiple connections and threading:** You can have multiple connections to the same DuckDB file, even within one Python process or across processes: - In one process, calling `duckdb.connect('my.db')` twice yields two connections. One can write at a time, but others can read in parallel. This is suitable for a scenario where perhaps one thread is updating a table while another thread is running read-only queries (the readers will see a consistent snapshot). - A single `DuckDBPyConnection` object is **thread-safe** for queries, but it will internally serialize execution (i.e., it won’t run two queries truly concurrently on one connection; it queues them)[\[52\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Multiple%20cursors%3A%20DuckDB%20allows%20multiple,safe%20and%20will%20serialize). If you want parallel query execution in the same process, use multiple connections. - You can also spawn **multiple cursors** from one connection (`conn.cursor()` returns another handle on the same underlying connection)[\[53\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Multiple%20cursors%3A%20DuckDB%20allows%20multiple,if%20used%20from%20multiple%20threads). This can be useful if an API expects a DB-API cursor object, but in DuckDB’s case, the connection itself acts as a cursor. Multiple cursors on the same connection will still execute sequentially, but they can have independent state (like their own iterator over result sets).

In a complex system where threads might be producing and consuming data, you might use one connection dedicated to writes (ingestion of new parsed data), and others for reads (serving queries to users or other parts of the system). Because DuckDB is in-process, there’s no separate server coordinating; you ensure consistency by using transactions appropriately. If you open the database in `read_only=True` mode on some connections, those can safely run alongside a writer without risking interference (attempts to write in read-only mode will error out).

## Performance and Resource Tuning

DuckDB is designed to automatically utilize resources, but you have control via the Connection API to tune performance: - **Parallel Threads:** DuckDB will use multiple threads for query execution by default (especially for large scans or joins). By default it often uses the number of CPU cores. You can override this. For example, running `SET threads = 4;` on the connection will restrict DuckDB to 4 threads for subsequent queries[\[54\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Parallel%20Threads%3A%20DuckDB%20will%20use,You%20can%20change%20it). In Python, you can also specify at connection time: `duckdb.connect(..., config={'threads': 8})` to set an 8-thread limit. Tuning thread count can help if you need to share CPU with other tasks or if hyper-threading makes diminishing returns beyond a certain point. Heavy OLAP queries often benefit from using all cores, but if you are doing smaller operations, capping threads avoids overhead of thread startup. - **Memory Limit:** By default DuckDB will try to use all available memory (it manages memory for cache, processing etc., and will spill to disk if needed). To prevent it from consuming too much in a multi-tenant environment, you can set `memory_limit` in the config. For instance, `SET memory_limit='2GB';` on the connection ensures DuckDB will attempt to keep memory usage under 2 GB[\[55\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Memory%20Limit%3A%20By%20default%2C%20DuckDB,You%20can%20impose%20a%20limit). It’s a soft limit (DuckDB will try to spill intermediate data to disk if the limit is hit, rather than erroring out immediately). You can also set this in `duckdb.connect(config={'memory_limit': '2GB'})`. For your use case of graph processing with not-huge data, you might not hit memory issues, but it’s good to know if you co-locate DuckDB with other processes. - **Temporary storage (spill) location:** By default DuckDB uses the system temp directory for large operations that need disk (e.g., sorting a very large table might use external merge sort on disk). You can configure a specific temp directory with `SET temp_directory='/path/to/tmp'`[\[56\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Temporary%20Directory%3A%20DuckDB%20uses%20disk,no%20spilling%20to%20disk). If you want to enforce in-memory operation only, `SET temp_directory=''` will disable spilling to disk entirely (which can cause out-of-memory errors if data truly doesn’t fit, but guarantees no temp files are written). Setting a custom temp directory can be useful to ensure spills go to a fast disk (SSD) or a location with sufficient space. - **Progress bar:** In interactive use, enabling `SET enable_progress_bar=true;` will show a progress bar for long queries[\[57\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Progress%20Bar%3A%20For%20long,Enable%20it%20with). In headless systems or Jupyter notebooks this might not be very visible until completion. It’s more of a UI nicety, but could be useful during development or debugging of long ETL queries to see if DuckDB is making progress. - **Query profiling:** DuckDB can output detailed profiling information for a query. As an advanced user, you might do `SET enable_profiling='json'; SET profiling_mode='detailed';` on the connection[\[58\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Profiling%20details%3A%20For%20programmatic%20access,enable%20profiling%20output%20as%20JSON). Then after running a query, you can query the `duckdb_profiles()` table to get a JSON of the query execution profile[\[58\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Profiling%20details%3A%20For%20programmatic%20access,enable%20profiling%20output%20as%20JSON). This includes timing and row count per operator. It’s incredibly useful for pinpointing bottlenecks if a query is slow. You can also do `EXPLAIN SELECT ...` to get the query plan without running it, or `EXPLAIN ANALYZE SELECT ...` to get plan plus execution metrics in text form[\[59\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Query%20Planning%20and%20Profiling%3A%20Use,to%20see%20the%20planned%20execution)[\[60\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=This%20will%20output%20the%20query,row%20counts%2C%20use%20EXPLAIN%20ANALYZE). These tools help ensure that as you design more complex queries (especially on your graph structures), you can monitor performance and adjust (adding indexes, altering query structure, etc.) without guesswork.

In essence, the advanced connection options allow you to tune DuckDB for your workload and hardware. For instance, parsing code into graphs might be I/O bound if reading many files – you might increase threads to maximize parallel read. Conversely, if CPU-bound on analysis, you ensure enough threads are allocated but also avoid oversubscription if running multiple queries.

## Parameterized Queries and Prepared Statements

When writing Python code that executes SQL queries with varying parameters (e.g., filtering by different values), it’s important to use DuckDB’s **parameter substitution** rather than string formatting. DuckDB supports both **positional** (`?`) parameters and **named** (`$name`) parameters in SQL queries, following the standard DB-API practice.

**Positional parameters:** Use `?` as a placeholder in the SQL and provide a tuple or list of values as the second argument to `execute()` or `conn.sql()`. DuckDB will prepare the query (parse and optimize it) once, and then bind the given values. Example:

    conn.execute("SELECT * FROM nodes WHERE id = ? AND label = ?", [node_id, label]).fetchall()

DuckDB will replace the first `?` with `node_id` and second with `label` safely[\[61\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Create%20a%20table%20and%20insert,2000). This not only prevents SQL injection but also can reuse the prepared query if you call it repeatedly. If you need to run multiple inserts or similar in one call, you can use `executemany()` with a list of parameter lists. For instance, inserting multiple records:

    data = [(1, 'Alice'), (2, 'Bob'), (3, 'Eve')]
    conn.executemany("INSERT INTO people VALUES (?, ?)", data)

DuckDB will prepare the insert statement once and execute it for each tuple in `data`[\[62\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Batch%20insert%20multiple%20rows%20with,300%2C%202).

**Named parameters:** DuckDB also supports a `$name` syntax for parameters[\[63\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Named%20parameters%3A%20Use%20the%20%24name,syntax%20and%20provide%20a%20dictionary). You supply a Python dictionary mapping names to values. For example:

    query = "SELECT $greet || ', ' || $noun"
    conn.execute(query, {"greet": "Hello", "noun": "DuckDB"}).fetchone()
    # returns ("Hello, DuckDB",)

In the SQL, every occurrence of `$greet` is replaced with `"Hello"` and `$noun` with `"DuckDB"`[\[64\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=,%28%27Hello%2C%20DuckDB)[\[65\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Here%20%24greet%20and%20%24noun%20in,will%20use%20the%20same%20value). Named parameters can be convenient for building queries with optional filters (you can include or exclude parts of the query string and use a dictionary to supply only those that apply).

DuckDB even accepts numbered placeholders like `$1, $2` which correspond to positional indexes[\[66\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=DuckDB%20also%20accepts%20numbered%20parameters,70)[\[67\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=DuckDB%20also%20accepts%20numbered%20parameters,which%20correspond%20to%20the%201st). For example, `"SELECT $1, $2, $1"` with parameters `["duck", "goose"]` would produce `"duck", "goose", "duck"` for \$1 reused[\[68\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=values,will%20use%20the%20same%20value).

**Performance considerations:** Using parameterized queries is efficient. If you execute the exact same SQL text with different parameters repeatedly, DuckDB will likely reuse the cached prepared statement internally[\[69\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Note%20on%20performance%3A%20DuckDB%20can,77). This means you don’t pay the cost of re-parsing and optimizing the query every time. For large batches, instead of many `executemany` calls, it might be more efficient to load data via DataFrame or copy (e.g., using `conn.from_df` or the `COPY` SQL for bulk insert)[\[70\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=repeatedly%20via%20execute,77), but for moderate sizes, parameterized inserts are fine. The key is that you **avoid Python string concatenation for SQL** – always use placeholders so DuckDB can handle the binding and avoid injection vulnerabilities[\[71\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Important%3A%20Avoid%20using%20Python%20string,76).

In an expert system, you might even prepare statements manually (DuckDB’s C++ API allows explicit preparation, but Python API uses caching implicitly). Although Python API doesn’t expose a separate prepare step, trusting the internal caching is usually sufficient.

## Defining and Using Python UDFs (User-Defined Functions)

DuckDB allows you to **register Python functions as SQL functions** within a connection[\[72\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Use%20conn,calls%20the). This advanced feature is incredibly powerful: it lets you extend DuckDB’s SQL with arbitrary Python logic, which can be applied per row or in a vectorized manner.

Use `conn.create_function(name, function, parameters, return_type, **options)` to define a UDF[\[72\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Use%20conn,calls%20the): - `name`: The name of the function as used in SQL (e.g. `"calculate_score"`). - `function`: The actual Python callable (a function or lambda). This will be invoked by DuckDB when the UDF is used. - `parameters`: A list of DuckDB types for the function arguments. For example, `[duckdb.sqltypes.INTEGER, duckdb.sqltypes.VARCHAR]` if your function takes an int and a string[\[72\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Use%20conn,calls%20the). You can also specify these as strings (e.g. `"INT"`) for convenience. If your Python function has type annotations, you can pass `parameters=None` and DuckDB will attempt to infer the types. - `return_type`: The DuckDB type of the return value (e.g. `duckdb.sqltypes.DOUBLE` or `"DOUBLE"` for a function returning a float). - `**options`: Optional settings controlling how the UDF is executed. Key options include: - `type`: `"native"` or `"arrow"`. The default `"native"` mode calls your Python function for each scalar value (typical row-by-row UDF). If you set `type='arrow'`, DuckDB will invoke your function on a **column batch** represented as PyArrow arrays[\[73\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=annotations%2C%20you%20can%20often%20pass,default%29%20or%20%27special). Arrow mode (vectorized UDF) can greatly improve performance by allowing your function to process a whole chunk of data at once (using numpy/pandas operations internally, for example)[\[74\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=type%3D%27native%27%20,default%29%20or%20%27special). - `null_handling`: `"NULL"` or `"special"`. By default (NULL), if any argument to your UDF is NULL, DuckDB will **skip calling** your function and directly return NULL for that row (NULL in, NULL out)[\[75\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=as%20Arrow%20arrays%29,in%20the%20UDF%20simply%20produce). This is often fine for simple functions. If you need to handle NULLs in your function (maybe to implement a custom behavior), set `null_handling='special'`. Then DuckDB will pass Python `None` for any SQL NULL inputs, and you can decide what to return (including None for NULL output)[\[75\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=as%20Arrow%20arrays%29,in%20the%20UDF%20simply%20produce). - `exception_handling`: `"throw"` or `"return_null"`. Default is throw, which means if your Python function raises an exception during execution, that exception will propagate (likely causing the query to fail). If you prefer the UDF to be fault-tolerant, use `exception_handling='return_null'`. Then if an exception occurs for a given row or batch, DuckDB will catch it and just output NULL for that result, allowing the query to continue[\[75\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=as%20Arrow%20arrays%29,in%20the%20UDF%20simply%20produce). This is useful for UDFs where an occasional error is expected/acceptable (e.g., a parsing function that returns NULL for unparsable input rather than stopping the entire query). - `side_effects`: `False` (default) or `True`. Set this to True if your function has side effects or is not deterministic. DuckDB’s query optimizer may otherwise assume it can call the function fewer times or in a different order. Marking it as having side effects prevents optimizations like common subexpression elimination on it, ensuring it’s called exactly as needed[\[76\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=to%20handle%20NULLs%20inside%20your,If%20your).

**Example – Scalar UDF:** A simple scalar UDF adding 10 to a number:

    def add_ten(x: int) -> int:
        return x + 10

    conn.create_function("add_ten", add_ten)  # no types given, uses annotations to infer INT -> INT[77]

    # Now use it in SQL:
    print(conn.sql("SELECT add_ten(5)").fetchone())  # Outputs: (15,)

DuckDB inferred the parameter and return types from the annotation in `add_ten`[\[77\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=conn.create_function%28%22add_ten%22%2C%20add_ten%29%20%20,%2815). This function will be executed by DuckDB for each row where it’s applied. If you do `SELECT add_ten(col) FROM table`, DuckDB will call into Python for each value of `col`. Note that excessive calls can be slow due to Python’s overhead, but for moderate data sizes it’s fine.

**Example – Handling NULLs and exceptions:**

    def safe_divide(a: float, b: float) -> float:
        if b == 0 or b is None:   # treat None as NULL
            return None
        return a / b

    conn.create_function(
        "safe_divide", safe_divide, 
        [duckdb.sqltypes.DOUBLE, duckdb.sqltypes.DOUBLE], duckdb.sqltypes.DOUBLE,
        null_handling='special', exception_handling='return_null'
    )

Here, we define a division that returns NULL on division by zero (or if either input is NULL). We specified the parameter types explicitly as DOUBLE. We set `null_handling='special'` so that if either argument is SQL NULL, our function still gets called (with Python `None`) and we handle it in the code[\[78\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=conn.create_function%28,instead%20of%20stopping%20the%20query). We also set `exception_handling='return_null'` so that any unexpected error in the function would yield NULL instead of crashing the query[\[78\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=conn.create_function%28,instead%20of%20stopping%20the%20query). Now we can do:

    SELECT safe_divide(x, y) FROM numbers;

and if `y` is 0 for some row, the result will be NULL (instead of error)[\[79\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=instead%20of%20stopping%20the%20query). This demonstrates using DuckDB’s UDF options to build robust functions.

**Example – Vectorized (Arrow) UDF:** If performance is key and your operation can be done with vectorized libraries:

    import numpy as np

    def vector_add(col1, col2):
        # col1, col2 will be PyArrow arrays (because we'll use type='arrow')
        arr1 = col1.to_numpy(zero_copy_only=False)  # convert to numpy (zero_copy_only=False if types require conversion)
        arr2 = col2.to_numpy(zero_copy_only=False)
        result = arr1 + arr2  # vectorized addition using NumPy
        return result

    conn.create_function("vector_add", vector_add, 
                         [duckdb.sqltypes.BIGINT, duckdb.sqltypes.BIGINT], duckdb.sqltypes.BIGINT,
                         type='arrow')

Now calling `SELECT vector_add(i, j) FROM some_table` will execute the addition in batches: DuckDB will pass large chunks of column `i` and `j` as Arrow arrays to our function[\[80\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=match%20at%20L563%20conn.create_function%28,3%2C4)[\[81\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=conn.create_function%28,3%2C4). The function converts them to NumPy and adds them at C speed, returning a NumPy array (which Arrow can wrap back, and DuckDB accepts as a column of results). This can be *orders of magnitude faster* than row-by-row Python for large data. The tradeoff is the function logic must be capable of handling array inputs. With Arrow format, one must also handle possible chunking (DuckDB might call with chunks of maybe 1024 or 2048 values by default).

**Resource management:** Keep in mind that Python UDFs run inside the DuckDB process (since DuckDB is embedded). A misbehaving UDF (e.g., one that leaks memory or crashes the Python interpreter) can affect the whole process. Also, GIL (Global Interpreter Lock) means even if DuckDB uses multiple threads, if those threads are all calling into Python UDFs, they will execute one at a time. Thus, heavy use of Python UDFs could become a bottleneck. The general guidance is to use them for things not easily expressed in SQL or when integrating with Python libraries, but not for simple arithmetic or aggregations that DuckDB can do internally much faster.

In designing your system, Python UDFs provide an **escape hatch** to perform custom computations during queries (for example, running a complex graph algorithm on subsets of data, or parsing code structures on the fly). But consider the performance implications and possibly restrict to smaller data or vectorize them. For larger data, it might be better to pre-compute in Python and load the results into DuckDB, or use DuckDB’s native SQL features if possible.

## Extension and Integration Capabilities

*(Note: This section goes slightly beyond strictly the Connection API, but is worth mentioning for completeness in advanced usage.)*

DuckDB’s functionality can be extended via extensions and it integrates with external storage and formats out of the box: - The **HTTPFS extension** (usually included in the Python package) allows DuckDB to read from `s3://`, `gs://` (Google Cloud Storage), and web URLs directly. You just need to set up credentials (see next section on Secrets) or configure fsspec. For instance, `duckdb.query("SELECT * FROM 's3://bucket/data.parquet'")` just works after credentials are set. - **fsspec integration:** You can register custom filesystems via `duckdb.register_filesystem(pyfilesystem)`. For example, using Google Cloud Storage through `fsspec`: `duckdb.register_filesystem(fsspec.filesystem('gcs'))` then DuckDB can use `gcs://` paths[\[82\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=register%20an%20fsspec%20filesystem%3A)[\[83\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=DuckDB%20will%20route%20the%20gcs%3A%2F%2F,wide%20range%20of%20storage%20backends). This is an advanced trick to handle protocols DuckDB doesn’t natively support by routing through Python I/O. - **Extensions for formats**: There are DuckDB extensions to handle JSON (`json` extension, although DuckDB Python often has it loaded by default), Excel (`excel` extension), Parquet/Arrow (built-in), etc. In Python, you typically enable an extension by `conn.execute("LOAD extension_name");`. Once loaded, it might add new table functions or types (e.g., `read_json` function, or a `postgres_scanner` to query Postgres). For example, with the `json` extension, `duckdb.read_json('file.json')` becomes available to parse JSON files.

While these are not connection methods per se, they often require using the connection to install or load. In a complex system, these capabilities mean DuckDB can be a **central query engine** not just for its own tables, but for data in many formats and locations (CSV, Parquet, JSON, Excel, other DBs, cloud storage, etc.) without heavy ETL. This could save design effort by anticipating that you might need to pull in data from an S3 bucket or share results with a cloud data warehouse (MotherDuck or others) later on.

*(If cloud integration is a factor, one should also explore DuckDB’s* `CREATE SECRET` *and related commands, which allow managing credentials for S3/Azure etc., but that might be outside the immediate scope of your sections. DuckDB’s* *Secrets Manager* *can securely store access keys and automatically apply them to queries on S3/GS URLs[\[84\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Use%20the%20CREATE%20SECRET%20SQL,to%20store%20AWS%20S3%20keys)[\[85\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Using%20Secrets%20for%20Cloud%20Access). This prevents hardcoding credentials in code.)*

# DuckDB Relational API – Advanced Functionality

## Lazy Execution and Incremental Query Building (Recap)

The **Relational API** (DuckDBPyRelation) allows constructing SQL queries step by step using Python methods instead of writing full SQL statements. This API is *lazy*: no query actually runs until you explicitly request results or materialization[\[86\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=No%20data%20has%20been%20materialized,lazy)[\[87\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=None%20of%20these%20operations%20hit,a%20whole%2C%20potentially%20improving%20performance). For an expert user, leveraging this laziness is key to building complex data pipelines without unnecessary overhead.

When you call `conn.sql("...")` or `conn.from_df(df)` etc., you get a `DuckDBPyRelation` object that symbolically represents a result set, but no data is fetched yet[\[88\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=rel%20%3D%20conn.sql%28,lazy). You can then call additional methods on it (filter, join, project, etc.) to refine the query. Internally, DuckDB is building a single combined query. **None of these operations are executed immediately**[\[87\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=None%20of%20these%20operations%20hit,a%20whole%2C%20potentially%20improving%20performance). This means you can construct arbitrarily complex transformations without intermediate data movement. DuckDB will optimize and execute the final query in one go when needed, which can significantly improve performance by pushing filters down or combining operations.

For example:

    rel = conn.from_df(df) \
              .filter("score > 50") \
              .project("user, score*1.1 AS adjusted_score") \
              .order("adjusted_score DESC")
    # (At this point, no query has run yet)[89]
    ...
    result = rel.fetchall()  # triggers execution

All the filtering, projection, and ordering will be executed in one optimized SQL query when `fetchall()` is called[\[90\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Calling%20rel,and%20bring%20results%20into%20Python)[\[87\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=None%20of%20these%20operations%20hit,a%20whole%2C%20potentially%20improving%20performance). The laziness allows DuckDB to decide, for instance, to use an index on `score` for the filter if available, or to only select needed columns.

For an evolving system where you might chain many operations (especially if generating higher-order graphs through successive transformations of data), this approach means you don’t materialize intermediate states unless you explicitly choose to. It saves memory and time, and you can insert checkpoints by materializing to a table only when necessary.

## Method Chaining and Composability

The relational API covers essentially all common SQL operations through method calls. Some key methods (with their SQL analogs) include[\[91\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=relation.filter%28,13): - `filter(condition)` – adds a `WHERE` clause[\[91\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=relation.filter%28,13). - `project(expressions)` – akin to `SELECT ...` (choosing columns or expressions)[\[91\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=relation.filter%28,13). - `join(other_relation, condition, how='inner')` – performs an `JOIN` with another relation on the given condition[\[92\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=relation.join%28other_relation%2C%20,uses%20SQL%20join%20semantics). By default it’s an inner join; you can specify `how='left'` for LEFT OUTER, etc. - `aggregate(aggr_expr, group_expr)` – does a `GROUP BY` aggregation. You provide a list of aggregate expressions and grouping keys[\[92\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=relation.join%28other_relation%2C%20,uses%20SQL%20join%20semantics). - `order(order_expr)` – applies an `ORDER BY`[\[93\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=SQL%20join%20semantics%29). - `distinct()` – like `SELECT DISTINCT`, to remove duplicates. - `limit(n)` – like `LIMIT n`. - `union(other_relation)` – union (all) of two result sets, etc.

These operations are *composable*. You can chain multiple in one go as shown in the example, or save intermediate relations in variables and reuse them. For instance:

    base = conn.table('events').project("user, event_type, ts")
    recent = base.filter("ts > now() - interval '7 days'")
    stats = recent.aggregate("COUNT(*) as cnt", "event_type")

Here we created a base relation from a table, then a filtered relation, then aggregated by event type. Each step is lazy. You could then do `stats.to_df()` to get a Pandas DataFrame of results.

**Joins and multi-relation operations:** Joining two relations is straightforward – the `other_relation` can be another DuckDBPyRelation you obtained (from a table, a DataFrame, a subquery, etc.). The join condition can be given as a string (DuckDB expects the columns named, possibly qualified) or as an Expression. For example: `rel1.join(rel2, "rel1.id = rel2.id")`. If the column names don’t clash, you could also do `rel1.join(rel2, "id")` as shorthand (DuckDB will join on same-named column). You can join more than two by chaining: `rel1.join(rel2, "...").join(rel3, "...")` building up complex joins.

**Set operations:** The relational API also includes set operations like `union()`, `except_()`, and `intersect()`, corresponding to SQL UNION, EXCEPT, INTERSECT. E.g., `rel1.union(rel2)` gives a new relation representing all rows from rel1 and rel2 (by default UNION ALL behavior). If you need distinct union, you might chain a `.distinct()` after. Check the DuckDB Python API reference for exact usage, but typically these also do not execute until materialized.

The ability to derive one relation from another means you can modularize query logic. Perhaps you have a relation that computes a base graph from raw data, then another that filters that graph, another that computes metrics on it. Each can be its own relation, and you can independently test them or combine them further.

## Integrating Python Data Structures on the Fly

One of the most powerful advanced features is DuckDB’s ability to treat **Python data structures as tables** (through **replacement scans** or explicit registration). This blurs the line between in-memory Python objects and SQL tables, enabling very flexible workflows.

**Querying Pandas DataFrames directly:** If you have a Pandas DataFrame `df`, you can do `conn.sql("SELECT * FROM df")` directly[\[94\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Direct%20DataFrame%20Querying%3A%20If%20you,Example). DuckDB will automatically detect that `df` is a DataFrame in the local Python scope and substitute it as a table source[\[95\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=DuckDB%20will%20detect%20that%20df,scan%20of%20the%20DataFrame%E2%80%99s%20data). Under the hood, it uses a “replacement scan”, meaning the table name `df` in the SQL is resolved to an in-memory scan of that DataFrame’s data. This requires no prior copying of data into DuckDB – it happens at query time. The same works for **Polars DataFrame or LazyFrame** (name resolution will find a Polars object)[\[96\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Supported%20in,58), **PyArrow Table/RecordBatch/Dataset**, **NumPy arrays** (treated as tables), and even **DuckDB relations** (a relation object can appear as a subquery input by name)[\[96\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Supported%20in,58). In practice, that means if you have `rel = conn.from_csv_auto('data.csv')` producing a relation, you could do `conn.sql("SELECT * FROM rel")` too, but more commonly you’d just use the relation API on `rel`. For NumPy, if you have a structured array or a 2D array, referencing it by variable name in SQL will turn it into a set of columns (DuckDB tries to use field names for structured dtypes, otherwise generic column names)[\[96\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Supported%20in,58)[\[97\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Arrow%20objects%3A%20If%20you%20use,For%20example).

**Name precedence and explicit registration:** If you have a name that could refer to multiple things (like a DataFrame and a table in the DB with the same name), DuckDB resolves in this order: (1) an object explicitly *registered* via `register()` takes priority, (2) then a persistent table/view in the database, and (3) finally a Python object in scope for replacement scan[\[98\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Name%20precedence%3A%20If%20a%20name,you%20override%20existing%20tables%20safely). **Explicit registration** allows you to bind a Python object to a specific name in DuckDB to avoid ambiguity or to use it beyond the scope it was defined. For example, `duckdb.register('temp_view', some_dataframe)` will create a temporary view in the connection named `temp_view` that refers to `some_dataframe`[\[99\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Explicit%20object%20registration%3A%20In%20cases,you%20can%20explicitly%20register%20it). After this, you can run multiple SQL queries referring to `temp_view` without needing the `some_dataframe` variable to exist in the scope of each query. It’s essentially like creating a temporary table, but it doesn’t copy the data until used (it’s a view onto the data). When using multiple dataframes or mixing with actual tables, registering them with clear names might be safer to ensure the correct data is referenced.

Using replacement scans and registration means that as your in-memory Python data (like parsed code graphs) evolves, you can run SQL directly on it without an ETL step. For example, after constructing a graph as a pandas DataFrame of edges, you can immediately run DuckDB SQL to analyze degrees or patterns on it by just calling `conn.sql("SELECT ... FROM edges_df WHERE ...")`. This can save a lot of time during iterative development.

**Materializing Python data into DuckDB:** If you do want to persist an in-memory dataset into the DuckDB database (for reuse or performance), you have a couple of options: - Use `CREATE TABLE AS SELECT * FROM df` in SQL[\[100\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=After%20querying%20a%20DataFrame%2FArrow%2C%20you,Failed%20to%20cast%20value). This will pull the DataFrame data into a new DuckDB table on disk (or memory if your DB is in-memory). Similarly, `INSERT INTO existing_table SELECT * FROM df` appends data from the DataFrame[\[100\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=After%20querying%20a%20DataFrame%2FArrow%2C%20you,Failed%20to%20cast%20value). DuckDB under the hood fetches the data in chunks to avoid bloating memory. - Use the relation API: `conn.from_df(df).create('new_table')`[\[101\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Materialize%20as%20Table%2FView%3A%20Use%20relation.create%28,33%5D.%20For%20example) does the same in one call: creates `new_table` with the contents of `df`. If the table exists, `rel.insert_into('existing_table')` could be used (DuckDB might not have `insert_into` in Python API, but you can always do an SQL insert). - These are efficient because DuckDB uses vectorized fetch from Pandas (and can infer column types by sampling the DataFrame – by default it checks the first 1000 rows for object dtypes to guess types)[\[102\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=fetch%20the%20DataFrame%E2%80%99s%20data%20in,or)[\[103\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Similarly%2C%20if%20mytable%20exists%2C%20conn.execute%28,66). If you have mixed types in an object column and the inference fails, you can tweak `duckdb.CONFIG` like `SET pandas_analyze_sample=10000` to use more rows for type inference[\[102\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=fetch%20the%20DataFrame%E2%80%99s%20data%20in,or)[\[103\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Similarly%2C%20if%20mytable%20exists%2C%20conn.execute%28,66).

## Creating Views and Tables from Relations

We touched on `relation.create(name)` which materializes a relation as a table. This is a critical feature for breaking down a pipeline or caching results. If you have an expensive relational query that will be reused, you can persist it:

    rel = (...)  # some complex relation
    rel.create('intermediate_table')

Now `intermediate_table` exists in the DuckDB database with the result of that relation[\[101\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Materialize%20as%20Table%2FView%3A%20Use%20relation.create%28,33%5D.%20For%20example). Subsequent operations could use it, and it saves recomputation (at the expense of storage).

Alternatively, `relation.create_view('view_name')` registers the relation as a **view**[\[101\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Materialize%20as%20Table%2FView%3A%20Use%20relation.create%28,33%5D.%20For%20example). A view in DuckDB is just a stored query, so creating a view is instantaneous and doesn’t execute the relation immediately. The view will execute its query each time it’s accessed (unless DuckDB caches it in the same session). Use a view if you want to save the query logic under a name and maybe inspect it or use it multiple times, but the data might change underneath. Use a table if you want to freeze the output at a point in time.

**Example:** In graph building, after creating a relation that represents your graph’s edges with some computed weights, you might do `edges_rel.create('edges_table')`. Then you can attach that DuckDB file elsewhere or just ensure faster lookups. Or, if you are prototyping, `edges_rel.create_view('edges_v')` and then run various queries against `edges_v` to test different metrics without re-joining or re-computing the base relation each time.

## Direct Data Export from Relations

The relation API provides convenient methods to **export query results to files** without bringing them to Python, which is useful for larger data sharing or checkpointing: - `relation.to_parquet("file.parquet")` will execute the relation and write the result as a Parquet file[\[104\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=relation.to_parquet%28,30%5D.%20For%20example). It supports writing to a single file or partitioned into multiple files (if you use `.partition_by()` on the relation or provide partitioning options). This is valuable if you want to hand off the data to another system or just store it in a columnar format. DuckDB’s Parquet writer is efficient and you can specify compression (e.g., `compression='ZSTD'`). - `relation.to_csv("file.csv")` similarly writes out to CSV[\[104\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=relation.to_parquet%28,30%5D.%20For%20example). There are aliases `write_parquet` and `write_csv` which do the same[\[104\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=relation.to_parquet%28,30%5D.%20For%20example). - You can also directly do SQL’s `COPY` command via `conn.execute("COPY (SELECT * FROM ... ) TO 'file.csv' (OPTIONS)")`, but the relation methods are easier when you already have a relation object.

Given that your system might need to **share data over MCP** (perhaps a custom protocol or just a network share), being able to output a Parquet or CSV from any intermediate step with one line can save a lot of coding. For example, you’ve constructed a high-order graph representation as a relation – you can quickly dump it to Parquet, which could be memory-mapped or sent over the network to another service.

## Interfacing with External Tools (DataFrames, Arrow, ML frameworks)

We’ve discussed moving data from Python to DuckDB, but equally important is getting results out in the format needed for other components: - To get a **pandas DataFrame** from a relation, use `relation.df()` or `relation.to_df()`[\[105\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Pandas%20DataFrame%3A%20Use%20relation,20%5D.%20For%20example). This triggers execution and pulls all data into a pandas DataFrame (be cautious with very large results, as this will use memory accordingly). Internally, DuckDB uses zero-copy or efficient copying where possible to make this fast. - For **Polars**, `relation.pl()` will give a Polars DataFrame. You can also do `relation.pl(lazy=True)` to get a Polars `LazyFrame`[\[106\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Polars%20DataFrame%3A%20Use%20relation). That means you could even combine DuckDB and Polars lazy operations if you want to, although that’s quite advanced and one would need to be careful about what executes where. - For **Arrow**, `relation.arrow()` gives a PyArrow Table for the result[\[107\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Arrow%20Table%3A%20Use%20relation,24%5D.%20For%20example). Or `relation.fetch_arrow_table()` similar. There’s also `relation.fetch_arrow_reader()` if you want an Arrow RecordBatchReader (useful to stream data out in batches, say to avoid memory blow-up or to pipe into another system). - For **NumPy**, `relation.fetchnumpy()` returns a dictionary of column names to NumPy arrays[\[108\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=NumPy%3A%20Use%20relation,numpy.ndarray%5B27%5D.%20For%20example). This is convenient for integration with libraries that prefer NumPy arrays (scikit-learn, etc.). - For **PyTorch**, `relation.torch()` returns a dict of column name -\> PyTorch tensor; and `relation.tf()` does the same for TensorFlow (as long as those libraries are installed)[\[109\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=expecting%20NumPy%20arrays). This is particularly useful in machine learning pipelines where you might do feature preprocessing in DuckDB and then feed tensors directly into a model – all without manual conversion. - The relation API also supports `relation.map(...)` in recent versions, which allows you to apply a Python function to each chunk of the result as it’s pulled, which can be used to integrate with other stream processing, though this is an advanced usage not covered in depth here.

All these conversions are **one-liners**, making DuckDB act as a universal data hub between formats. The zero-copy Arrow integration means converting to Arrow or handing off to Polars is extremely fast and doesn’t duplicate memory if not necessary[\[110\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=arrow_table%20%3D%20conn.sql%28,pyarrow.Table)[\[111\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=DuckDB%E2%80%99s%20Arrow%20integration%20is%20zero,26). For example, if you call `.arrow()` and then convert that Arrow table to a pandas DataFrame, it could still avoid copying via Arrow’s pandas integration.

## Example: End-to-End Usage in a Complex Pipeline

Let’s tie this together with a hypothetical scenario akin to your use-case:

Suppose you parse source code into a base table `functions` (with function definitions), and another table `calls` (function call graph edges). You want to iteratively build higher-order call graphs (like which functions indirectly call which via 2 hops, 3 hops, etc.). DuckDB can help by recursive SQL, but you might also do it in Python and then use DuckDB to query the results.

1.  **Load base data:** Perhaps you stored the initial parse results in DuckDB or as CSV. You can read directly:

- funcs = duckdb.read_csv('functions.csv')  # relation for functions
      calls = duckdb.read_csv('calls.csv')      # relation for call edges (caller, callee)

  These are relations you can query or turn into tables (if you want to do multiple queries on them, you might do `funcs.create('functions')` to have a persistent table).

2.  **First-order analysis:** Using relational API, find immediate metrics, e.g., number of calls per function:

- calls_per_func = calls.aggregate("count(*) as call_count", "caller")

3.  **Higher-order graph building:** Suppose you wrote a Python function to compute transitive closure of the call graph (just as an example of higher-order structure). You run that in Python and get a list of pairs or a DataFrame `indirect_calls` with columns `caller, target, distance` (distance = number of hops in the call chain). Now, without writing to disk, you can register that:

- duckdb.register('indirect_calls', indirect_calls_df)

  Now `indirect_calls` is available in DuckDB. You can join it with other data, or query it:

      result = conn.sql("""
          SELECT f.name, count(*) as targets_reachable
          FROM functions f
          LEFT JOIN indirect_calls i ON f.id = i.caller
          GROUP BY f.name
          """).to_df()

  This query mixes the persistent `functions` table and the ephemeral `indirect_calls` view coming from the DataFrame. DuckDB’s optimizer will handle the join efficiently, fetching `indirect_calls_df` in chunks as needed.

4.  **Iteration and updates:** If the `indirect_calls_df` gets updated in Python (say you compute a larger closure), you can re-run queries without re-registering if you modified the object in place (since the view still points to it). If you replaced the DataFrame object, you’d register again. Alternatively, you could insert it into a DuckDB table if you want it managed on the DuckDB side.

5.  **Sharing results:** At any point, you can do `duckdb.sql("SELECT * FROM indirect_calls").to_parquet("indirect_calls.parquet")` to write the results to a Parquet file for others to use[\[104\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=relation.to_parquet%28,30%5D.%20For%20example). Or send it to pandas with `.to_df()` for further plotting, etc.

Throughout this, you have not had to export/import data manually or write intermediate CSVs – DuckDB handled the integration. The relational API calls like `.aggregate` and `.join` allowed you to express computations clearly, and the connection handled multi-database attachment and registration for combining sources.

**Conclusion:** The advanced features of DuckDB’s connection and relational APIs provide a *rich toolbox* for building complex data processing systems: - You can attach multiple data sources and query them together. - Tune the engine for performance and resource usage. - Use parameterized queries and UDFs for flexible, custom computation safely. - Construct queries dynamically using the relational and expression APIs without dropping to string manipulation. - Seamlessly move data between DuckDB and Python (DataFrames, Arrow, etc.), minimizing friction and copying.

By understanding and leveraging these features from the start, you ensure your system can accommodate new requirements (like new data formats, larger scale, more complex queries) without a fundamental redesign. DuckDB is essentially acting as an embedded analytical database and query accelerator for your Python application, and these advanced operations are what make it versatile for evolving, high-complexity workloads.

[\[1\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=The%20table%20below%20shows%20the,in%20types%20to%20DuckDB%20type) [\[2\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=Type%20DuckDB%20type%20bool%20BOOLEAN,BIGINT%20int8%20TINYINT%20uint16%20USMALLINT) [\[4\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=Nested%20Types) [\[5\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=) [\[6\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=) [\[7\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=) [\[8\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=,and%20values%20of%20the%20dict) [\[9\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=) [\[10\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=) [\[11\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=) [\[12\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=) [\[13\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=DuckDB%20type%20BIGINT%20BIT%20BLOB,DATE%20DOUBLE%20FLOAT%20HUGEINT%20INTEGER) [\[14\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=TIMESTAMP_NS%20TIMESTAMP_S%20TIMESTAMP_TZ%20TIMESTAMP%20TINYINT,UBIGINT%20UHUGEINT%20UINTEGER%20USMALLINT%20UTINYINT) [\[15\]](https://duckdb.org/docs/stable/clients/python/types#:~:text=INTERVAL%20SMALLINT%20SQLNULL%20TIME_TZ%20TIME,TIMESTAMP_MS%20TIMESTAMP_NS%20TIMESTAMP_S%20TIMESTAMP_TZ%20TIMESTAMP) Types API – DuckDB

<https://duckdb.org/docs/stable/clients/python/types>

[\[3\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=data%20types%20for%20the%20input,By) [\[38\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=The%20ATTACH%20statement%20adds%20another,be%20queried%20with%20that%20qualifier) [\[39\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=SELECT%20s,customer_id) [\[40\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Here%20sales_db,only%5B94%5D%5B95) [\[41\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=only) [\[42\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=can%20do%20cross) [\[43\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=ATTACH%20%27s3%3A%2F%2Fmy) [\[44\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Data%20transfer%20between%20DBs%3A%20Attaching,2%2B%20introduced%20COPY%20FROM%20DATABASE) [\[45\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Inferring%20alias%3A%20If%20you%20do,96) [\[46\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Additionally%2C%20ATTACH%20supports%20attaching%20non,databases%20via%20extensions) [\[47\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Encryption%3A%20DuckDB%20supports%20transparent%20encryption,you%20supply%20an%20encryption%20key) [\[48\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=When%20upgrading%20DuckDB%20versions%20or,EXPORT%20DATABASE%20and%20IMPORT%20DATABASE) [\[49\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=EXPORT%20DATABASE%20%27backup_dir%27%20) [\[50\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=IMPORT%20DATABASE%20%27dir_path%27%20does%20the,115) [\[51\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Auto,commit%28%29%20methods%20if%20needed) [\[52\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Multiple%20cursors%3A%20DuckDB%20allows%20multiple,safe%20and%20will%20serialize) [\[53\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Multiple%20cursors%3A%20DuckDB%20allows%20multiple,if%20used%20from%20multiple%20threads) [\[54\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Parallel%20Threads%3A%20DuckDB%20will%20use,You%20can%20change%20it) [\[55\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Memory%20Limit%3A%20By%20default%2C%20DuckDB,You%20can%20impose%20a%20limit) [\[56\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Temporary%20Directory%3A%20DuckDB%20uses%20disk,no%20spilling%20to%20disk) [\[57\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Progress%20Bar%3A%20For%20long,Enable%20it%20with) [\[58\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Profiling%20details%3A%20For%20programmatic%20access,enable%20profiling%20output%20as%20JSON) [\[59\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Query%20Planning%20and%20Profiling%3A%20Use,to%20see%20the%20planned%20execution) [\[60\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=This%20will%20output%20the%20query,row%20counts%2C%20use%20EXPLAIN%20ANALYZE) [\[61\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Create%20a%20table%20and%20insert,2000) [\[62\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Batch%20insert%20multiple%20rows%20with,300%2C%202) [\[63\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Named%20parameters%3A%20Use%20the%20%24name,syntax%20and%20provide%20a%20dictionary) [\[64\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=,%28%27Hello%2C%20DuckDB) [\[65\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Here%20%24greet%20and%20%24noun%20in,will%20use%20the%20same%20value) [\[66\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=DuckDB%20also%20accepts%20numbered%20parameters,70) [\[67\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=DuckDB%20also%20accepts%20numbered%20parameters,which%20correspond%20to%20the%201st) [\[68\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=values,will%20use%20the%20same%20value) [\[69\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Note%20on%20performance%3A%20DuckDB%20can,77) [\[70\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=repeatedly%20via%20execute,77) [\[71\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Important%3A%20Avoid%20using%20Python%20string,76) [\[72\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Use%20conn,calls%20the) [\[73\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=annotations%2C%20you%20can%20often%20pass,default%29%20or%20%27special) [\[74\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=type%3D%27native%27%20,default%29%20or%20%27special) [\[75\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=as%20Arrow%20arrays%29,in%20the%20UDF%20simply%20produce) [\[76\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=to%20handle%20NULLs%20inside%20your,If%20your) [\[77\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=conn.create_function%28%22add_ten%22%2C%20add_ten%29%20%20,%2815) [\[78\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=conn.create_function%28,instead%20of%20stopping%20the%20query) [\[79\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=instead%20of%20stopping%20the%20query) [\[80\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=match%20at%20L563%20conn.create_function%28,3%2C4) [\[81\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=conn.create_function%28,3%2C4) [\[82\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=register%20an%20fsspec%20filesystem%3A) [\[83\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=DuckDB%20will%20route%20the%20gcs%3A%2F%2F,wide%20range%20of%20storage%20backends) [\[84\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Use%20the%20CREATE%20SECRET%20SQL,to%20store%20AWS%20S3%20keys) [\[85\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Using%20Secrets%20for%20Cloud%20Access) [\[86\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=No%20data%20has%20been%20materialized,lazy) [\[87\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=None%20of%20these%20operations%20hit,a%20whole%2C%20potentially%20improving%20performance) [\[88\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=rel%20%3D%20conn.sql%28,lazy) [\[89\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=rel%20%3D%20conn.from_df%28df%29%20%5C%20.filter%28,adjusted_score%20DESC) [\[90\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Calling%20rel,and%20bring%20results%20into%20Python) [\[91\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=relation.filter%28,13) [\[92\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=relation.join%28other_relation%2C%20,uses%20SQL%20join%20semantics) [\[93\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=SQL%20join%20semantics%29) [\[94\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Direct%20DataFrame%20Querying%3A%20If%20you,Example) [\[95\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=DuckDB%20will%20detect%20that%20df,scan%20of%20the%20DataFrame%E2%80%99s%20data) [\[96\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Supported%20in,58) [\[97\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Arrow%20objects%3A%20If%20you%20use,For%20example) [\[98\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Name%20precedence%3A%20If%20a%20name,you%20override%20existing%20tables%20safely) [\[99\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Explicit%20object%20registration%3A%20In%20cases,you%20can%20explicitly%20register%20it) [\[100\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=After%20querying%20a%20DataFrame%2FArrow%2C%20you,Failed%20to%20cast%20value) [\[101\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Materialize%20as%20Table%2FView%3A%20Use%20relation.create%28,33%5D.%20For%20example) [\[102\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=fetch%20the%20DataFrame%E2%80%99s%20data%20in,or) [\[103\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Similarly%2C%20if%20mytable%20exists%2C%20conn.execute%28,66) [\[104\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=relation.to_parquet%28,30%5D.%20For%20example) [\[105\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Pandas%20DataFrame%3A%20Use%20relation,20%5D.%20For%20example) [\[106\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Polars%20DataFrame%3A%20Use%20relation) [\[107\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=Arrow%20Table%3A%20Use%20relation,24%5D.%20For%20example) [\[108\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=NumPy%3A%20Use%20relation,numpy.ndarray%5B27%5D.%20For%20example) [\[109\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=expecting%20NumPy%20arrays) [\[110\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=arrow_table%20%3D%20conn.sql%28,pyarrow.Table) [\[111\]](file://file-C6naX8tRxEh6rmu1Bza6tk#:~:text=DuckDB%E2%80%99s%20Arrow%20integration%20is%20zero,26) DuckDB Python Client Advanced Reference (v1.4+).docx

<file://file-C6naX8tRxEh6rmu1Bza6tk>

[\[16\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=The%20Expression%20class%20also%20contains,applied%20to%20any%20Expression%20type) [\[17\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=Why%20Would%20I%20Use%20the,Expression%20API) [\[18\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=The%20,instance%20of%20an%20expression) [\[19\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=Column%20Expression) [\[20\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=This%20expression%20contains%20a%20single,value) [\[21\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=const%20%3D%20duckdb) [\[22\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=Star%20Expression) [\[23\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=star%20%3D%20duckdb.StarExpression%28exclude%20%3D%20,show) [\[24\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=This%20expression%20selects%20all%20columns,of%20the%20input%20source) [\[25\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=Function%20Expression) [\[26\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=) [\[27\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=Case%20Expression) [\[28\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=hello%20%3D%20ConstantExpression,world) [\[29\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=This%20expression%20contains%20a%20,value%20%3D) [\[30\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=match%20at%20L652%20SQLExpression%28,1%20then%201%20else%200) [\[31\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=SQL%20Expression) [\[32\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=duckdb.df%28df%29.filter%28%20SQLExpression%28,%29.aggregate) [\[33\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=SQLExpression%28,1%20then%201%20else%200) [\[34\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=col_list%20%3D%20%5B%20duckdb.ColumnExpression%28%27a%27%29%20,col_list%29.show) [\[35\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=col_list%20%3D%20%5B%20duckdb.ColumnExpression%28%27a%27%29%20,col_list%29.show) [\[36\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=Order%20Operations) [\[37\]](https://duckdb.org/docs/stable/clients/python/expression#:~:text=SQLExpression%28) Expression API – DuckDB

<https://duckdb.org/docs/stable/clients/python/expression>
